<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="DOI" content="10.1045/may2015-xu" />
<meta name="description" content="D-Lib Magazine" /> 
<meta name="keywords" content="Image Annotation, Semantic Description, Hierarchical Model, Domain Vocabulary" />
<link rel="metadata" href="05xu.meta.xml" />
<link rel="metadata" href="../05bib.meta.bib" />
<link rel="metadata" href="../05ris.meta.ris" />
<link href="../../../style/style1.css" rel="stylesheet" type="text/css" />
<title>Semantic Description of Cultural Digital Images: Using a Hierarchical Model and Controlled Vocabulary</title>
</head>

<body>
<form action="https://www.dlib.org/cgi-bin/search.cgi" method="get">

<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#2b538e">
<tr>
<td><img src="../../../img2/space.gif" alt="" width="10" height="2" /></td></tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0">
<tr>
<td valign="bottom" colspan="4" align="right" bgcolor="#4078b1">

<table border="0">
<tr>
<td align="right" class="search"><img src="../../../img2/search2.gif" alt="" width="51" height="20" align="middle" />Search D-Lib:</td>

<td>
<input type="text" name="words" value="" size="25" />
</td>

<td align="left" valign="middle">
<input type="submit" name="search" value="Go!" />
<input type="hidden" name="config" value="htdig" />
<input type="hidden" name="restrict" value="" />
<input type="hidden" name="exclude" value="" /> 
</td>
</tr>
</table>

</td></tr></table>

<table width="100%" border="0" cellpadding="0" cellspacing="0">
<tr>
<td valign="bottom" colspan="4">

<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#e04c1e" id="outer" summary="Main Table">
<tr>
<td><img src="../../../img2/space.gif" alt="" width="10" height="1" /></td></tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#F6F6F6" id="bannertable">
  <tr>
    <td width="830" bgcolor="#4078b1" class="backBannerImage" align="left"><img src="../../../img2/D-Lib-blocks.gif" alt="D-Lib Magazine" width="450" height="100" border="0" /></td>
  </tr>
  <tr>
    <td width="830" bgcolor="#e04c1e"><img src="../../../img2/transparent.gif" alt="spacer" height="1" /></td>
  </tr>
  <tr>
    <td width="830" bgcolor="#eda443" align="left"><img src="../../../img2/magazine.gif" alt="The Magazine of Digital Library Research" width="830" height="24" border="0" /></td>
  </tr>
  <tr>
    <td width="830" bgcolor="#e04c1e"><img src="../../../img2/transparent.gif" alt="spacer" height="1" /></td>
   </tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0" id="navtable">
  <tr>
    <td width="5" height="20" bgcolor="#2b538e">&nbsp;</td>
    <td width="24" height="20" bgcolor="#2b538e"><img src="../../../img2/transparent.gif" alt="" width="24" height="20" /></td>
    <td height="20" align="left" bgcolor="#2b538e" class="navtext" nowrap="nowrap"><a href="../../../dlib.html">HOME</a>&nbsp;|&nbsp;<a href="../../../about.html">ABOUT D-LIB</a>&nbsp;|&nbsp;<a href="../../../contents.html" class="navtext">CURRENT ISSUE</a>&nbsp;|&nbsp;<a href="../../../back.html">ARCHIVE</a>&nbsp;|&nbsp;<a href="../../../author-index.html">INDEXES</a>&nbsp;|&nbsp;<a href="../../../groups.html">CALENDAR</a>&nbsp;|&nbsp;<a href="../../author-guidelines.html">AUTHOR GUIDELINES</a>&nbsp;|&nbsp;<a href="https://www.dlib.org/mailman/listinfo/dlib-subscribers">SUBSCRIBE</a>&nbsp;|&nbsp;<a href="../../letters.html">CONTACT D-LIB</a></td>
    <td width="5" height="20" bgcolor="#2b538e">&nbsp;</td>
  </tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0">
  <tr>
    <td width="55" height="1" bgcolor="#e04c1e"><img src="../../../img2/space.gif" alt="transparent image" width="1" height="1" /></td></tr>
</table>

<!-- CONTENT TABLE -->
<table width="100%" border="0" align="center" cellpadding="0" cellspacing="0">
  <tr>
  <td>
 
<!-- BEGIN MAIN CONTENT TABLE -->

<table width="100%" border="0" cellspacing="0" cellpadding="10" bgcolor="#ffffff">
<tr>

<td width="10"><img src="../../../img2/space.gif" alt="" width="1" height="1" /></td>

<td valign="top"> 

<h3 class="blue-space">D-Lib Magazine</h3>
<p class="blue">May/June 2015<br />
Volume 21, Number 5/6<br />
<a href="../05contents.html">Table of Contents</a>
</p> 

<div class="divider-full">&nbsp;</div>

<h3 class="blue-space">Semantic Description of Cultural Digital Images: Using a Hierarchical Model and Controlled Vocabulary</h3>

<p class="blue">
Lei Xu<br />
Wuhan University, Hubei, China<br />
xlei&#064;whu.edu.cn<br /><br />

Xiaoguang Wang<br /> 
Wuhan University, Hubei, China<br />
whu_wxg&#064;126.com

<br /><br />DOI: 10.1045/may2015-xu
 </p>

<div class="divider-full">&nbsp;</div>

<p class="blue"><a href="05xu.print.html" class="fc">Printer-friendly Version</a></p>

<div class="divider-full">&nbsp;</div>

 <!-- Abstract or TOC goes here --> 

<h3 class="blue">Abstract</h3>

<p class="blue">
Semantic description and annotation of digital images is key to the management and reuse of images in humanities computing. Due to the lack of domain-specific hierarchical description schema and controlled vocabularies for digital images, annotation results produced by current methods, such as machine annotation based on low-level visual features and human annotation based on experts' experiences, are inconsistent and of poor quality. To solve this problem, we propose a semantic description framework for content description, based on information needs and retrieval theory. The framework combines the semantic description with a domain thesaurus. In this paper we describe the relationship between the semantic levels under this description framework.  We conduct a preliminary test with this method in the cultural heritage field using digital images of the Dunhuang frescoes. We discuss the effect of semantic granularity on the annotation cost, from the point view of image semantic description granularity, and control strategies for an image's semantic description quality. Our findings show that this framework is applicable to the description of cultural digital image content.
</p>

<div class="divider-full">&nbsp;</div>

<p class="blue">
Keywords: Image Annotation, Semantic Description, Hierarchical Model, Domain Vocabulary</p>

<!-- Article goes next --> 

<div class="divider-full">&nbsp;</div>
<h3>1 Introduction</h3>

<p>In the Web 2.0 era, managing and reusing digital images has become a tough problem because the quantity of images being stored is growing dramatically. Metadata, which serves as an informative index or even as a substitute for the data itself sometimes, has achieved a lot in organizing digital images effectively. However, existing metadata schemas mainly focus on describing the shallow attributes of images, such as date, location, sizes and  format, but lack the necessary specifications for true image content description. In particular, the "semantic gap" between low-level features and the high-level semantic content of images has impeded the semantic retrieval of image content for many years. No general guiding framework for image semantic description exists, which has led to inconsistent and low quality, missing, incorrect, or meaningless semantic annotation in projects. Automatic annotation approaches based on machine learning techniques also have not reached a satisfactory level of development. Consequently a large number of digital images are not fully used for humanities computing [<a href="05xu.html#1">1</a>].</p>

<p>Cultural digital images, for example cultural heritage fresco digital images or digital images or paintings of historic sites, may help to preserve the original objects by substituting for them when they are not suitable to be touched or visited any more due to their age. There may be many stories behind them that are valuable for memories and studies. In this article we will discuss image description problems in the cultural field, especially those of frescoes and paintings, but the ideas and methods used in this study are also suitable for other cultural image descriptions.</p>

<p>The <a href="http://en.dha.ac.cn/">Dunhuang</a> frescoes digital images are our object of research. They constitute a special area in the field of Chinese cultural heritage. These frescoes are gems of the human cultural patrimony and are appreciated for both their artistic and humanities research value. With the popularization of digitizing objects of cultural heritage, researchers created an extensive number of digital images of the Dunhuang frescoes, allowing wide dissemination of these images and thus laying a foundation for relevant studies. However, because of the lack of semantic annotations, these images are not used to the fullest extent possible. To uncover the semantic information embodied in digital images of the Dunhuang frescoes and improve access to them, a semantic description method must be developed. This will not only benefit automatic semantic annotation and retrieval, but also publishing and presenting these cultural heritages on the semantic web.[<a href="05xu.html#2">2</a>]</p>

<p>In the following sections we will first review the studies related to semantic annotation of digital images, and then propose a semantic annotation method along with a hierarchical model framework and a domain vocabulary for cultural digital images. This method is designed to achieve standardization of artificial annotation content for cultural digital images thus laying a foundation for automatic annotation by machine. Although the method described in this paper is mainly designed for frescoes it also could be transformed easily to other similar fields by replacing the domain vocabulary. </p>

<div class="divider-full">&nbsp;</div>
<h3>2 Related Research</h3>

<div class="divider-dot">&nbsp;</div>
<h4>2.1 Digital image semantic annotation approaches</h4>

<p>Valid annotation is crucial to the management, preservation, analysis and sharing of digital images. Generally, the more detailed the image annotation, the higher the cost, but retrieval effectiveness is better, and vice versa. There are two approaches to image annotation: manual annotation and automatic annotation. Manual annotation is carried out by experts with the help of metadata standards and controlled vocabularies. Recently, several image annotation frameworks and software tools were developed for manual annotation, such as the <a href="http://hdl.handle.net/1903/14737">Text-Image Linking Environment</a> project, DM project [<a href="05xu.html#3">3</a>], <a href="http://mat.uwaterloo.ca/MAT/about-imagemat/">imageMAT</a> project, SharedCanvas project [<a href="05xu.html#4">4</a>], and <a href="http://www.islandora.ca/">Islandora</a> project.</p>

<p>Digital image annotation by machine is currently a frontier research topic in the field of computer graphics. The key idea behind image semantic annotation is to create an efficient method to map un-annotated images to existing semantic categories. The annotation process involves image visual feature extraction and annotation model building. Image visual features include color, texture and shape, etc. The main methods for feature extraction are based on the whole picture, area, or objects in it. The representation methods for these features are the histogram, regional characteristics, visual word bag and the size invariant features transform (SIFT) and so on. Automatic annotation models for images are generally based on statistical and machine learning models. Latent Semantic Analysis (LSA), probabilistic Latent Semantic Analysis model (pLSA) and Latent Dirichlet Allocation (LDA), etc., are of the generative type. Support Vector Machine (SVM), Bayes Discriminant Model, and Gaussian Mixture Model (GMM),  are examples of the discriminative type. The two types of models can be combined to boost performance [<a href="05xu.html#5">5</a>].</p>

<p>Currently, research on automatic annotation has been fruitful, but there are still many problems to be solved. Especially in the cultural field, the "semantic gap" between visual features and high-level semantic content of an image is still difficult to bridge. Due to the lack of a domain specific, standardized semantic description framework, the quality of automatic semantic annotation is difficult to judge and the significance of annotation results is also hard to evaluate.</p>

<div class="divider-dot">&nbsp;</div>
<h4>2.2 Digital image metadata formats</h4>

<p>A digital image metadata format refers to a set of image description and management terms, widely used in libraries and museums. Visual Resources Association (VRA) is a common description format for images and works in digital cultural heritage management. Exchangeable image file format (EXIF), ia standard set of metadata for digital images, includes items such as shutter, aperture, focal length, and other equipment information collected during shooting. DIG35 is a widely used format which provides users with a simple and agile access. There are <a href="http://www.w3.org/2005/Incubator/mmsem/XGR-interoperability">other metadata formats</a> for digital images, such as Dublin Core, EMP, SVG, etc.</p>

<p>Multimedia Content Description Interface (MPEG-7) is a popular international multimedia annotation standard, which provides a series of tools for image content description, such as Descriptors, Description Schemas, Description Definition Language and the relationships between them, and Description Defined Language used for description information. MPEG-7 supports a variety of audio and visual resources, including free text, statistics, multi-dimensional space-time structure, subjective and objective attributes, production attributes and combination information, etc. MPEG-7 is not for a specific field, but it provides the broadest possible support for images, audio, video and other multimedia resources. </p>

<p>Joan Beaudoin also proposed a contextual metadata framework for digital preservation of cultural objects from eight dimensions: <i>Technical</i>, <i>Utilization</i>, <i>Physical</i>, <i>Intangible</i>, <i>Curatorial</i>, <i>Authentication</i>, <i>Authorization</i>,  and <i>Intellectual</i>. [<a href="05xu.html#31">31</a>]</p>

<p>In general, the existing image metadata formats are designed principally to describe the external features of images to facilitate image indexing, preservation, retrieval and discovery. Due to a lack of high-level semantic content description, however, these metadata fail to support further semantic retrieval, content analysis, and knowledge discovery for humanities computing. Although there are some metadata designed to describe an image's semantic content, such as the <b>subject</b> or <b>meaning</b> of images, taking the metadata <b>meaning</b> in Joan Beaudoin's research for example, the value of this metadata was not structural at all &#151; it is textual only. In order to reveal entity objects and their relationships in the image content, and high-level semantic information such as scenes and activities, we have to create a more detailed semantic description framework for image annotating.</p>

<div class="divider-dot">&nbsp;</div>
<h4>2.3 User retrieval requirements for digital images</h4>

<p>To describe and annotate digital images, we need to understand user needs pertaining to searching for images or using images, specifically the information in the images users really care about, in Semantic Based Image Retrieval (SBIR). </p>

<p>From the perspective of cognitive psychology, J&#246;rgensen extracted 12 types of image description elements, such as people, objects, color, content/story, and visual elements obtained through an analysis of the descriptive text of needed images [<a href="05xu.html#7">7</a>]. Cunningham, <i>et al</i>., described user image retrieval needs with eight basic elements: image metadata, content, genre, occasion, color, example, affect, abstract [<a href="05xu.html#8">8</a>]. Shatford proposed a three-layer image needs classification model including general concept, specific concept, and abstract concept. Each layer has four dimensions: people, time, location, and content [<a href="05xu.html#9">9</a>].</p> 

<p>Jamis and Chang proposed a ten layer model for user image needs description on the basis of Shatford's model. The four layers at the bottom of the hierarchy are related to perception, including image type/technology, color, shape and texture. The six layers at the top of the hierarchy inherit from Shatford's three-layer model, but it adds two additional layers for each layer in Shatford's model: layers for objects contained in images as well as a layer for the whole scene shown in the image [<a href="05xu.html#10">10</a>].  Batley defines four types of image requirements: specific, general/named, general/abstract and general/subjective [<a href="05xu.html#11">11</a>]. These research results were integrated by Hollink, <i>et al</i>., who proposed a description framework for a text description and the visual features of images from the user's perspective [<a href="05xu.html#12">12</a>].</p>

<div class="divider-dot">&nbsp;</div>
<h4>2.4 Semantic description of digital images</h4>

<p>The objects, scenes, and their relations comprise part of the description content of images. We must consider the details, terms used or semantic layers in the process of image description, otherwise the description content of the images will be inconsistent with user queries. </p>

<p>The hierarchical semantic description of images has been recognized by many researchers. In literature, the word "hierarchy" is equally used for both composition and inheritance relations. Epshtein and Ullman build a hierarchy of visual features [<a href="05xu.html#13">13</a>]. They start with an informative fragment and search recursively for smaller informative fragments in it. Fidler and Leonardis build a hierarchy of image parts using unsupervised statistics [<a href="05xu.html#14">14</a>]. Each layer is built by via the composition of features of the previous layers. Lower layers are built from simple features, and higher layers describe more complex ones. Ahuja and Todorovic build a hierarchy of object parts in images based on their co-occurrence in a same object [<a href="05xu.html#15">15</a>].</p> 

<p>Rege, <i>et al</i>., build a semantic hierarchy based on users' experiences and their feedback [<a href="05xu.html#16">16</a>]. Categories of parts stem from the information extracted through the feedback and the hierarchical organization of images is able to be determined, and more intuitive search and browsing are also allowed. Eric and Thonnat classify objects top-down [<a href="05xu.html#17">17</a>]. When going down the hierarchy, a category has sub-categories in which feature extraction can be adapted depending on the candidate category, and objects could be classified. Torralba, <i>et al</i>. [<a href="05xu.html#18">18</a>] use many tiny images, with nearest neighbors. Because these images are labeled with WordNet nouns, they can categorize images at different levels using a hierarchical vote, where a label also votes for its parents. Fan, <i>et al</i>., [<a href="05xu.html#19">19</a>]; [<a href="05xu.html#20">20</a>]; [<a href="05xu.html#21">21</a>] propose a hierarchical boosting algorithm based on ontology and <a href="https://en.wikipedia.org/wiki/WordNet">WordNet</a> allowing image annotation at different generic levels.</p>

<p>The existing research has proved that the hierarchical descriptions of images basically stem from the structured vocabularies' hierarchies. Though semantic hierarchies and structured vocabularies are crucial to the success of image annotation, there is still not a unified standard for image description [<a href="05xu.html#22">22</a>].</p>

<div class="divider-full">&nbsp;</div>
<h3>3 Semantic description of cultural digital images</h3>

<div class="divider-dot">&nbsp;</div>
<h4>3.1 The hierarchical model</h4>

<p>We propose a hierarchical semantic model for digital image description. When integrating semantic layer information for images from the perspective of user demands, the twelve kinds of image description elements that J&#246;rgensen [<a href="05xu.html#7">7</a>] summarized have a corresponding layer in our semantic hierarchy model. Cunningham's [<a href="05xu.html#8">8</a>] eight image elements have involved common image metadata. We can also find a corresponding place in a behavior semantic layer that represents the four dimensions (people, content, time and location) proposed by Shatford [<a href="05xu.html#9">9</a>]. The description layers for image requirements proposed by Jaimes and Chang [<a href="05xu.html#10">10</a>] include general, specific and abstract concepts. These concepts come from users' knowledge, expressed through corresponding thesauruses or general vocabularies. Therefore, on the basis of the image semantic layer, our proposed model adds general, specific and abstract concepts to the top of the hierarchical model. As for the object layer, we can divide the object classification into three levels (the general, specific and abstract concepts) when we describe a specific object. This classification task can be completed using domain ontologies or other classification terminologies and vocabularies. In our work, we integrate the general, specific and abstract concepts and other corresponding vocabularies into the high-level Hierarchical Semantic Model. Our proposed model makes the description content for images comprehensive, therefore better meeting user image retrieval requirements. The description of the framework is shown in Figure1.</p>

<div align="center">
<img src="xu-fig1.png" alt="xu-fig1" width="564" height="390" class="borderGray" vspace="10" />
<p><i>Figure 1: Hierarchical semantic model for digital images</i></p>
</div>

<ol>
	<li><b>Bottom Features.</b>  Image color, texture, and shape are features that exist in an image. This information can be extracted automatically by pattern recognition and graph learning technology, or a mix of methods. However, the Dunhuang fresco digital images are digital copies of artistic works which are the expression of the real world or an abstract world. The spatial relationship among objects in the frescoes may be different from the relationship reflected in the pixel distance found in digital images. The scene and sentiment information in frescoes are different from that of the real world. In the literature (Liang Guanyu, <a href="05xu.html#24">2000</a>), experimental results showed that using the feature extraction methods mentioned previously to identify high-level semantic information is not completely suitable for frescoes. In this paper, we will not delve further into details of the extraction methods for bottom visual features of cultural digital images.</li>
</ol>

<ol start="2">
	<li><b>Objects.</b> The description of objects is the most important part of the hierarchical semantic model. Here, we divide the description content of objects into two categories: classification and attributes. In addition to metadata and visual features, objects in an image have their own properties, such as the name, gender, attitude, and other attributes related to specific characters in the image. Using pattern recognition to determine high-level semantic content is problematic, so these attributes cannot be entirely recognized through bottom visual feature extraction. We need to use corresponding domain-specific controlled vocabularies or ontologies to describe them, such as the Dunhuang studies dictionary [<a href="05xu.html#25">25</a>] and other vocabularies. </li>
</ol>

<ol start="3">
	<li><b>Object Space</b>. Object space can be divided into three categories: direction relations of an object area in the digital image level, the topological relations reflected in objects themselves, and semantic spatial relations among objects. We use the spatial relationship description in the MPEG-7 framework to describe Dunhuang frescoes (see Table 1). The spatial description scheme in MPEG-7 includes object spatial direction relations, topological relations, and semantic space relations.</li>
</ol>

<div align="center">
<table align="center" border="0" cellpadding="8" cellspacing="0">

<tr>
<td class="topLeft"><b>Relation Type</b></td>
<td class="topLeftRight" style="white-space:nowrap" align="left"><b>Examples</b></td>
</tr>

<tr>
<td class="topLeft">Space Direction Relations</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">topof, bottomof, leftof, rightof, middleof</td>
</tr>

<tr>
<td class="topLeft">Topological Relations</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">nearby, within, contain, adjacentto</td>
</tr>

<tr>
<td class="topLeftBottom">Semantic Space Relations</td>
<td class="all" style="white-space:nowrap" align="left">belongsto, partof, relatedto, consistsof</td>
</tr>
</table>

<p><i>Table 1: Object Space Relation Description</i></p>
</div>

<p style="padding-left: 3em;">A spatial semantic relation is the logical relationship among objects, such as affiliation, dependency, correlativity, among others. These descriptions usually cannot be extracted automatically. Spatial direction relations can be extracted automatically according to the regional location of objects, but the confirmation of the direction depends on the methods used to divide object areas (such as rectangles, circles, etc.) and the methods used to calculate the direction (such as the center of mass, or the bottom edge of area based methods, etc). Different methods may lead to different outcomes. Spatial direction relation data can be used to describe topological relations. For example, the "middle of" relation between objects may be the "within" topological relation. However, it depends on context. </p>

<ol start="5">
	<li><b>Scene.</b> The parts of an image other than the main objects can be called the scene. A scene contains environment, such as rain, snow, etc. Scene data can be used to solve some of the ambiguity at the image semantic level. For example, images of the same people may express different meanings in different scenes. Without understanding the differences, we may get a wrong result. Identifying scenes can provide the context necessary to understand the semantic content. Different people may have different opinions on what the main objects are. We can use human visual attention mechanisms [<a href="05xu.html#26">26</a>] to identify the main or significant areas of an image, or the regions that users are interested in. There are differences between a scene painted in frescoes and the real world. We can identify the scene by background color, texture, shape, and visual features of digital images shot in the real world, for example, a large green zone in an image may represent green grass in the real world. Scenes in frescoes with different artistic compositions describe the real world differently and cannot be obtained by the same method. Some scenes are mixed, such as the alternation of inside and outside scenes. Moreover, the scenes painted in frescoes are not real and some frescoes contain scenes, such as story paintings and JingBian paintings, while others do not contain scenes, such as figure paintings and pattern paintings, and in other frescoes we cannot distinguish the specific scene effectively. In this paper, we divide scenes into three types: geographic scenes, time scenes and weather scenes. A geographic scene includes sky, indoor, outdoor, lake, river, mountain, sea, field, etc. A time scene contains spring, summer, autumn, winter, morning, evening, noon, etc. A weather scene contains rain, snow, wind, thunder, lightning, sunshine, etc. It should be noted that some geographic scenes, such as a lake, a sea, or a mountain, may be described in the object layer, but the theme of Dunhuang frescoes is primarily Buddhist stories, so we usually put these words or terms in the scene layer.</li>
</ol>

<ol start="6">
	<li><b>Sentiment.</b> There are two points of view in image sentiment description. The first is people's subjective feelings about the entire image, such as their feelings when they see red or green color, and this is not our focus. Another is the emotions of the characters themselves in the image, such as anger, serenity, etc. Psychologists divide human emotions into six types [<a href="05xu.html#27">27</a>]: happiness, surprise, fear, sadness, disgust, and anger. These can be applied to the classification of affections in our description framework. However, the emotions exhibited by characters in images could be differently interpreted by different annotators. Or the image could be too vague to ascertain an emotion. Therefore, sentiment recognition of characters in images requires a combination of characters and the environment around them. For example, the emotion of characters in a celebration can be described as "happy". However, in general, sentiment information cannot be extracted automatically.</li>
</ol>

<div class="divider-dot">&nbsp;</div>
<h4>3.2 Semantic association in image content</h4>

<p>Through the above analysis, the complicated semantic relationships are expressed. Furthermore, an object itself also has inherent attributes. To avoid confusion, the object's attributes are divided into two facets, the data attributes that reflect the inherent characteristics of the object, and the object relation attributes which reflect the interjacent characteristics between objects.</p>

<p>Since characters appear frequently in Dunhuang frescos, we give a detailed description of the characters' attributes. Table 2 presents some of the data attributes of characters. </p>

<div align="center">
<table align="center" border="0" cellpadding="8" cellspacing="0">

<tr>
<td class="topLeft"><b>Property</b></td>
<td class="topLeftRight" style="white-space:nowrap" align="left"><b>Value Range</b></td>
</tr>

<tr>
<td class="topLeft">Gender</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">male, female</td>
</tr>

<tr>
<td class="topLeft">Eye Color</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">blue, green, etc.</td>
</tr>

<tr>
<td class="topLeft">Action</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">walk, sit, kneel, run, jump, etc.</td>
</tr>

<tr>
<td class="topLeft">Head Direction</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">up, down, left, right, etc.</td>
</tr>

<tr>
<td class="topLeft">Direction</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">up, down, left, right</td>
</tr>

<tr>
<td class="topLeft">Hands Action</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">lift, cross before breast, etc.</td>
</tr>

<tr>
<td class="topLeft">Posture</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">chubby, delicate</td>
</tr>

<tr>
<td class="topLeftBottom">Backlight color</td>
<td class="all" style="white-space:nowrap" align="left">yellow, white, etc.</td>
</tr>
</table>

<p><i>Table 2: Some Data Attributes of Characters</i></p>
</div>

<p>The relationships between objects can be divided into three levels: relationship at the object space level (Table 1), relationship in behavior level, and other relationships that can be used as object properties. We have already discussed the object space and behavior level relations, so we will focus on the object property relation in this section. </p>

<p>First, we should determine the differences between behavior level relation and object property relation. For example, the behavior "shooting an arrow", implies the meaning of a relationship as "a figure <b>holds</b> a bow", but for "dancing" behavior, the case is different. In another example, the object property "with Clothes On" implies behavior by the verb "wear". These two kinds of relationships complement each other and enrich the semantic associations between objects. The verbs such as "play", "dress" or other similar terms are very numerous and jumbled if they are treated as object properties; therefore, image content that can be described at the behavior level or as a space relationship will not be described as object properties. When the description information cannot be obtained at the behavior level, we will consider description using the object property. The general object properties are summarized and shown in Table 3 below.</p>

<div align="center">
<table align="center" border="0" cellpadding="8" cellspacing="0">

<tr>
<td class="topLeft"><b>Object Property</b></td>
<td class="topLeft"><b>Explanation</b></td>
<td class="topLeftRight" style="white-space:nowrap" align="left"><b>Range</b></td>
</tr>

<tr>
<td class="topLeft">Hold</td>
<td class="topLeft">figures' and other objects' relations</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">lift, support from under, clasp, etc.</td>
</tr>

<tr>
<td class="topLeft">Clothes</td>
<td class="topLeft">figures' headwear, clothes, accessories</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">&#151;</td>
</tr>


<tr>
<td class="topLeft">Social Relationship</td>
<td class="topLeft">relationships among people in society</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">disciple, minister, etc.</td>
</tr>

<tr>
<td class="topLeftBottom">Pattern</td>
<td class="topLeftBottom">pattern on objects</td>
<td class="all" style="white-space:nowrap" align="left">pattern on clothes, utensils, buildings</td>
</tr>
</table>

<p><i>Table 3: The Object Relation Attributes</i></p>
</div>

<p>Figure 2 explains the relationship between the semantic layers of images. The object layer is the center of this model.  The other high-level semantic layers rely on the object layer. The collection of objects forms the main body of an image, but an object has its own properties. It produces associations through object properties, spatial relation information, and behavior data between objects. The behavior layer depends on the object layer and becomes connected to the scene layer. Digital images themselves have metadata description properties. The visual characteristics of digital images also have different property description methods.</p>

<div align="center">
<img src="xu-fig2.png" alt="xu-fig2" width="608" height="415" class="borderGray" vspace="10" />
<p><i>Figure 2: Correlation Diagram of Image Semantic Levels</i></p>
</div>

<div class="divider-dot">&nbsp;</div>
<h4>3.3 Thesaurus of cultural digital image content</h4>

<p>Domain terms are good descriptors of the field and contain the characteristics of objects to be described. For example, the classification of Dunhuang frescoes's Buddha involves a lot of fresco terms, such as flying apsaras, bodhisattva, etc. We refer to the "Dunhuang studies dictionary" [<a href="05xu.html#25">25</a>] and other research literature [<a href="05xu.html#28">28</a>] about the classification of Dunhuang frescoes to sort out professional terms in this field and classify these vocabularies effectively. Finally, we formed a classification thesauri system for the Dunhuang frescoes. The Dunhuang frescoes content classification system is shown in Table 4. It is classified according to Figure 2 and only shows the classification of terms in the highest level of each layer of the semantic hierarchy model.</p>

<div align="center">
<table align="center" border="0" cellpadding="8" cellspacing="0">

<tr>
<td class="topLeft"><b>Semantic Level</b></td>
<td class="topLeftRight" style="white-space:nowrap" align="left"><b>Classification Terms</b></td>
</tr>

<tr>
<td class="topLeft">Object</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">people, animal, plant, utensil, vehicle, building, clothes, pattern</td>
</tr>

<tr>
<td class="topLeft">Object Space</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">direction relations, topological relations, semantic relations</td>
</tr>

<tr>
<td class="topLeft">Behavior</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">recreational and sports activities, production and living, Buddhism activities, general behavior</td>
</tr>

<tr>
<td class="topLeft">Scene</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">geographic scene, time scene, weather scene</td>
</tr>

<tr>
<td class="topLeftBottom">Sentiment</td>
<td class="all" style="white-space:nowrap" align="left">happiness, surprise, fear, sadness, disgust, anger, serene</td>
</tr>
</table>

<p><i>Table 4: Top-level Classification Terms of Dunhuang Frescoes</i></p>
</div>

<p>The terminologies that appear most frequently in relation to Dunhuang are scattered in semantic layers of objects (such as people, building, clothes, etc.) and behaviors (such as JingBian, music dance, production and living, etc). The objects and behaviors terms can make the field characteristics of Dunhuang frescoes stand out, but the object space, scene, and sentiment semantic information is universally used through the analysis and used not only in fresco digital image fields but also in other areas. The description of classifications may be the same and differences may be not obvious when we describe the object space, scene, and sentiment of a digital image in another field.</p>

<div class="divider-full">&nbsp;</div>
<h3>4 Description methods and examples</h3>

<div class="divider-dot">&nbsp;</div>
<h4>4.1 Hierarchical description method</h4>

<p>Considering users' demands for image retrieval and the semantic hierarchical model of the image content comprehensively, we propose a hierarchical description method combined with domain-specific terms. We reference the conceptual context + focus and zoom [<a href="05xu.html#29">29</a>] for graphical information visualization. First, we describe the content as a whole to accommodate most of the content found in the images; this is the overall description content. In this phase, we use metadata combining the top-level classification from domain-specific terminologies as shown in Table 4, such as behavior and scene information. On the basis of the fixed size of the area of vision, we continue to amplify the image and describe it, using local information. At this stage, we do not use metadata any longer; instead, we use object, object space relationships, scene, behavior and sentiment layer terms to describe images, combining these terms with subprime terminologies, or lower level terms, to describe the cultural digital image. We keep describing the image until it cannot be divided into smaller objects. This method is, in essence, a kind of granularity analysis method. When we exaggerate images gradually, we make more fine-grained observations. High-level terms correspond to generalized concepts and terms at the bottom correspond to specific concepts that represent users' different understandings of an image. The key to hierarchical description is information fusion at all levels using the principle of granularity.</p>

<p>The advantage of hierarchical description is that the lower layer of an image carries more description information, because it inherits description information from the surrounding graphical environment; thus, it is unnecessary to describe a certain area of an extraction from the digital image alone. The part extracted contains the upper inheritance information; we only need to change the metadata content slightly for the area changed by the extraction operation. The hierarchical description method can also satisfy users' needs for image retrieval from different angles. There are more details in objects. These description details, however, are limited by controlled terms. </p>

<div class="divider-dot">&nbsp;</div>
<h4>4.2 Application example of digital image description framework</h4>

<p>The Dunhuang fresco digital image description standard can be applied to the Dunhuang fresco image annotation system, retrieval system, the classification of frescoes, and facet browsing. The fundamental purpose of image description and annotation is to meet users' needs for image retrieval, connecting users' requirements and image descriptions, solving the dilemma of "No relevant search results". We provide below a description on an actual image using the semantic hierarchical model mentioned previously. Table 5 presents the semantic description results for Figure 3.</p>

<div align="center">
<img src="xu-fig3.png" alt="xu-fig3" width="542" height="380" class="border" vspace="10" />
<p><i>Figure 3: Scattering flowers and flying Apsaras [<a href="05xu.html#30">30</a>]</i></p>
</div>

<div class="divider-gray">&nbsp;</div>
<div class="divider-white">&nbsp;</div>

<div align="center">
<table align="center" border="0" cellpadding="8" cellspacing="0">

<tr>
<td class="topLeft"><b>Semantic Level</b></td>
<td class="topLeftRight" style="white-space:nowrap" align="left" colspan="4"><b>Description</b></td>
</tr>

<tr>
<td class="topLeft">Bottom visual features</td>
<td class="topLeftRight" style="white-space:nowrap" align="left" colspan="4">color histogram, texture, shape, etc.</td>
</tr>

<tr>
<td class="topLeft" rowspan="2">Object</td>
<td class="topLeft" style="white-space:nowrap" align="left">First level</td>
<td class="topLeftRight" style="white-space:nowrap" align="left" colspan="3">scattering flower, flying Apsaras, lotus, auspicious clouds</td>
</tr>

<tr>
<td class="topLeft" style="white-space:nowrap" align="left">Second level</td>
<td class="topLeftRight" style="white-space:nowrap" align="left" colspan="3">parts of object such as Apsaras' torso, limbs, figure's clothing, wreaths, belt</td>
</tr>

<tr>
<td class="topLeft" rowspan="3">Object Space</td>
<td class="topLeft" style="white-space:nowrap" align="left">Space direction</td>
<td class="topLeftRight" style="white-space:nowrap" align="left"colspan="3" >Apsaras is <b>in upper part</b> of the image, lotus is <b>in lower part</b> of the image</td>
</tr>

<tr>
<td class="topLeft" style="white-space:nowrap" align="left">Topology</td>
<td class="topLeftRight" style="white-space:nowrap" align="left" colspan="3">Apsaras is <b>above</b> auspicious clouds</td>
</tr>

<tr>
<td class="topLeft" style="white-space:nowrap" align="left">Semantic space</td>
<td class="topLeftRight" style="white-space:nowrap" align="left" colspan="3">Apsaras object <b>consists of</b> head, torso, limbs, etc.</td>
</tr>

<tr>
<td class="topLeft">Scene</td>
<td class="topLeftRight" style="white-space:nowrap" align="left" colspan="4">outdoor scene, sky scene, hill</td>
</tr>


<tr>
<td class="topLeft" rowspan="3">Behavior</td>
<td class="topLeft" style="white-space:nowrap" align="left">Objects</td>
<td class="topLeftRight" style="white-space:nowrap" align="left" colspan="3">Apsaras, lotus, auspicious clouds</td>
</tr>

<tr>
<td class="topLeft" style="white-space:nowrap" align="left">Content</td>
<td class="topLeftRight" style="white-space:nowrap" align="left" colspan="3">Apsaras <b>scattering flower</b> (<b>scattering flower</b> is a term in Dunhuang)<br />Apsaras <b>clothes</b> belt (<b>clothes</b> is a object property)<br />Apsaras <b>hold</b> lotus  (<b>hold</b> is a object property)
</td>
</tr>

<tr>
<td class="topLeft" style="white-space:nowrap" align="left">Location</td>
<td class="topLeft" style="white-space:nowrap" align="left">sky</td>
<td class="topLeft" style="white-space:nowrap" align="left">time</td>
<td class="topLeftRight" style="white-space:nowrap" align="left">middle tang dynasty</td>
</tr>

<tr>
<td class="topLeftBottom">Sentiment</td>
<td class="all" style="white-space:nowrap" align="left" colspan="4">serene</td>
</tr>
</table>

<p><i>Table 5: Description Results for Figure 3 Using the Semantic Hierarchical Model</i></p>
</div>

<p>The hierarchical description method makes a comprehensive description of all objects in the image feasible. For example, in the description shown in Figure 3, the description of the character object can be divided into two levels. The first level is the individual level and the second is part of the individual level. Of course, objects can still be divided into smaller parts, for example the description of a statue fresco with drawn-in details. The head can be further divided into hair ornament, backlight, eyes, eyebrows, nose, ears, and mouth. A further description process is necessary, because Dunhuang fresco researchers summarized the characteristics and style of certain periods by observing the figures' face distributions, postures, and facial expressions. The description of the object space relations and the scene are easy compared to the object layer. We must grasp main characteristics of images to make the description standardized and comprehensive. It is easy to process object space and scene. For instance, the head and torso of a character in a fresco has a <b>topof</b> relationship, but we prefer to use semantic space relation, such as <b>consistof</b>, to express the relationship that a character's head and torso are parts of the body. In describing the layer of behavior, we used Dunhuang terminologies to standardize the results and used terms or unified verbs to describe the content of behaviors. From this analysis, we conclude that while description of the sentiment level is relatively simple, sentiment recognition is difficult, but based on the scene and behavioral information combined with the character's facial expression, we can describe an emotion using a word like "serene" for this character. </p>

<p>The properties of objects in images need to be described according to the granularity and identifiably of images. In Figure 3, the character in the image whose dress color is brown, is female, the figure is chubby, both hands are lifting a lotus, the neck wears wreaths, etc. These can be described by their properties and produce semantic associations with other objects in the image. </p>

<p>In addition, Table 5 does not represent a full record of Figure 3. Much descriptive information about this image is omitted, such as the title, size, format, etc., because the approach we proposed came primarily from user needs, and we focused on image semantic content rather than the external features of the image. The main semantic content is described according to our approach, but other content was not. </p>

<div class="divider-full">&nbsp;</div>
<h3>5 Discussion and Conclusions</h3>

<div class="divider-dot">&nbsp;</div>
<h4>5.1 Granularity and cost</h4>

<p>As discussed previously, we must improve the description granularity of images if we want to guarantee the comprehensiveness of those descriptions. The granularity of image descriptions includes the granularity of description terms and the hierarchical descriptions. Description term granularity depends on the way images are divided by domain terms, while the granularity of hierarchical description depends on the control of the image level during the process of actual image description creation. In general, description term granularity and image hierarchical granularity are consistent with each other. However the comprehensiveness and granularity of image description are restricted in practical cultural digital image description processes, given the different data volume to be annotated, the differences of specific applications, human and capital investment, etc. If description data is too fine, a describer's work will be so large as to be unmanageable, and the complexity of annotation will be too large. On the contrary, the annotation effect will suffer if the granularity is too coarse.</p>

<p>There are some principles we must follow when we describe cultural digital images. The description data should be rough when objects in frescoes are abstract, and finer when the objects are described in detail; some characters not prominent in images could be annotated in the top level rather than the bottom level, etc. Some cultural digital images have adornment, natural scenery, and decorative patterns as frames. We can identify the complete subject of the images according to these decorative patterns to avoid missing semantic descriptive information. Because cultural digital images may include antiques, some of the content cannot be identified effectively. Images that cannot be identified or confirmed through literature are not described in our research.</p>

<div class="divider-dot">&nbsp;</div>
<h4>5.2 Quality control</h4>

<p>From the view of information source control, we choose domain-specific terms such as the "Dunhuang studies dictionary" supplemented with other scholars' research as the source of the terms. We provide complete concepts and entry words that have been sorted, classified, and identified with the help of experts in the field. In the information generation phase, we guaranteed the quality of the description terms' clarity, consistency, scalability, accuracy and comprehensiveness, etc. In the information output stage, we investigated the quality of terms by analyzing the organization, form, ease of sharing, and versatility of description results. At the information use phase, quality control work on terms will be carried out by analyzing the application scope, effect, social value, and significance of the description framework in practical application.</p>

<p>In this paper we presented a cultural digital image description framework. According to our analysis, either the RDF (Resource Description Framework) or OWL semantic format are a suitable organization form for image description results. Both can link all layers of a semantic hierarchy model and represent the information about object properties. Organization form research will be addressed in detail in our future work. </p>

<div class="divider-dot">&nbsp;</div>
<h4>5.3 Portability</h4>

<p>The portability of this description framework is directly related to its application scope. The framework proposed in this paper depends on a specific area only and has nothing to do with any specific application. It does not involve the use of a software or hardware environment and does not depend on any system environment. This framework can be applied in different fields of image description work, especially for painting digital images. The open standard ensures that it can coexist with other description specifications and is easy to replace. Because it is related to domain-specific fields such as Dunhuang frescoes, portability of the framework depends on its extensibility. The semantic hierarchy of this image description framework can be used for description of any digital images. The main differences lie in the varied semantic description content resulting from the change of concept systems related to specific fields, as well as the change in image metadata according to different application requirements. Spatial relations, scene and sentiment semantic hierarchies have generality in different domains. The object and behavior hierarchy, however, partly depend on a specific domain. This framework can be used in interdisciplinary applications by changing the domain concept system, and redefining relations between objects. </p>

<div class="divider-dot">&nbsp;</div>
<h4>5.4 Future work</h4>

<p>In this paper, we propose a semantic description method for Dunhuang fresco digital images. This method has already been applied to some image samples in our project as a proof of concept, and in the future, more digital images of Dunhuang frescoes will be described according to this approach.  An image description framework is not unique when we describe images in actual processes. We need to weigh the pros and cons of various factors when determining the weight of each part of this framework according to demands and different applications. This is especially true in task or domain oriented applications. Moreover, an image description process may be not completely finished according to this framework. We need to design image annotation tools with extended interfaces to ensure sharing and extension of descriptions relevant to different applications in the future.</p>

<p>In the next stage we will design annotation tools for cultural digital images according to the digital image description framework described in this paper. We will focus on the specific organizational form of image annotation information by referencing the <a href="http://www.openannotation.org/spec/core/">Open Annotation Data Model</a>.</p>

<div class="divider-full">&nbsp;</div>
<h3>Acknowledgements</h3>

<p>This research is funded by National key basic research development plan of China (973 plan, 904171200). This research is also partly supported by Youth talent support plan of China.</p>

<div class="divider-full">&nbsp;</div>
<h3>References</h3>

<p><a name="1">[1]</a> Schreibman, S., Siemans, R., &amp; Unsworth, J. (Eds.). (2004). A companion to digital humanities. Malden, MA: Blackwell.</p>

<p><a name="2">[2]</a> Benjamins, V. R., Contreras, J., Bl&#225;zquez, M., Dodero, J. M., Garcia, A., Navas, E.,  &amp; Wert, C. (2004). Cultural heritage and the semantic web. In The Semantic Web: Research and Applications (pp. 433-444). Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/978-3-540-25956-5_30">http://doi.org/10.1007/978-3-540-25956-5_30</a></p>

<p><a name="3">[3]</a> Martin Foys, Shannon Bradshaw. (2011). <a href="http://www.digitalmedievalist.org/journal/7/foys/">Developing Digital Mappaemundi: An Agile Mode for Annotating Medieval Maps</a>, Digital Medievalist 7.</p>

<p><a name="4">[4]</a> Sanderson, R., Albritton, B., Schwemmer, R., &amp; Van de Sompel, H. (2011, June). Sharedcanvas: a collaborative model for medieval manuscript layout dissemination. In Proceedings of the 11th annual international ACM/IEEE joint conference on Digital libraries (pp. 175-184). ACM. <a href="https://doi.org/10.1145/1998076.1998111">http://doi.org/10.1145/1998076.1998111</a></p>

<p><a name="5">[5]</a> Yang, C., Dong, M., &amp; Hua, J. (2006). Region-based image annotation using asymmetrical support vector machine-based multiple-instance learning. In Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on (Vol. 2, pp. 2057-2063). IEEE. <a href="https://doi.org/10.1109/CVPR.2006.250">http://doi.org/10.1109/CVPR.2006.250</a></p>

<p><a name="6">[6]</a> Carneiro, G., Chan, A. B., Moreno, P. J., &amp;l Vasconcelos, N. (2007). Supervised learning of semantic classes for image annotation and retrieval. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(3), 394-410. <a href="https://doi.org/10.1109/TPAMI.2007.61">http://doi.org/10.1109/TPAMI.2007.61</a></p>

<p><a name="7">[7]</a> J&#246;rgensen, C. (1998). <a href="http://www.sciencedirect.com/science/article/pii/S0306457397000770">Attributes of images in describing tasks</a>. Information Processing &amp; Management, 34(2), 161-174.</p>

<p><a name="8">[8]</a> Cunningham, S. J., Bainbridge, D., &amp; Masoodian, M. (2004, June). How people describe their image information needs: A grounded theory analysis of visual arts queries. In Proceedings of the 4th ACM/IEEE-CS joint conference on Digital libraries (pp. 47-48). ACM. <a href="https://doi.org/10.1145/996350.996362">http://doi.org/10.1145/996350.996362</a></p>

<p><a name="9">[9]</a> Shatford, S. (1986). <a href="http://www.tandfonline.com/doi/abs/10.1300/J104v06n03_04">Analyzing the subject of a picture: a theoretical approach</a>. Cataloging &amp; classification quarterly, 6(3), 39-62.</p>

<p><a name="10">[10]</a> Jaimes, A., &amp; Chang, S. F. (2000). <a href="http://www.ee.columbia.edu/ln/dvmm/publications/00/ajaimes-spie00_internet.pdf">A conceptual framework for indexing visual information at multiple levels</a>. IS&amp;T/SPIE Internet Imaging, 3964, 2-15.</p>

<p><a name="11">[11]</a> Batley, S. (1988). <a href="http://cat.inist.fr/?aModele=afficheN&amp;cpsidt=6582626">Visual information retrieval: browsing strategies in pictorial databases</a>. In International online information meeting. 12 (pp. 373-381). </p>

<p><a name="12">[12]</a> Hollink, L., Schreiber, A. T., Wielinga, B. J., &amp; Worring, M. (2004). Classification of user image descriptions. International Journal of Human-Computer Studies, 61(5), 601-626. <a href="https://doi.org/10.1016/j.ijhcs.2004.03.002">http://doi.org/10.1016/j.ijhcs.2004.03.002</a></p>

<p><a name="13">[13]</a> Epshtein, B., &amp; Uliman, S. (2005, October). Feature hierarchies for object classification. In Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on (Vol. 1, pp. 220-227). IEEE. <a href="https://doi.org/10.1109/ICCV.2005.98">http://doi.org/10.1109/ICCV.2005.98</a></p>

<p><a name="14">[14]</a> Fidler, S., &amp; Leonardis, A. (2007, June). Towards scalable representations of object categories: Learning a hierarchy of parts. In Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on (pp. 1-8). IEEE. <a href="https://doi.org/10.1109/CVPR.2007.383269">http://doi.org/10.1109/CVPR.2007.383269</a></p>

<p><a name="15">[15]</a> Ahuja, N., &amp; Todorovic, S. (2007, October). Learning the Taxonomy and Models of Categories Present in Arbitrary Images. In ICCV (pp. 1-8). <a href="https://doi.org/10.1109/ICCV.2007.4409039">http://doi.org/10.1109/ICCV.2007.4409039</a></p>

<p><a name="16">[16]</a> Rege, M., Dong, M., &amp; Fotouhi, F. (2007). Building a user-centered semantic hierarchy in image databases. Multimedia systems, 12(4-5), 325-338. <a href="https://doi.org/10.1007/s00530-006-0049-6">http://doi.org/10.1007/s00530-006-0049-6</a></p>

<p><a name="17">[17]</a> Eric Maillot, N., &amp; Thonnat, M. (2008). Ontology based complex object recognition. Image and Vision Computing, 26(1), 102-113. <a href="https://doi.org/10.1016/j.imavis.2005.07.027">http://doi.org/10.1016/j.imavis.2005.07.027</a></p>

<p><a name="18">[18]</a> Eakins, J. P. (2002). Towards intelligent image retrieval. Pattern Recognition, 35(1), 3-14. <a href="https://doi.org/10.1016/S0031-3203(01)00038-3">http://doi.org/10.1016/S0031-3203(01)00038-3</a></p>

<p><a name="19">[19]</a> Fan, J., Gao, Y., &amp; Luo, H. (2007, July). Hierarchical classification for automatic image annotation. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval (pp. 111-118). ACM. <a href="https://doi.org/10.1145/1277741.1277763">http://doi.org/10.1145/1277741.1277763</a></p>

<p><a name="20">[20]</a> Gao, Y., &amp; Fan, J. (2006, October). Incorporating concept ontology to enable probabilistic concept reasoning for multi-level image annotation. In Proceedings of the 8th ACM international workshop on Multimedia information retrieval (pp. 79-88). ACM. <a href="https://doi.org/10.1145/1178677.1178691">http://doi.org/10.1145/1178677.1178691</a></p>

<p><a name="21">[21]</a> Fan, J., Gao, Y., Luo, H., &amp; Jain, R. (2008). Mining multilevel image semantics via hierarchical classification. Multimedia, IEEE Transactions on, 10(2), 167-187. <a href="https://doi.org/10.1109/TMM.2007.911775">http://doi.org/10.1109/TMM.2007.911775</a></p>

<p><a name="22">[22]</a> Tousch, A. M., Herbin, S., &amp; Audibert, J. Y. (2012). Semantic hierarchies for image annotation: A survey. Pattern Recognition, 45(1), 333-345. <a href="https://doi.org/10.1016/j.patcog.2011.05.017">http://doi.org/10.1016/j.patcog.2011.05.017</a></p>

<p><a name="23">[23]</a> Eakins, J. P. (1996, May). <a href="http://www.cs.uu.nl/docs/vakken/mir/materials/literature/eakins.pdf">Automatic image content retrieval-are we getting anywhere?</a>. In ELVIRA-PROCEEDINGS (pp. 121-134).</p>

<p><a name="24">[24]</a> Liang Guanyu. (2000). <a href="http://cdmd.cnki.com.cn/Article/CDMD-80132-2006191018.htm">Storage and Retrieval for the Archive Data and Fresco Image of MoGao Grotto</a>. Institute of computing technology of the Chinese academy of sciences.</p>

<p><a name="25">[25]</a> Ji XianLin. (1998). Dunhuang studies dictionary.Shanghai dictionaries press.</p>

<p><a name="26">[26]</a> Bulthoff, H. H., Lee, S. W., Poggio, T. A., &amp; Wallraven, C. (2003). <a href="http://link.springer.com/content/pdf/10.1007/3-540-36181-2.pdf">Biologically motivated computer vision</a>. Springer-Verlag.</p>

<p><a name="27">[27]</a> Ekman, P., Friesen, W. V., &amp; Ellsworth, P. (1972). Emotion in the human face: Guidelines for research and an integration of findings.</p>

<p><a name="28">[28]</a> Zheng Binglin, Sha Wutian. (2005). Introduction to Dunhuang grottoes. art Gansu Culture Press.</p>

<p><a name="29">[29]</a> Cockburn, A., Karlson, A., &amp; Bederson, B. B. (2008). A review of overview+ detail, zooming, and focus+ context interfaces. ACM Computing Surveys (CSUR), 41(1), 2. <a href="https://doi.org/10.1145/1456650.1456652">http://doi.org/10.1145/1456650.1456652</a></p>

<p><a name="30">[30]</a> Zheng Ruzhong, Tai Jianqun. (2002). Dunhuang grottoes corpora: Apsaras picture scroll. The Commercial Press.</p>

<p><a name="31">[31]</a> Beaudoin, J. E. (2012). A Framework for Contextual Metadata Used in the Digital Preservation of Cultural Objects. <i>D-Lib Magazine</i>, 18(11), 2. <a href="https://doi.org/10.1045/november2012-beaudoin2">http://doi.org/10.1045/november2012-beaudoin2</a></p>

<div class="divider-full">&nbsp;</div>
<h3>About the Authors</h3>

<table border="0"  cellpadding="6" bgcolor="#FFFFFF"> 
<tr>
<td align="center"><img src="xu.jpg" class="border" alt="xu" width="100" height="115" /></td>
<td>
<p class="blue"><b>Lei Xu</b> is a postdoctoral researcher at the School of Information Management of Wuhan University in China. He received his PhD degree from the same university in May 2014. His research interests include ontology development and evaluation, complex networks, semantic web, and digital libraries.</p>
</td>
</tr>
</table>

<div class="divider-full">&nbsp;</div>

<table border="0"  cellpadding="6" bgcolor="#FFFFFF"> 
<tr>
<td align="center"><img src="wang.jpg" class="border" alt="wang" width="100" height="124" /></td>
<td>
<p class="blue"><b>Xiaoguang Wang</b> is a Professor at the School of Information Management of Wuhan University in China. He currently works on Semantic Publication and Knowledge Discovery. His interests include digital humanities, scientific knowledge, and networks.</p>
</td>
</tr>
</table>

<div class="divider-full">&nbsp;</div>


 <!-- Standard Copyright line here  -->

<div class="center">

<p class="footer">Copyright &copy; 2015 Lei Xu and Xiaoguang Wang</p>  
  </div>
</td>
 </tr>
</table>

<table width="100%" border="0" align="center" cellpadding="0" cellspacing="0">
  <tr>
    <td height="1" bgcolor="#2b538e"><img src="../../../img2/transparent.gif" alt="transparent image" width="100" height="2" /></td>
  </tr>
</table>

</td></tr></table>
</td></tr></table>
</form>

</body>
</html>