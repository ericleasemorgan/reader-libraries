<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
			"http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<!-- Formatting 02/9/01; CE done, bw; proofed, cb; proofed, ma; spell check done; Sayeed's changes made; timmo's changes made.  -->
<head>

<title>Strike Up the Score: Deriving Searchable and Playable Digital Formats from Sheet Music</title>

	<link rel="metadata" href="02choudhury.meta.xml">
<link rel="stylesheet" type="text/css" href="../style/main.css" title="Default Style Sheet"> 
	
	<meta name="DOI" content="10.1045/february2001-choudhury"> 
	<meta HTTP-EQUIV="content-type" content="text/html; CHARSET=iso-8859-1"> 
	<meta name="description" content="D-Lib Magazine">  
	<meta name="keywords" content="D-Lib Magazine, Digital Libraries, Digital Library Research">
</head>


<body bgcolor="#ffffff">
<div class="center">
<table width="700" border="0" cellspacing="0" cellpadding="0">
   
   <tr>
       <td height="20" colspan="2" valign="TOP" bgcolor="#ffffff">

<table width="700" border="0" cellspacing="0" cellpadding="0" class="banner">
	
	
	<col width="700">
	
    <tr>
        
        <td class="center">
        <a class="menu" href="../../../Architext/AT-dlib2query.html" target="_top">Search &nbsp;|</a>
        &nbsp;&nbsp;
        <a class="menu" href="../../../back.html" target="_top">Back Issues &nbsp;|</a>
        &nbsp;&nbsp;
        <a class="menu" href="../../../author-index.html" target="_top">Author Index &nbsp;|</a>
        &nbsp;&nbsp;
        <a class="menu" href="../../../title-index.html" target="_top">Title Index &nbsp;|</a>
        &nbsp;&nbsp;
        <a class="menu" href="../02contents.html" target="_top">Contents</a>
        </td>
    </tr>
</table>
    </td></tr>
</table>

<br>
<img src="../images/articles00.gif" width="500" height="16" alt="Articles">
</div>

<!-- Begin Article Header -->

<table border="0" cellpadding="0" cellspacing="0" width="100%">
<colgroup>
	<col width="6%">
	<col width="94%">
</colgroup>
	<tr>
		<td><img src="../images/spacer00.gif" width="10" height="10" alt="spacer"></td>
		<td><h3 class="blue">D-Lib Magazine<br>February 2001</h3>
		<h6 class="blue">Volume 7 Number 2<br><br>
        ISSN 1082-9873</h6>
        <h2 class="blue">Strike Up the Score</h2>
<H3 class="blue">Deriving Searchable and Playable Digital Formats from Sheet Music
</H3>


		</td>
	</tr>

	<tr>
		<td>&nbsp; </td>
		<td> <p class="blue">
<a href="../authors/02authors.html#Choudhury">G. Sayeed Choudhury</a><br>
<a href="../authors/02authors.html#DiLauro">Tim DiLauro</a><br>
Digital Knowledge Center<br>
Milton S. Eisenhower Library<br>
Johns Hopkins University<BR>
<em><A HREF="https://www.dlib.org/cdn-cgi/l/email-protection#8af9ebf3efefeecae0e2ffa4efeeff"><span class="__cf_email__" data-cfemail="1b687a627e7e7f5b71736e357e7f6e">[email&#160;protected]</span></A></em><br>
<em><A HREF="https://www.dlib.org/cdn-cgi/l/email-protection#b1c5d8dcdcdef1dbd9c49fd4d5c4"><span class="__cf_email__" data-cfemail="2054494d4d4f604a48550e454455">[email&#160;protected]</span></A></em></p>

<p class="blue">
<a href="../authors/02authors.html#Droettboom">Michael Droettboom</a><br>
<a href="../authors/02authors.html#Fujinaga">Ichiro Fujinaga</a><br>
<a href="../authors/02authors.html#MacMillan">Karl MacMillan</a><br>
Peabody Conservatory of Music<br>
Johns Hopkins University<BR>
<em><A HREF="https://www.dlib.org/cdn-cgi/l/email-protection#6d00090f0202002d1d080c0f0209144307051843080918"><span class="__cf_email__" data-cfemail="224f46404d4d4f62524743404d465b0c484a570c474657">[email&#160;protected]</span></A></em><br>
<em><A HREF="https://www.dlib.org/cdn-cgi/l/email-protection#8ae3e9e2cafaefebe8e5eef3a4e0e2ffa4efeeff"><span class="__cf_email__" data-cfemail="2b4248436b5b4e4a49444f520541435e054e4f5e">[email&#160;protected]</span></A></em><br>
<em><A HREF="https://www.dlib.org/cdn-cgi/l/email-protection#016a60736d6c606241716460636e65782f6b69742f646574"><span class="__cf_email__" data-cfemail="dab1bba8b6b7bbb99aaabfbbb8b5bea3f4b0b2aff4bfbeaf">[email&#160;protected]</span></A></em></p>
		 </td>
 	</tr>
</table>

<div class="center">
<p><img src="../images/redline00.gif" width="500" height="2" alt="Red Line"></p>
</div>

<!-- Story goes next -->

<table border="0" cellpadding="0" cellspacing="0" width="90%">
<colgroup>
	<col width="4%">
	<col width="96%">
</colgroup>

	<tr>
		<td><img src="../images/spacer00.gif" width="10" height="10" alt="spacer"></td>
		<td>
<!-- Abstract or TOC goes here -->


<!-- <h3 class="blue">Abstract</h3> -->

<!-- <p class="blue"> text</p> -->

<!-- Text of Story Starts Here -->

<H3>Introduction</H3>

<table align=right border=0 cellpadding=10 cellspacing=0 width=315>
<td width=5><br></td>
<td width=310>
<p><img src="FamousAmericanSongs.cover.JPEG" width="305" border=0 height="397" alt="Image of the cover of a piece of sheet music"></p>
<p align=center><strong>Figure 1. A cover sheet from the Levy Collection.</strong></p>
</td>

</tr>
</table>

<p>The Lester S. Levy Collection of Sheet Music represents one of the largest collections of sheet music available online.<sup><a href="02choudhury.html#note1">1</a></sup>  The Collection, part of the Special Collections<sup><a href="02choudhury.html#note2">2</a></sup> of the Milton S. Eisenhower Library (MSEL) at Johns Hopkins University, comprises nearly 30,000 pieces of music which correspond to nearly 130,000 sheets of music and associated cover art.  It provides a rich, multi-faceted view of life in late 19th and early 20th century America, featuring famous songs such as &quot;The Star-Spangled Banner&quot;, &quot;Hail Columbia&quot;, and &quot;Yankee Doodle Dandy&quot; along with engravings, lithographs, and many forms of early photo reproduction on song covers.  Scholars from various disciplines have used the Collection for both research and teaching; the online collection, described below, has proven popular with the general public as well.  In the early 1990s, the MSEL considered the need for preservation of the Collection, while respecting the need for continued access.  Accordingly, the MSEL evaluated two ideas to meet the dual goals of enhancing access while reducing the handling of the physical collection-microfilming and digitization.</p>
<p>With funding from the National Endowment for the Humanities (NEH)<sup><a href="02choudhury.html#note3">3</a></sup> in 1994, the Milton S. Eisenhower Library began the process of digitizing the Levy Collection. While there is now a reasonable amount of experience with digitization of library collections, this was not the case in 1994.  Not only is the Levy Collection a relatively large online collection, it is also one of the first major digitization efforts by an academic research library.  The Levy (Phase I) Project team initially hired a subcontractor to implement and manage the digitization.   Both the subcontractor and the Levy team realized some rather &quot;painful&quot; lessons regarding large-scale digitization projects.  The workload associated with digitizing the Levy Collection, especially the process of inspecting, editing, and correcting images and attaching appropriate metadata, proved onerous and overwhelming.  In fact, the subcontractor declared bankruptcy, leaving the responsibility for completing the digitization with the Levy team.</p> 
<p>In the final report to NEH, the Curator of Special Collections at the MSEL stated, &quot;the most useful thing we learned from this project was that you can never overestimate the amount of time it will take to create a quality digital product&quot; (<a href="02choudhury.html#Requardt">Requardt</a> 1998). The word &quot;resources&quot; might represent a more comprehensive choice than the word &quot;time&quot; in this previous statement.  This &quot;sink&quot; of time and resources manifested itself by an increasing allocation of human labor and time to deal with workflow issues related to large-scale digitization.  The Levy Collection experience provides ample evidence that there will be mistakes during and after digitization and that unforeseen challenges or difficulties will arise, especially when dealing with rare or fragile materials.  The current strategy of allocating additional human labor neither limits costs nor scales well.</p>
<p>Consequently, the Digital Knowledge Center (DKC)<sup><a href="02choudhury.html#note4">4</a></sup> of the Milton S. Eisenhower Library sought and secured funding for the development of a workflow management system through the National Science Foundation’s (NSF)<sup><a href="02choudhury.html#note5">5</a></sup> Digital Libraries Initiative, Phase 2 and the Institute for Museum and Library Services (IMLS)<sup><a href="02choudhury.html#note6">6</a></sup> National Leadership Grant Program.  The Levy family and a technology entrepreneur in Maryland provided additional funding for other aspects of the project.</p>
<p>The mission of this second phase of the Levy project (&quot;Levy II&quot;) can be summarized as follows:</p>
<ul><li>Reduce costs for large collection ingestion by creating a suite of open-source processes, tools and interfaces for workflow management</li></ul>
<ul><li>Increase access capabilities by providing a suite of research tools</li></ul>
<ul><li>Demonstrate utility of tools and processes with a subset of the online Levy Collection</li></ul>
<p>The cornerstones of the workflow management system include: optical music recognition (OMR) software to generate a logical representation of the score -- for sound generation, musical searching, and musicological research -- and an automated name authority control system to disambiguate names (e.g., the authors Mark Twain and Samuel Clemens are the same individual).  The research tools focus upon enhanced searching capabilities through the development and application of a fast, disk-based search engine for lyrics and music, and the incorporation of an XML structure for metadata.</p>
<p>Though this paper focuses on the OMR component of our work, a companion paper to be published in a future issue of D-Lib will describe more fully the other tools (e.g., the automated name authority control system and the disk-based search engine), the overall workflow management system, and the project management process.</p>
<H3>Optical Music Recognition</H3>
<p>The philosophy of the Levy II project comprises the reduction, not elimination, of human intervention in the workflow of a large-scale digitization project.  Our optical music recognition (OMR) software exemplifies this philosophy.  Compared to commercial OMR systems, it offers several advantages which are highly relevant in the context of the Levy II workflow management system.  The OMR software is:</p>
<ul><li>Open-source</li></ul>
<ul><li>Platform independent</li></ul>
<ul><li>Able to run in batch mode</li></ul>
<ul><li>Able to extract lyrics</li></ul>
<ul><li>Able to produce a searchable logical representation of the music</li></ul>
<ul><li>Able to &quot;learn&quot; different types of musical and other symbols -- an attribute that expresses the adaptive nature of this software</li></ul>
<p>The entire OMR process begins with graphic images for its input and eventually produces both GUIDO (<a href="02choudhury.html#Hoos">Hoos and Hamel</a> 1997) and MIDI (<a href="02choudhury.html#MIDI">MIDI</a> 1986) files, along with a file of full-text lyrics.  GUIDO and MIDI represent non-proprietary file formats. GUIDO allows the encoding of many different layers of detail in a musical score, including both its auditory and visual aspects.  This allows for seamless interchange between music notation editing applications. MIDI provides low-bandwidth transmission of music over the Internet so users can listen to music with their web browsers (<a href="02choudhury.html#Choudhury">Choudhury, et al</a>. 2000).</p>
<p>Our optical music recognition (OMR) system consists of two components:</p>
<blockquote><strong><em>Adaptive Optical Music Recognition (AOMR) System </em></strong>(<a href="02choudhury.html#Fujinaga-b">Fujinaga</a> 1996b). AOMR is responsible for recognizing and classifying musical symbols (e.g., staves, clefs, notes) from a page image.</blockquote>
<blockquote><strong><em>Optical Music Interpretation (OMI) System</em></strong> (<a href="02choudhury.html#Droettboom">Droettboom and Fujinaga</a> 2001). OMI organizes and interprets the symbols from AOMR into their musical &quot;meanings&quot;. For example, a half note on the third staff line, preceded by a bass clef with no key signature and a 4/4 time signature, &quot;means&quot; to play a D for two beats.</blockquote>
<H3><em>Optical Music Recognition Overview</em></H3>
<p>Before providing the details of the OMR process, which may be difficult to understand without a background in music, it will be helpful to present a general summary. </p>
<p>Before batch processing can begin, AOMR must be &quot;trained&quot; to recognize the various symbols.  A human feeds pages from the training set to AOMR and makes corrections in the training interface when AOMR makes classification mistakes.  Once this process is completed, the training data is fed into a genetic algorithm, which &quot;tunes&quot; the training data to improve accuracy.</p>
<p>Once the initial training is completed, the system is ready to perform batch processing on a large scale. The AOMR process is performed for each music page image in the collection:</p>
<ul><li>Simple and pervasive symbols, such as staff lines, text, stems, and black noteheads, are detected and classified using basic image processing techniques</li></ul>
<ul><li>Other symbols are classified using an adaptive algorithm</li></ul>
<p>The overall architecture of the adaptive optical music recognition (AOMR) system is depicted in the figure below.</p>
<p align="center"><img src="omr-chart.gif" width="426" border=0 height="506" alt="Architecture of the adaptive learning algorithm"><br><br>
<strong>Figure 2. Overview of the adaptive learning algorithm. The feature data gleaned during training is represented by the &quot;Knowledge Base&quot; symbol.</strong>
</p>

<p>The OMI process is executed once for each complete musical score, which might comprise multiple pages.  The Levy Collection, for example, averages about four music pages per piece.</p>
<ul><li>OMI associates each symbol with the appropriate staff, sorts the symbols in temporal (time) order, and determines relationships between the symbols</li></ul>

<ul><li>OMI uses this information to perform syntactic and semantic analysis, possibly correcting some errors from the AOMR process</li></ul>

<ul><li>Finally, OMI creates output containing a representation of the music</li></ul>

<p>An online guided tour of the AOMR/OMI process is available.<sup><a href="02choudhury.html#note7">7</a></sup>

<H3><em>Optical Music Recognition in Detail</em></h3>

<H4>Training Phase</H4>
<p>
AOMR is pre-configured with basic knowledge, but will perform better with a short period of supervised learning on a representative training set selected from the collection to be processed.  In this interactive session, AOMR processes pages normally (as described in the next section), but is corrected by a human each time it makes a mistake.  The list of matching features and the associated symbol classifications are saved for the exemplar-based symbol classifier and for later processing by a genetic algorithm (GA).  Once all the pages from the training set have been processed, the GA can be applied to the training data.  The role of the GA is to pick a near-optimal weight (or relative importance) for each of the features used in the k-nearest neighbor (k-NN) classifier.  This data is stored for later use by AOMR.  GA's are often used when an exhaustive search of appropriate weights is impossible or prohibitive (<a href="02choudhury.html#Fujinaga-a">Fujinaga</a> 1996a).  After the training has been completed and weights have been assigned to each of the features, AOMR is ready to process all the pages in the collection.</p>

<p><strong>Adaptive Optical Music Recognition Process</strong><br>
The AOMR program starts by loading a graphic image of the musical page into memory (Figure 3).  The recognition process begins by finding the &quot;easiest&quot; symbols and symbol components first.  The algorithms used to recognize these symbols use standard image processing techniques and do not rely on the k-NN classifier.  The idea is to reduce the sample size and the number of features necessary for the k-NN classifier to recognize the more difficult shapes.  This reduces the processing time required by both the classifier and the genetic algorithm.</p>


<p align=center> <img src="original-1024.gif" width="293" border=0 height="219" alt="Graphic image of a page of music"><br><br>
<strong>Figure 3. Graphic image of a page of music.</strong>
</p>


<p>
Because staff lines are pervasive, relatively easy to locate, and because they make it difficult to recognize other symbols, they are removed first.  They are identified by organizing the image into lists of vertical &quot;runs&quot; of pixels with a technique known as run-length coding, and then using projection analysis to locate short vertical runs with long horizontal widths.  If the staff lines do not project straight across the page, then the page is said to be skewed.  Using the information gathered in this process, the image is de-skewed (or straightened) and the staff lines are removed. Stave coordinates and line spacing data is stored for later use. <em>Figure 4 below has staff lines removed</em>.</p>

<p align=center>
<img src="nostaves-1024.gif" width="293" border=0 height="219" alt="Page of music with the staff lines removed"><br><br>
<strong>Figure 4. Page of music with the staff lines removed.</strong>
</p>

<p>
Text, including performance indications and lyrics, is removed using some heuristics (e.g., aspect ratio, minimum height and width, and side-by-side placement).  Runs of text are removed and stored as bounding box coordinates so that the text tokens may be fed to an OCR process later.  This will allow the capture of lyrics, multi-letter dynamic markings, and other performance information. <em>Text has been removed from the image in Figure 5 below.</em>.</p>

<p align=center>
<img src="notext-1024.gif" width="293" border=0 height="219" alt="Page of music with the text removed"><br><br>
<strong>Figure 5. Page of music with the text removed.</strong> 
</p>


<p>Using run-length coding and connected component analysis (<a href="02choudhury.html#Fujinaga-b">Fujinaga</a> 1996b), stems, barlines, and solid (black) noteheads are detected and removed. Tall narrow elements indicate stems and barlines, while &quot;square-ish&quot; components with a preponderance of black represent solid noteheads. <em>The remaining symbols (Figure 6) will be handled by the k-NN classifier</em>.</p>

<p align=center>
<img src="pre-k-NN-1024.gif" width="293" border=0 height="244" alt="Page of music with stems, barlines, and solid noteheads removed"><br><br>
<strong>Figure 6. Page of music with stems, barlines, and solid noteheads removed.</strong>
</p>

<p>After the easier symbols have been removed, connected component analysis is performed on the new intermediate image to segment the remaining symbols and marks.  Each of the symbols is classified by the k-NN algorithm.  The k-NN algorithm is used to implement the exemplar-based classifier that makes AOMR adaptive.  k-NN works by calculating the distance between the feature vector of  the unclassified symbol and the feature vector of each of the symbols manually classified during the training phase.  The class indicated by most of the closest neighbors is assigned to the symbol being tested.</p>
<p>After the remaining symbols have been processed, all of the symbols collected by AOMR (staves, text, noteheads, clefs, etc.) are stored for later interpretation by OMI.  This process is repeated for each page in a musical score.</p>
<H4>Optical Music Interpretation Process</H4>
<p>Now that AOMR has processed all the pages of a score, all the symbols from that score are available for interpretation.  OMI begins by reading the text and symbol classification information provided by AOMR. OMI refers to these text and symbol objects as glyphs.  The content of each of the text tokens is extracted from the original image.  We plan to implement an interface to send these tokens' images to an external OCR program.  Once that interface is completed, both the text image and OCR results will be attached to the corresponding text glyph.</P>  
<p>Next, the glyphs need to be sorted into temporal, or time, order.  In order to do this, and to establish relationships with other glyphs, each glyph must be associated with a staff.  The glyphs are sorted in time order, by part, voice, and staff.  Glyphs that occur at the same vertical position on the same staff (as in a chord) are ordered from top to bottom. Doing so makes melodic searching easier, since the melody is more likely to follow the highest pitches.</p>
<p>With the glyphs organized in temporal order by staff, it is now possible to determine relationships between glyphs.  These connections are important for inferring the musical meaning of related sets of symbols. To facilitate this, OMI implements an object-oriented class hierarchy in which the base classes are named by the abilities or properties that we wish to project onto the derived glyph classes.  For example, all noteheads can have a stem, and thus are derived from the STEMMABLE class, which is in turn derived from several other classes, and so on.  This scheme allows OMI to join a stem to an appropriate and properly adjoining object by asking that object if it can have a stem.  A notehead object will indicate that it can, and the relationship will be established.  Rests, however, are not derived from the STEMMABLE class, thus any rest in similar proximity would tell OMI that it cannot be stemmed.  This technique allows the reference assignment code to work in a highly abstract way, on a wide variety of glyph classes.  An example serves to illustrate the importance of the interaction between the various glyphs.</p>
<blockquote>A quarter note followed by a dot should be treated as a single logical entity.  The dot augments the quarter note’s duration by half, so clearly it is important to know the relationship of these two glyphs.  However, we will not know the true time duration of this dotted quarter note until we determine time-related context information -- such as the tempo and time signature -- that is in effect when this glyph appears.  Further, we will not know which pitch to voice until we determine the clef (e.g., treble, base, alto); the notehead’s position relative to the staff; the last preceding accidental (e.g., sharp, flat, natural) for that note in the same measure; and, in the absence of any accidentals, the key signature.</blockquote>
<p>Armed with the full musical meaning of the collection of glyphs, OMI can apply rules to detect errors in the music and possibly correct them.  For example, under most circumstances OMI can detect and sometimes correct errors that result in too few or too many beats occurring in a measure.  A group of seven algorithms implements this functionality.  In the future, new rules may be implemented to further improve (or at least measure) the accuracy of the process, and further reduce the amount of human intervention required.</p>
<p>Finally, OMI produces a representation of the music as output.  Furthermore, because OMI implements a plug-in architecture for output processing, the results can be tailored to the needs of the user.  Output methods for each glyph class are merged from the backend into the core glyph classes.  Since output formats differ in the ordering and encoding of musical objects, the output processor is first given the opportunity to reorder the glyph list before the output method is called for each glyph.</p>
<p>We have chosen to use GUIDO in our project; consequently, an output plug-in has been developed for this format.  However, other output processors are planned.  GUIDO was chosen because of its representational completeness (auditory, visual, and logical aspects of the score), its non-proprietary definition, and its text-based, human-readable syntax.</p>
<H3><em>Optical Character Recognition</em></h3>
<p>As stated above, AOMR extracts the text tokens, when present, from the images of sheet music.  This feature represents an advantage over commercial OMR systems and provides the possibility to incorporate automated tools for dealing with full-text lyrics.  Our team considered and evaluated optical character recognition (OCR) software for this purpose, focusing on performance and integration into the open-source workflow management system.  The OCR software was used to recognize and export text strings from the text tokens that were generated by AOMR from the sheet music images.</p>  
<p>We identified several open-source OCR systems, and felt two packages offered the most mature systems, SOCR<sup><a href="02choudhury.html#note8">8</a></sup> and GOCR.<sup><a href="02choudhury.html#note9">9</a></sup>   We were unable to compile SOCR, but were able to compile and perform tests using GOCR.  Commercial OCR software was also considered, based on a hypothesis that commercial software might offer superior performance (e.g., accuracy).  The Perseus Project<sup><a href="02choudhury.html#note10">10</a></sup> team has identified Prime Recognition<sup><a href="02choudhury.html#note11">11</a></sup>   as effective OCR software; however, it runs only on proprietary operating systems (Windows NT, version 4.0 or Windows 2000).  Consequently, we evaluated OCR Shop<sup><a href="02choudhury.html#note12">12</a></sup> and Pixel!OCR,<sup><a href="02choudhury.html#note13">13</a></sup> two commercial OCR packages that run on Linux.</p>
<p>OCR Shop had difficulty with the text tokens.  Specifically, it performed well with test images with &quot;standard&quot; amounts of text (i.e., the typical number of words found on an average page), but locked up with the tokens.  Pixel!OCR managed to recognize lyrics from the text tokens and performed better than GOCR. Though Pixel!OCR runs on Linux and outperformed GOCR, we still hoped that a fully open-source option would be possible.</p>
<p>Given the nature of the text tokens, we speculated that standard OCR software faced problems with syllables and other &quot;incomplete&quot; words.  OCR Shop’s good performance with standard images of text and poor performance with AOMR text tokens reinforced this idea.  OCR Shop, like other OCR packages, seemed to search for words based on its recognition of individual letters.  In the case of AOMR-generated text tokens, often featuring only a syllable of a word, the software seemed to lack the necessary context to recognize the characters.</p>
<p>Realizing that AOMR recognizes individual symbols, we began to explore the possibility of using AOMR as an OCR system.  Based on preliminary tests, the AOMR system has outperformed GOCR, the open-source OCR system.  Further tests of AOMR, GOCR, and Pixel!OCR will be conducted.  However, the initial tests provided enough encouragement to pursue the AOMR as an OCR system for the Levy II project.  If AOMR performs well, it will be possible to retain the open-source nature of the workflow management system while ensuring a reasonable level of performance.</p>
<H3><em>OMR Deliverables</em></H3>
<p>After the overall optical music recognition process (AOMR and OMI), the following are available:</p>
<ul><li><strong><em>Text-based, logical representation of the score, in the form of GUIDO music notation</em></strong>.  The music notation data will be used for a variety of purposes. Currently, MIDI output is derived from GUIDO using the gmn2midi (<a href="02choudhury.html#Martin">Martin and Hoos</a> 1997) program. Rhythmic and melodic searching within a single score have been implemented.  Furthermore, we plan to index the GUIDO to develop music searching across the entire collection.</li></ul>
<ul><li><strong><em>Text tokens</em></strong>.  Text strings will be extracted from the image tokens via OCR.  Based on the location of the tokens within the score, OMI will assign the text as either performance information or lyrics.  Because lyrics are often presented as syllables instead of complete words, we plan to develop a process that uses natural language processing techniques to rebuild the complete words from the syllables.</li></ul>
<ul><li><strong><em>Confidence measure</em></strong>.  We plan to have our OMR process produce a measure of its confidence in the overall recognition and interpretation of the score.  That measure will be used within the workflow management framework to help collection managers determine when human intervention is required.</li></ul>
<H3>Conclusion</H3>
<p>Optical music recognition is a critical component for building strong online collections that include musical scores.  Without it, collection managers would have to either limit end-user searching to descriptive metadata or bear the expense of manually transcribing and encoding their collections to provide music searching, lyric searching, and online playback.</p>  
<p> Though critical, OMR is only one of the tools that will be integrated into the Levy II workflow management system. As was mentioned earlier, a companion paper will discuss our process for project planning, the automated name authority control tool, and the overall framework for the workflow management system itself.</p>
<H3>References</H3>
<A name="Choudhury"> </a>
<p>Choudhury, S., T. DiLauro, M. Droettboom, I. Fujinaga, B. Harrington and K. MacMillan. 2000. Optical music recognition system within a large-scale digitization project. ISMIR 2000 Conference. &lt;<a href="http://ciir.cs.umass.edu/music2000/papers/choudhury_paper.pdf">http://ciir.cs.umass.edu/music2000/papers/choudhury_paper.pdf</a>&gt;</p>

<A name="Droettboom"> </a>
<p>Droettboom, M. and I. Fujinaga. 2001. Interpreting the semantics of music notation using an extensible and object-oriented system. (To be published in Proceedings of the 9th Python Conference).</p>

<A name="Fujinaga-a"> </a>
<p>Fujinaga, I. 1996a. Exemplar-based learning in adaptive optical music recognition system. Proceedings of the International Computer Music Conference: 55-6.</p>

<A name="Fujinaga-b"> </a>
<p>Fujinaga, I. 1996b. Adaptive optical music recognition. Ph.D. thesis, McGill University.</p>

<A name="Hoos"> </a>
<p>Hoos, H. H. and K. A. Hamel. 1997. The GUIDO music notation format version 1.0, Specification Part 1: Basic GUIDO. Technical Report TI 20/97, Technische Universität Darmstadt.
&lt;<a href="http://www.informatik.tu-darmstadt.de/AFS/GUIDO/docu/spec1.htm">http://www.informatik.tu-darmstadt.de/AFS/GUIDO/docu/spec1.htm</a>&gt;
</p>

<A name="Martin"> </a>
<p>Martin, L. and H. H. Hoos. 1997. gmn2midi, version 1.0. Computer Program (Microsoft Windows, Apple Macintosh OS, IBM OS/2, UNIX). &lt;<a href="http://www.informatik.tu-darmstadt.de/AFS/GUIDO/">http://www.informatik.tu-darmstadt.de/AFS/GUIDO/</a>&gt;.</p>

<A name="MIDI"> </a>
<p>MIDI. 1986. The complete MIDI 1.0 specification. MIDI Manufacturers Association, Inc.  &lt;<a href="http://www.midi.org">http://www.midi.org/</a>&gt;</p>

<A name="Requardt"> </a>
<p>Requardt, C. 1998. Preservation of and automated access to the Lester S. Levy Collection of Sheet Music. NEH Final Performance Report PS-20945-95 (March). </p>

<H3>Notes</H3>
<A name="note1"> </a>
<p>[1] The Lester S. Levy Collection of Sheet Music, &lt;<a href="http://levysheetmusic.mse.jhu.edu">http://levysheetmusic.mse.jhu.edu/</a>&gt;</p>

<A name="note2"> </a>
<p>[2] Special Collections, Milton S. Eisenhower Library, &lt;<a href="http://archives.mse.jhu.edu:8000/">http://archives.mse.jhu.edu:8000/</a>&gt;</p>

<A name="note3"> </a>
<p>[3] National Endowment for the Humanities, &lt;<a href="http://www.neh.gov/">http://www.neh.gov/</a>&gt;</p>

<A name="note4"> </a>
<p>[4] Digital Knowledge Center, Milton S. Eisenhower Library, &lt;<a href="http://dkc.mse.jhu.edu/">http://dkc.mse.jhu.edu/</a></p>

<A name="note5"> </a>
<p>[5] National Science Foundation, &lt;<a href="http://www.nsf.gov">http://www.nsf.gov/</a>&gt;</p>

<A name="note6"> </a>
<p>[6] Institute of Museum and Library Services, &lt;<a href="https://www.imls.gov/">http://www.imls.gov/</a>&gt;</p>

<A name="note7"> </a>
<p>[7] An online guided tour of the AOMR/OMI process is available. &lt;<a href="http://mambo.peabody.jhu.edu/omr/demo/">http://mambo.peabody.jhu.edu/omr/demo/</a>&gt;</p>

<A name="note8"> </a>
<p>[8] SOCR optical character recognition, &lt;<a href="http://www.socr.org/">http://www.socr.org/</a>&gt;</p>

<A name="note9"> </a>
<p>[9] GOCR optical character recognition, &lt;<a href="http://jocr.sourceforge.net/">http://jocr.sourceforge.net/</a>&gt;</p>

<A name="note10"> </a>
<p>[10] The Perseus Project, &lt;<a href="http://www.perseus.tufts.edu/">http://www.perseus.tufts.edu/</a>&gt;</p>

<A name="note11"> </a>
<p>[11] Prime Recognition (PrimeOCR), &lt;<a href="http://www.primerecognition.com/">http://www.primerecognition.com/</a>&gt;</p>

<A name="note12"> </a>
<p>[12] Vividata (OCR Shop) optical character recognition software, 
&lt;<a href="http://www.vividata.com/">http://www.vividata.com/</a>&gt;</p>

<A name="note13"> </a>
<p>[13] Mentalix Pixel!OCR optical character recognition software, 
&lt;<a href="http://www.mentalix.com/">http://www.mentalix.com/</a>&gt;</p>





<!-- Standard Copyright line here -->

    <h6>Copyright&copy; 2001 G. Sayeed Choudhury, Tim DiLauro, Michael Droettboom, Ichiro Fujinaga, and Karl MacMillan</h6> 
    </td>
  </tr>

<!-- Begin the bottom sections -->

  <tr>
    <td><img src="../images/spacer00.gif" width="10" height="10" alt="spacer"></td>
    <td><hr width="80%" noshade size="1"></td>
  </tr>
  <tr>
    <td><img src="../images/spacer00.gif" width="10" height="10" alt="spacer"></td>
    <td>
    <p class="cbs">
    <a href="02choudhury.html#Top">Top</a> 
    | <a href="../02contents.html">Contents</a><br>
    <a href="../../../Architext/AT-dlib2query.html">Search</a> 
    | <a href="../../../author-index.html">Author Index</a> 
    | <a href="../../../title-index.html">Title Index</a> 
    | <a href="../../../back.html">Back Issues</a><br>
    <a href="../02bookreview.html">Book Review</a> 
 
    |<a href="../nelson/02nelson.html">Next Article</a><br>
    <a href="../../../dlib.html">Home</a>
    | <a href="https://www.dlib.org/cdn-cgi/l/email-protection#b3d7dfdad1f3d0ddc1da9dc1d6c0c7dcdd9dc5d29dc6c0">E-mail the Editor</a></p>
    </td>
  </tr>
  <tr>
    <td><img src="../images/spacer00.gif" width="10" height="10" alt="spacer"></td>
    <td><hr width="80%" noshade size="1"></td>
  </tr>
  <tr>
    <td><img src="../images/spacer00.gif" width="10" height="10" alt="spacer"></td>
    <td>
    <p class="small70"><a href="../../../access.html">D-Lib Magazine Access Terms and Conditions</a></p>
    <p class="small70"><a href="https://www.doi.org"><b>DOI</b></a>:
    10.1045/february2001-choudhury</p> 
    <p> &nbsp;</p></td>
  </tr>

</table>


<script data-cfasync="false" src="../../../cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script></body>
</html>
