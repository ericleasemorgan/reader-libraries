<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"   
                       "http://www.w3.org/TR/REC-html40/loose.dtd"> 
<html>    <!-- Formatting 12/6/06, bw; -->  
<head>  
   <title>The Music Information Retrieval Evaluation eXchange (MIREX)</title>   
            <link rel="metadata" href="12downie.meta.xml"> 
  <link rel="stylesheet" type="text/css" href="../style/main.css" title="Default Style Sheet">  <style type="text/css">
.lefttop				{border-top: 1px solid #000000;
					      border-left: 1px solid #000000;
						}
	        
.lefttopright		{border-top: 1px solid #000000;
	         		      border-left: 1px solid #000000;
	         		      border-right: 1px solid #000000;
						}

.lefttopbottom	{border-top: 1px solid #000000;
	         			  border-left: 1px solid #000000;
	         			  border-bottom: 1px solid #000000;
	        			}	        

.all	{border-top: 1px solid #000000;
	      border-left: 1px solid #000000;
	      border-bottom: 1px solid #000000;
	      border-right: 1px solid #000000;
	     }	
</style>                     <meta name="DOI" content="10.1045/december2006-downie"> 
            <meta HTTP-EQUIV="content-type" content="text/html; CHARSET=iso-8859-1"> 
         <meta name="description" content="D-Lib Magazine">   
          <meta name="keywords" content="D-Lib Magazine, Digital Libraries, Digital Library Research Open Archives Initiative, OAI"> 
  </head> 
      <body bgcolor="#ffffff"> 
    <div class="center">   
      <table border="0" cellspacing="0" cellpadding="0" align="center">  
                                          <tr>  
                  <td class="center"> 
      <p><font style="font-weight: bold;">
 <a class="menu" href="../../../Architext/AT-dlib2query.html" target="_top">Search</a>

 &nbsp; <a name="Top">|</a>  &nbsp; <a class="menu" href="../../../back.html" target="_top">Back Issues</a> 
&nbsp; |  &nbsp;             <a class="menu" href="../../../author-index.html" target="_top">Author Index</a> 
&nbsp; | &nbsp;           <a class="menu" href="../../../title-index.html" target="_top">Title Index</a> 
&nbsp; | &nbsp;            <a class="menu" href="../12contents.html" target="_top">Contents</a> 
 </font>
 </p> 
</td>
     </tr> 
 </table> 
   <p align="center"> 
 <img src="../images/conf-rpt00.gif" width="500" height="16" alt="Conference Report"></p> 
  </div> 
   <!-- Begin Article Header --> 
    <table border="0" cellpadding="0" cellspacing="0" width="100%">  
  <colgroup>  
          <col width="6%">   
         <col width="94%"> 
  </colgroup>   
         <tr>   
                 <td>
<img src="../images/spacer00.gif" width="10" height="10" alt="spacer">
</td>                
 <td> 
<h3 class="blue">D-Lib Magazine<br>December 2006</h3> 
                   <h6 class="blue">Volume 12 Number 12<br><br> 
 ISSN 1082-9873</h6>           
<h2 class="blue-space">The Music Information Retrieval Evaluation eXchange (MIREX)</h2> 

  </td>  
          </tr> 
            <tr>  
                  <td>&nbsp; </td>  
                  <td>  
  <p class="blue"><a href="../authors/12authors.html#DOWNIE">J. Stephen Downie</a><br>
Graduate School of Library and Information Science<br>
University of Illinois at Urbana-Champaign<br>
&lt;jdownie&#064;uiuc.edu&gt;
 </p> 
 
    </td>  
</tr> 
   </table>  
       <div class="center"> 
  <p><img src="../images/redline00.gif" width="500" height="2" alt="Red Line"></p> </div> 

      <!-- Story goes next -->     

  <table border="0" cellpadding="0" cellspacing="0" width="90%"> 
  <colgroup>   
 <col width="6%">   
 <col width="94%">   
</colgroup> 
 <tr> 

 <td><img src="../images/spacer00.gif" width="10" height="10" alt="spacer">
  </td>   
                
  <td>
 <!-- Abstract or TOC goes here -->  
   <!-- Report text starts here -->   

<h3>1. Introduction</h3>

<p>
The Music Information Retrieval Evaluation eXchange (MIREX) [<a href="12downie.html#1">1</a>] is a community-based formal evaluation framework coordinated and managed by the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) [<a href="12downie.html#2">2</a>] at the University of Illinois at Urbana-Champaign (UIUC). IMIRSEL has been funded by both the National Science Foundation and the Andrew W. Mellon Foundation to create the necessary infrastructure for the scientific evaluation of the many different techniques being employed by researchers interested in the domains of Music Information Retrieval (MIR) and Music Digital Libraries (MDL). 
</p>

<p>
For the past two years MIREX participants have met under the auspices of the International Conferences on Music Information Retrieval (ISMIR). The first MIREX plenary convened 14 September 2005 in London, UK, as part of ISMIR 2005. The second plenary of MIREX 2006 was convened in Victoria, BC on 12 October 2006 as part of ISMIR 2006. Table 1 summarizes the wide variety of MIR/MDL tasks that have been formally evaluated over the past two years. Some of these tasks, such as "Audio Onset Detection," represent micro level MIR/MDL research (i.e., accurately locating the beginning of music events in audio files, necessary for indexing). Others, such as "Symbolic Melodic Similarity," represent macro level MIR/MDL research (i.e., retrieving music based upon patterns of similarity between queries and pieces within the collections). 
</p>

<p align="center"><b>Table 1. Task Lists for MIREX 2005 and 2006</b></p>
<table cellspacing="0" cellpadding="4" width="65%" align="center">
	<tr>
		<td nowrap class="lefttop" align="center"><b>2005</b></td>
		<td nowrap class="lefttopright" align="center"><b>2006</b></td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Audio Artist Identification</td>
		<td nowrap class="lefttopright">Audio Beat Tracking</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Audio Drum Detection</td>
		<td nowrap class="lefttopright">Audio Cover Song Identification</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Audio Genre Identification</td>
		<td nowrap class="lefttopright">Audio Melody Extraction (2 subtasks)</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Audio Melody Extraction</td>
		<td nowrap class="lefttopright">Audio Music Similarity and Retrieval</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Audio Onset Detection</td>
		<td nowrap class="lefttopright">Audio Onset Detection</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Audio Tempo Extraction</td>
		<td nowrap class="lefttopright">Audio Tempo Extraction</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Audio Key Finding</td>
		<td nowrap class="lefttopright">Query-by-Singing or Humming (2 subtasks)</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Symbolic Genre Classification</td>
		<td nowrap class="lefttopright">Score Following</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Symbolic Key Finding</td>
		<td nowrap class="lefttopright">Symbolic Melodic Similarity (3 subtasks)</td>
	</tr>
	<tr>
		<td nowrap class="lefttopbottom">Symbolic Melodic Similarity</td>
		<td nowrap class="all">&nbsp;</td>
	</tr>
</table>

<p>
The tasks run for each MIREX were defined by community input via a set of topic-based mailing lists and Wiki pages [<a href="12downie.html#3">3</a>]. In this sense, MIREX, is similar to the Text Retrieval Conference (TREC) [<a href="12downie.html#4">4</a>] approach to the evaluation of text retrieval systems. Both MIREX and TREC are built upon three basic components:
</p>


<ol type="a" start="1">
	<li>a set of standardized collections; </li>
	<li class="spacetop">a set of standardized tasks/queries to be performed against these collections; and,</li>
	<li class="spacetop">a set of standardized evaluations of the results generated with regard to the tasks/queries.</li>
</ol>


<h3>2. Some Challenges </h3>

<p>
MIREX, however, does differ from TREC in one very important aspect. Unlike TREC, the datasets used in the MIREX evaluation process are not distributed freely among the participants. There are several overlapping reasons for this, including:
</p>


<ol type="a" start="1">
	<li>the current litigious state of enforcement of music intellectual property; </li>
	<li class="spacetop">the reluctance of data contributors (i.e., the creators of some of the more labour-intensive ground-truth sets) to have their work distributed widely; and, </li>
	<li class="spacetop">the general-consensus that free distribution would create "overfitting" of submitted systems to specific collections of data.</li>
</ol>


<p>
Since moving the data to participants is problematic, MIREX has the participants submit their algorithms to IMIRSEL for running against the collections. This scenario makes IMIRSEL personnel responsible for gathering (from various sources) and managing huge collections of music and ground-truth data (in a wide variety of formats). Responsibilities also include verifying the integrity of the test data itself [<a href="12downie.html#note-1">Note 1</a>]  and securing the data sets from malicious downloading. IMIRSEL is also responsible for managing the massive amounts of intermediate data that are created during the evaluation process, usually in the form of very large feature sets and similarity matrices that are common to many audio-based techniques. 
</p>

<p>
As Table <a href="12downie.html#table-2">2</a> indicates, IMIRSEL has run 164 algorithms over the past two years. For IMIRSEL, the actual act of successfully running each of these algorithms has been one of its greatest challenges. During MIREX 2006, for example, the submissions employed various combinations of 10 different programming languages (e.g., Matlab, C, C++, Max-MSP, Perl, Python, Java, etc.) and execution environments (e.g., *nix, Windows, Mac, etc.). Notwithstanding constant reminders to the community to pay attention to defined input and output formats, not to dynamically link to non-existent specialized libraries, to have submissions return standard completion and error codes and to die gracefully when necessary, the vast majority of IMIRSEL personnel time has been spent debugging submitted code <a name="table-2">and</a> verifying the validity of the output sets. 
</p>

<p align="center"><b>Table 2. Summary data for MIREX 2005 and 2006</b></p>
<table cellspacing="0" cellpadding="4" width="50%" align="center">
	<tr>
		<td nowrap class="lefttop" align="center"><b>&nbsp;</b></td>
		<td nowrap class="lefttop" align="center"><b>2005</b></td>
		<td nowrap class="lefttopright" align="center"><b>2006</b></td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Number of Tasks</td>
		<td nowrap class="lefttop" align="right">10</td>
		<td nowrap class="lefttopright" align="right">13</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Number of Teams</td>
		<td nowrap class="lefttop" align="right">41</td>
		<td nowrap class="lefttopright" align="right">46</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Number of Individuals</td>
		<td nowrap class="lefttop" align="right">82</td>
		<td nowrap class="lefttopright" align="right">50</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Number of Countries</td>
		<td nowrap class="lefttop" align="right">19</td>
		<td nowrap class="lefttopright" align="right">14</td>
	</tr>
	<tr>
		<td nowrap class="lefttopbottom">Number of Runs</td>
		<td nowrap class="lefttopbottom" align="right">72</td>
		<td nowrap class="all" align="right">92</td>
	</tr>
</table>


<h3>2. Some Advances</h3> 

<p>
MIREX 2006 marked the introduction of two important enhancements to the MIREX framework: 
</p>


<ol type="a" start="1">
	<li>tests of statistical significance (i.e., Freidman Test); and</li>
	<li class="spacetop">the use of human evaluators (i.e., Evalutron 6000). </li>
</ol>


<h4>2.1 Friedman Test</h4>

<p>
The Friedman Test, also known as Friedman's ANOVA is a non-parametric test (i.e., does not assume normal distribution of the underlying data). Since many retrieval result sets have non-normal distributions, the Friedman Test has been used in the TREC domain for a number of years. It is used to determine whether there truly exist significant differences in system performances. For example, it helps determine whether System A with a "score" of "72" is really performing better than System B ("68") and/or System C ("65"), etc.  Properly set up, it also allows for the <i>statistically valid pair-wise comparison</i> of each of the system results to help researchers better understand system differences. In this regard, is it much superior to the commonly misused multiple Student's t-tests.
</p>

<p>
Several MIREX 2006 tasks underwent Friedman's ANOVA testing. These included "Query-by-Singing/Humming," "Audio Cover Song Identification," and "Audio Music Similarity and Retrieval." Similar to the results found in past TREC evaluations, the Friedman data for MIREX 2006 indicate that most MIR/MDL systems generally tend to perform on par with their peers (with a few outlying exceptions) and that most of the variance in the results appears across the various queries rather than between the systems themselves. 
</p>

<h4>2.2 Human Evaluations and the Evalutron 6000</h4>

<p>
A common complaint among MIREX 2005 participants was the lack of any human <i>ex post facto</i> input in evaluating the various tasks. All MIREX 2005 tasks had their ground-truth data determined <i>a priori</i> to the evaluation runs. This <i>a priori</i> system is adequate for such tasks as "Onset Detection," "Audio Key Finding" and "Audio Cover Song Identification" as the "answers" to these tasks are not really subject to human interpretation. However, MIREX 2006 participants wanted to take on two "real world" tasks ( "Audio Music Similarity and Retrieval" and "Symbolic Melodic Similarity") that required human evaluation of the results in order to best judge whether the results retrieved were truly similar in some way to each of the input queries. Since there exists no <i>a priori</i> data on the similarity of query music pieces to all possibly returned music pieces, an <i>ex post facto</i> human evaluation system was developed for MIREX by IMIRSEL called the "Evalutron 6000".
</p>

<p>
After running the MIREX 2006 "Audio" and "Symbolic" similarity tasks, the top-x results ("candidates") for each query ("seeds") from each system were collated into "seed/candidate" sets with all source information removed (i.e., to make the evaluations "blind"). These "seed/candidate" sets were then mounted within the Evalutron 6000 which is a web-based relational database system that presented randomly selected "seed/candidate" lists to the evaluators and recorded their evaluation scores for each "seed/candidate" pair. Evaluators were drawn from members of the MIR/MDL with their identities not disclosed to the participants. To minimize evaluator fatigue, a system of presenting subsets of the results was devised. Table 3 presents the summary data concerning the distribution of "seed/candidate" sets among the evaluators.
</p>

<p align="center"><b>Table 3. Summary data for the "Audio Similarity" and "Symbolic Similarity" human evaluations</b></p>
<table cellspacing="0" cellpadding="4" width="50%" align="center">
	<tr>
		<td nowrap class="lefttop" align="center"><b>&nbsp;</b></td>
		<td nowrap class="lefttop" align="center"><b>Audio</b></td>
		<td nowrap class="lefttopright" align="center"><b>Symbolic</b></td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Number of evaluators</td>
		<td nowrap class="lefttop" align="right">24</td>
		<td nowrap class="lefttopright" align="right">20</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Number of evaluators per query/candidate pair</td>
		<td nowrap class="lefttop" align="right">3</td>
		<td nowrap class="lefttopright" align="right">3</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Number of queries per evaluator</td>
		<td nowrap class="lefttop" align="right">7.5</td>
		<td nowrap class="lefttopright" align="right">15</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Size of the candidate lists</td>
		<td nowrap class="lefttop" align="right">30</td>
		<td nowrap class="lefttopright" align="right">15</td>
	</tr>
	<tr>
		<td nowrap class="lefttop">Number of queries</td>
		<td nowrap class="lefttop" align="right">60</td>
		<td nowrap class="lefttopright" align="right">17</td>
	</tr>
	<tr>
		<td nowrap class="lefttopbottom">Number of evaluations per evaluator</td>
		<td nowrap class="lefttopbottom" align="right">~210</td>
		<td nowrap class="all" align="right">~225</td>
	</tr>
</table>


<p>
<a href="12downie.html#fig1">Figure 1</a> illustrates the Evalutron 6000 interface as seen by an evaluator for one "seed/candidate" listing. The left-most column has the "seed" embedded within an audio player that allows the evaluator to listen to the "seed" song and to start, stop and rewind it at will. The next column has an embedded audio player (with similar functionalities) for each of the "candidate" songs to be evaluated for similarity with respect to the "seed" song. The third column (second from right) takes the "Coarse" similarity score for each of the "seed/candidate" pairs. The "Coarse" scoring options include "Not Similar," "Somewhat Similar," and "Very Similar". The left-most column takes the "Fine" similarity score for the same "seed/candidate" pair recorded on a scale from 0 (not similar) to 10 (highly similar). While more formal correlation analyses are currently underway, preliminary data do indicate a strong consistency both across the different evaluators (i.e., inter-rater reliability) and strong correlations between the "Coarse" and "Fine" scores. IMIRSEL also plans on analyzing the log data associated with the evaluator interactions with the system to determine what improvements can be made to the MIREX 2007 iteration of <a name="fig1">the</a> Evalutron 6000.
</p>

<p align="center"><img src="downie-fig1.png" width="640" height="480" alt="Screen shot of the Evalutron 6000 interface"><br><br>
<b>Figure 1. The Evalutron 6000 interface.<br>
For a larger view of Figure 1, click <a href="downie-fig1-large.png">here</a>.</b>
</p>


<h3>3. Future Developments</h3> 

<p>
If MIREX is to grow and thrive, it is obvious that more robust mechanisms need to be put into place to alleviate the intensive commitment of labour resources MIREX places each year on the IMIRSEL team debugging code and result sets. Recent collaborations between IMIRSEL and UIUC's, Automated Learning Group (ALG) are opening up new opportunities for meeting this labour intensity challenge. IMIRSEL has worked with ALG before in the development of the Music-to-Knowledge (M2K) [<a href="12downie.html#5">5</a>] music mining and evaluation framework. M2K is a Java-based data-flow environment built upon the foundation of ALG's modular Data-to-Knowledge (D2K) and Text-to-Knowledge (T2K) data mining systems [<a href="12downie.html#6">6</a>]. IMIRSEL has been using M2K to help simplify the in-house running of the MIREX 2005 and 2006 evaluation tasks. 
</p>

<p>
Over the past several years, ALG has been developing a web service implementation of the D2K/T2K framework called D2KWS (D2K Web Services). The Tomcat/Java-based D2KWS technology has matured enough for related *2K projects to begin experimenting with independent, domain-specific (e.g., music retrieval evaluation) prototype deployments. IMIRSEL has set up an experimental D2KWS system to begin demonstration and proof-of-concept work on several "Do-It-Yourself" (DIY) MIREX evaluation frameworks [<a href="12downie.html#7">7</a>].
</p>

<h3>4. MIREX "DIY" Frameworks: Benefits and Challenges</h3>

<p>
A principal benefit to be realized by the creation of prototype DIY web service frameworks for MIREX is the labour shift from the IMIRSEL team back to submitters themselves. If implemented correctly, this labour shift actually provides tangible benefits to the submitters in exchange for their added effort. For example, a properly realized prototype would be available to the community 24/7/365. The time constraints imposed by constant debugging of code have made the rerunning of previous evaluation tasks difficult (which hinders meaningful comparisons across years). Also, because IMIRSEL plans on having the "DIY" system store all previous result sets, research productivity within the MIR.MDL community should improve along two fronts. First, submitters intending to participate in a given MIREX will have the ability to see how their algorithms are performing in near real-time with respect to their peer participants. Currently, the participating labs only see the "final" results sets that are made available shortly before the MIREX plenary making the <i>de facto</i> research cycle a year long. Second, non-participants who have novel MIR/MDL techniques will be able to submit and evaluate on their own anytime during the year to quickly determine whether or not their techniques are reaching state-of-the-art effectiveness. 
</p>

<p>
Notwithstanding the important benefits to be derived from the establishment of DIY MIREX services, there remain several significant challenges that need addressing. First, IMIRSEL must ensure that only <i>results data</i> are transmitted from the DIY system back to the participants. We are currently experimenting with several data choke/filtering schemes to make the transmission of music data impossible. Second, IMIRSEL needs to develop a permission system/policy that effectively shields the music data from malicious access attempts made by the submitted code sets. At this point, this remains an open question, so in the early days we will be opening the prototype to select external labs with which IMIRSEL has built up a high trust relationship. Third, IMIRSEL needs to make formal assessments of the computational resources that will need to be dedicated to a 24/7/365 service. These are non-trivial as, for example, the current music collections are roughly 1 terabyte in size (and growing) and the "feature sets" generated by many of the algorithms can be larger than the underlying music they represent and can take hundreds of CPU hours to compute. Third, and finally, we need to make the DIY MIREX service package easily transportable so other MIR/MDL labs can take on some of the administrative work and to make their own special collections available as standardized evaluation resources. 
</p>

<h3>5. Acknowledgements</h3> 

<p>
Dr. Downie and his IMIRSEL team are supported by the Andrew W. Mellon Foundation and the National Science Foundation (NSF) under Grant Nos. NSF IIS-0340597 and NSF IIS- 0327371.
</p>

<h3>6. Note</h3>

<p>
<a name="note-1">Note 1</a>:  We are constantly surprised by the amount of "corrupted" data that makes its way into carefully collated music collections including damaged audio files, empty MIDI files, mislabeled file headers, etc. Since some task runs (e.g., "Audio Music Similarity and Retrieval") can take four days per algorithm to process, it is very important that the input data not cause a system crash during day three of a run.
</p>


<h3>7. References</h3> 

<p>
[<a name="1">1</a>] Downie, J. Stephen, Kris West, Andreas Ehmann and Emmanuel Vincent (2005). The 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005): Preliminary Overview. In <i>Proceedings of the Sixth International Conference on Music Information Retrieval (ISMIR 2005), London, UK, 11-15 September 2005</i>. Queen Mary, UK: University of London, pp. 320-323. Available: &lt;<a href="http://ismir2005.ismir.net/proceedings/xxxx.pdf">http://ismir2005.ismir.net/proceedings/xxxx.pdf</a>&gt;. 
</p>

<p>
[<a name="2">2</a>] Downie, J. Stephen, Joe Futrelle and David Tcheng (2004).  The International Music Information Retrieval Systems Evaluation Laboratory: Governance, access, and security.  In <i>Fifth International Conference on Music Information Retrieval (ISMIR 2004), 10-14 October 2004, Barcelona, Spain</i>.  Barcelona, Spain: Universitat Pompeu Fabra, pp. 9-14. Available: 
&lt;<a href="http://www.iua.upf.es/mtg/ismir2004/review/CRFILES/paper220-9f30cf2a39ea701625aad9ee86f6ac1f.pdf">http://www.iua.upf.es/mtg/ismir2004/review/CRFILES/paper220-9f30cf2a39ea701625aad9ee86f6ac1f.pdf</a>&gt;. 
</p>

<p>
[<a name="3">3</a>] See &lt;<a href="http://www.music-ir.org/mirex2006/">http://www.music-ir.org/mirex2006/</a>&gt;.
</p>

<p>
[<a name="4">4</a>] See &lt;<a href="http://trec.nist.gov/">http://trec.nist.gov/</a>&gt;.
</p>

<p>
[<a name="5">5</a>] See &lt;<a href="http://music-ir.org/evaluation/m2k/">http://music-ir.org/evaluation/m2k/</a>&gt;.
</p>

<p>
[<a name="6">6</a>] See &lt;<a href="http://alg.ncsa.uiuc.edu/do/tools/">http://alg.ncsa.uiuc.edu/do/tools/</a>&gt;.  
</p>

<p>
[<a name="7">7</a>] See &lt;<a href="http://cluster3.lis.uiuc.edu:8080/mirexdiydemo/">http://cluster3.lis.uiuc.edu:8080/mirexdiydemo/</a>&gt;.
</p>



   <!-- Standard Copyright line here -->  
     <center><h6>Copyright &copy; 2006 J. Stephen Downie</h6>   </center> 
       </td>  
    </tr> 
   <!-- Begin the bottom sections -->
       <tr>
        <td>
<img src="../images/spacer00.gif" width="10" height="10" alt="spacer"></td> 
    <td> <hr width="80%" noshade size="1"></td> 
   </tr>  
    <tr>  
<td><img src="../images/spacer00.gif" width="10" height="10" alt="spacer"></td>
     <td> 
     <p class="cbs">
      <a href="12downie.html#Top">Top</a> 
     | <a href="../12contents.html">Contents</a><br>
      <a href="../../../Architext/AT-dlib2query.html">Search</a> 
     |  <a href="../../../author-index.html">Author Index</a>   
   |  <a href="../../../title-index.html">Title Index</a>   
   |  <a href="../../../back.html">Back Issues</a><br> 
     <a href="../morris/12morris.html">Previous Conference Report</a> |  <a href="../boeke/12boeke.html">Next Conference Report</a> <br> 
       <a href="../../../dlib.html">Home</a> 
    | <a href="https://www.dlib.org/cdn-cgi/l/email-protection#7b1f1712193b1815091255091e080f1415550d1a550e08">E-mail the Editor</a></p>
      </td>
    </tr>  
    <tr>   
     <td><img src="../images/spacer00.gif" width="10" height="10" alt="spacer"></td> 
    <td> <hr width="80%" noshade size="1"></td>
    </tr>
      <tr> 
     <td><img src="../images/spacer00.gif" width="10" height="10" alt="spacer"></td> 
      <td> 
     <p class="small70"><a href="../../../access.html">D-Lib Magazine Access Terms and Conditions</a></p> 
       <p class="small70">doi:10.1045/december2006-downie</p> 
        <p>&nbsp;</p> 
  </td>    
  </tr>
     </table>  
   <script data-cfasync="false" src="../../../cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script></body> 
   </html>  