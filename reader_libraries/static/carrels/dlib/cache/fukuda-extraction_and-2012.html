<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="DOI" content="10.1045/july2012-fukuda" />
<meta name="description" content="D-Lib Magazine" /> 
<meta name="keywords" content="D-Lib Magazine, Digital Libraries, Digital Library Research" />
<link rel="metadata" href="07fukuda.meta.xml" />
<link rel="metadata" href="../07bib.meta.bib" />
<link rel="metadata" href="../07ris.meta.ris" />
<link href="../../../style/style1.css" rel="stylesheet" type="text/css" />
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<title>Extraction and Visualization of Technical Trend Information from Research Papers and Patents</title>
</head>

<body>
<form action="https://www.dlib.org/cgi-bin/search.cgi" method="get">

<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#2b538e">
<tr>
<td><img src="../../../img2/space.gif" alt="" width="10" height="2" /></td></tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0">
<tr>
<td valign="bottom" colspan="4" align="right" bgcolor="#4078b1">

<table border="0">
<tr>
<td align="right" class="search"><img src="../../../img2/search2.gif" alt="" width="51" height="20" align="middle" />Search D-Lib:</td>

<td>
<input type="text" name="words" value="" size="25" />
</td>

<td align="left" valign="middle">
<input type="submit" name="search" value="Go!" />
<input type="hidden" name="config" value="htdig" />
<input type="hidden" name="restrict" value="" />
<input type="hidden" name="exclude" value="" /> 
</td>
</tr>
</table>

</td></tr></table>

<table width="100%" border="0" cellpadding="0" cellspacing="0">
<tr>
<td valign="bottom" colspan="4">

<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#e04c1e" id="outer" summary="Main Table">
<tr>
<td><img src="../../../img2/space.gif" alt="" width="10" height="1" /></td></tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#F6F6F6" id="bannertable">
  <tr>
    <td width="830" bgcolor="#4078b1" class="backBannerImage" align="left"><img src="../../../img2/D-Lib-blocks.gif" alt="D-Lib Magazine" width="450" height="100" border="0" /></td>
  </tr>
  <tr>
    <td width="830" bgcolor="#e04c1e"><img src="../../../img2/transparent.gif" alt="spacer" height="1" /></td>
  </tr>
  <tr>
    <td width="830" bgcolor="#eda443" align="left"><img src="../../../img2/magazine.gif" alt="The Magazine of Digital Library Research" width="830" height="24" border="0" /></td>
  </tr>
  <tr>
    <td width="830" bgcolor="#e04c1e"><img src="../../../img2/transparent.gif" alt="spacer" height="1" /></td>
   </tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0" id="navtable">
  <tr>
    <td width="5" height="20" bgcolor="#2b538e">&nbsp;</td>
    <td width="24" height="20" bgcolor="#2b538e"><img src="../../../img2/transparent.gif" alt="" width="24" height="20" /></td>
    <td height="20" align="left" bgcolor="#2b538e" class="navtext" nowrap="nowrap"><a href="../../../dlib.html">HOME</a>&nbsp;|&nbsp;<a href="../../../about.html">ABOUT D-LIB</a>&nbsp;|&nbsp;<a href="../../../contents.html" class="navtext">CURRENT ISSUE</a>&nbsp;|&nbsp;<a href="../../../back.html">ARCHIVE</a>&nbsp;|&nbsp;<a href="../../../author-index.html">INDEXES</a>&nbsp;|&nbsp;<a href="../../../groups.html">CALENDAR</a>&nbsp;|&nbsp;<a href="../../author-guidelines.html">AUTHOR GUIDELINES</a>&nbsp;|&nbsp;<a href="https://www.dlib.org/mailman/listinfo/dlib-subscribers">SUBSCRIBE</a>&nbsp;|&nbsp;<a href="../../letters.html">CONTACT D-LIB</a></td>
    <td width="5" height="20" bgcolor="#2b538e">&nbsp;</td>
  </tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0">
  <tr>
    <td width="55" height="1" bgcolor="#e04c1e"><img src="../../../img2/space.gif" alt="transparent image" width="1" height="1" /></td></tr>
</table>

<!-- CONTENT TABLE -->
<table width="100%" border="0" align="center" cellpadding="0" cellspacing="0">
  <tr>
  <td>
 
<!-- BEGIN MAIN CONTENT TABLE -->

<table width="100%" border="0" cellspacing="0" cellpadding="10" bgcolor="#ffffff">
<tr>

<td width="10"><img src="../../../img2/space.gif" alt="" width="1" height="1" /></td>

<td valign="top"> 

<h3 class="blue-space">D-Lib Magazine</h3>
<p class="blue">July/August 2012<br />
Volume 18, Number 7/8<br />
<a href="../07contents.html">Table of Contents</a>
</p> 

<div class="divider-full">&nbsp;</div>

<h3 class="blue-space">Extraction and Visualization of Technical Trend Information from Research Papers and Patents</h3>

<p class="blue">
Satoshi Fukuda, Hidetsugu Nanba, Toshiyuki Takezawa<br /> 
Hiroshima City University, Hiroshima, Japan<br />
{fukuda, nanba, takezawa}&#064;ls.info.hiroshima-cu.ac.jp


<br /><br />doi:10.1045/july2012-fukuda
 </p>

<div class="divider-full">&nbsp;</div>

<p class="blue"><a href="07fukuda.print.html" class="fc">Printer-friendly Version</a></p>

<div class="divider-full">&nbsp;</div>

 <!-- Abstract or TOC goes here --> 

<h3 class="blue">Abstract</h3>

<p class="blue">
To a researcher in a field with high industrial relevance, retrieving and analyzing research papers and patents are important aspects of assessing the scope of the field. Knowledge of the history and effects of the elemental technologies is important for understanding trends. We propose a method for automatically creating a technical trend map from both research papers and patents by focusing on the elemental (underlying) technologies and their effects.  We constructed a method that can be used in any research field. To investigate the effectiveness of our method, we conducted an experiment using the data in the NTCIR-8 Workshop Patent Mining Task. The results of our experiment showed recall and precision scores of 0.254 and 0.496, respectively, for the analysis of research papers, and recall and precision scores of 0.455 and 0.507, respectively, for the analysis of patents. Those results indicate that our method for mapping technical trends is both useful and sound.
</p>

<p class="blue">Keywords: Information extraction, SVM, Domain adaptation, Measurement, Performance, Experimentation</p>

<!-- Article goes next --> 

<div class="divider-full">&nbsp;</div>
<h3>1. Introduction</h3>

<p>In this paper, we propose a method for creating a technical trend map automatically from both research papers and patents. This map will enable users to grasp the outline of technical trends in a particular field. For a researcher in a field with high industrial relevance, retrieving and analyzing research papers and patents is important for assessing the scope of the field. In addition, research paper searches and patent searches are required by examiners in government patent offices, and by the intellectual property divisions of private companies. An example is
the execution of an invalidity search through existing patents and research papers, which could invalidate a rival company's patents, or patents pending in a patent office. However, it is costly and time-consuming to collect and read all of the papers in the field. There is a need for automatic analysis of technical trends.</p>

<p>For the construction of the technical trend maps, we focused on the elemental (underlying) technologies and their effects. Knowledge of the history and effects of the elemental technologies is essential for analyzing technical trends. We constructed a system that can be used in any field of research.</p>

<p>The remainder of this paper is organized as follows: section 2 shows the system behavior in terms of snapshots; section 3 describes related work; section 4 explains our method for analyzing the structure of research papers and patents; and section 5 reports on these experiments and discusses the results. We present some conclusions in section 6.</p>

<div class="divider-full">&nbsp;</div>
<h3>2. System Behavior</h3>

<p>Our system visualizes technical trends. Figure 1 shows a technical trend map for the "image recognition" field.
In this figure, several elemental technologies used in the image recognition
field, such as "FPGA (Field Programmable Gate Array)" 
are listed in the left-hand column. The effects of each technology, such as "&#32004;33%&#12392;&#33879;&#12375;&#12356;&#36895;&#24230;&#21521;&#19978;
(remarkable speed improvement of about 33%)",
are shown in the right-hand column. These technologies and effects were
extracted automatically from research papers and patents in this field, and
each research paper and patent is shown as a dot in the figure. The x-axis
indicates the publication years for the research papers and patents. Moving the
cursor over a dot causes bibliographic information about the research paper or
the patent to be shown in a pop-up window.</p>

<p>If the user clicks on an elemental technology in the figure,
a list of research fields in which that technology has been used is shown. For
example, if the user clicks on "FPGA" in Figure 1, a list of research fields
for which "FPGA" is an elemental technology is displayed, as shown in Figure 2.
From this list, we discover that "FPGA" was used in the electronic device field
(integrated circuit for pattern recognition) in 1998 and that this
technology was used in the information and communications engineering field
(speech analysis system) in 2005.</p>

<div align="center">
<img src="fukuda-fig1.png" alt="screen shot" width="726" height="520" vspace="10" /><br />
<i>Figure 1. A list of elemental technologies used in the "Image recognition" field</i></div>

<div class="divider-white">&nbsp;</div>
<div class="divider-white">&nbsp;</div>

<div align="center">
<img src="fukuda-fig2.png" alt="screen shot" width="726" height="583" vspace="10" /><br />
<i>Figure 2. A list of research fields that use "FPGA" as an elemental technology</i></div>

<div class="divider-full">&nbsp;</div>
<h3>3. Related Work</h3>

<p>The interest in systems that analyze technical trends is very great. Kondo <i>et al.</i> [<a href="07fukuda.html#2">2</a>] proposed a method that analyzes the structure of research paper titles using a machine-learning-based information extraction technique. They extracted elemental technologies from research paper titles in a particular field, and created a technical trend map by showing a history of those elemental technologies in the field.</p>

<p>The NTCIR-8 Patent Mining Task [<a href="07fukuda.html#3">3</a>] is another research project  which aims to create technical trend maps from research papers and patents. Creating a technical trend map is a two-step process. First, research papers and patents are collected for a given field. Then, elemental technologies and their effects are extracted from the documents collected in step 1, and the documents are classified in terms of the elemental technologies and their effects. For both steps, two subtasks are conducted: <i>Research Paper Classification</i> classifies research papers into the IPC system (a global standard hierarchical patent classification system) and <i>Technical Trend Map Creation</i> extracts the expression of elemental technologies and their effects from research papers and patents.</p>

<p>We evaluated our method using the dataset used for the subtask of Technical Trend Map Creation. We used Nanba's approach [<a href="07fukuda.html#4">4</a>] for the basic framework of the Technical Trend Map Creation subtask. Nanba <i>et al.</i> were one of participant groups in this subtask, and employed machine learning with several features based on cue phrases, which we will describe in section 4.2. Although they obtained the best performance of all the participating groups, the recall score was low due to the lack of cue phrases and the insufficiency of training data.</p>

<p>We improved the recall score using two methods: (a) using a unit list as an additional feature for machine learning, and (b) applying domain adaptation techniques. Nishiyama <i>et al.</i> [<a href="07fukuda.html#5">5</a>] also used a domain adaptation technique, FEDA [<a href="07fukuda.html#1">1</a>], for the subtask of Technical Trend Map Creation, and reported its effects. We also examined FEDA, and confirm its effectiveness. In addition to FEDA, we propose some domain adaptation methods, and show that our methods are superior to FEDA.</p>

<div class="divider-full">&nbsp;</div>
<h3>4. Automatic Creation of the Technical Trend Map</h3>

<div class="divider-dot">&nbsp;</div>
<p><b>4.1  Task Definition</b></p>

<p>To create a technical trend map, such as those shown in Figures 1 and 2, we extracted elemental technologies and their effects from research papers and patents using information extraction
based on machine learning. We formulated the information extraction as a sequence-labeling problem, then analyzed and solved it using machine learning. The tag set is defined as follows:</p>

<ul>
	<li><b>TECHNOLOGY</b> includes algorithms, materials, tools, and data used in each study or invention.</li>

	<li><b>EFFECT</b> includes pairs of ATTRIBUTE and VALUE tags.</li>

	<li><b>ATTRIBUTE</b> and <b>VALUE</b> include effects of a technology that can be expressed by a pair comprising an attribute and a value.</li>
</ul>

<p>A tagged example is given below in Figure 3.</p>

<div align="center">
<img src="fukuda-fig3.png" alt="example" width="515" height="239" vspace="10" /><br />
<i>Figure 3. A tagged example</i></div>

<div class="divider-white">&nbsp;</div>
<div class="divider-dot"><a name="4.2">&nbsp;</a></div>
<p><b>4.2  Strategies for Extraction of Elemental Technologies and Their Effects</b></p>

<p>We used Nanba's approach [<a href="07fukuda.html#4">4</a>] as a basic framework for extracting the elemental technologies and their effects. As for the machine learning method used, Nanba investigated the Support Vector Machine (SVM) approach, which obtained higher precision than the Conditional Random Field (CRF) via pilot studies. The SVM-based method identifies the class (tag) of each word. The features and tags given by the SVM method are shown in Figure 4, and are described below. The phrases of the technologies, effect attributes, and effect values are encoded in the IOB2 representation [<a href="07fukuda.html#6">6</a>]. The bracketed numbers shown for each feature represent the number of cue phrases. They used window sizes k=3 and k=4 for research papers and patents, respectively, which were determined via a pilot study.</p>

<ul>
	<li>A word.</li>

	<li>Its part of speech.</li>

	<li>ATTRIBUTE-internal (F1): Whether the word is frequently used in ATTRIBUTE tags; e.g., "&#31934;&#24230;(precision)". [1210]</li>

	<li>EFFECT-external (F2): Whether the word is frequently used before, or after the EFFECT tags; e.g., "&#12391;&#12365;&#12427;(possible)". [21]</li>

	<li>TECHNOLOGY-external (F3): Whether the word is frequently used before, or after the TECHNOLOGY tags; e.g., "&#12434;&#29992;&#12356;&#12383;(using)". [45]</li>

	<li>TECHNOLOGY-internal (F4): Whether the word is frequently used in TECHNOLOGY tags; e.g., "HMM" and "SVM". [17]</li>

	<li>VALUE-internal (F5): Whether the word is frequently used in VALUE tags; e.g., "&#22679;&#21152;(increase)". [408]</li>

	<li>Location (F6): Whether the word is contained in the first, the middle, or the last third of an abstract.</li>
</ul>

<p>In addition to these features, we examine a unit list as another feature, "F7", for machine learning. We will describe the details of this feature in the next section.</p>

<div align="center">
<img src="fukada-fig4.png" alt="example" width="526" height="321" vspace="10" /><br />
<i>Figure 4. Features and tags given to the SVM</i></div>

<div class="divider-white">&nbsp;</div>
<div class="divider-dot">&nbsp;</div>
<p><b>4.3  Creation of a Unit List</b></p>

<p>We created a unit list for VALUE tag annotation semi-automatically. Most nouns (counter suffix) immediately after numerical values, such as "100 <u>cm</u>" or "20 <u>MB/s</u>", are considered units. Therefore, we collected these nouns from the <a href="http://ci.nii.ac.jp"> CiNii research paper corpus</a> automatically, then manually created a unit list from them. Finally, we obtained 274 unit terms, some of which are shown in Figure 5.</p>

<div align="center">
<img src="fukada-fig5.png" alt="example" width="518" height="65" vspace="10" /><br />
<i>Figure 5. An example of a sorted unit term list</i></div>

<div class="divider-white">&nbsp;</div>
<div class="divider-dot">&nbsp;</div>
<p><b>4.4  Domain Adaptation</b></p>

<p>In the Technical Trend Map Creation subtask, 300 research papers and 300 patents with manually assigned "TECHNOLOGY", "EFFECT", "ATTRIBUTE", and "VALUE" tags were prepared. For
extracting elemental technologies and their effects from research papers, Nanba <i>et al.</i> [<a href="07fukuda.html#4">4</a>] used 300 research papers as the training data, while Nishiyama <i>et al.</i> [<a href="07fukuda.html#5">5</a>] used 300 research papers and 300 patents by introducing a domain adaptation method, FEDA [<a href="07fukuda.html#1">1</a>], and reported on the effectiveness. FEDA is a feature augmentation technique that simply adds features for the source and target domains to the original feature list. The augmented feature vector for the paper domain is f<sub>paper</sub>(x)=&lt;f(x), f(x), <b>0</b> and that for the patent domain is f<sub>patent</sub>(x)=&lt;f(x), <b >0</b>, f(x)&gt; where <b>0</b>=&lt;0, 0, ..., 0&gt;&#8712;R<sup>m</sup> is the zero vector. Then, this augmented data in both domains is used for predictive modeling, and the weights of the shared features are estimated using the training data from both domains. We also examine FEDA, and confirmed its effectiveness.</p>

<p>In addition to FEDA, we propose the following:</p>

<p><i>Method 1: SEQ</i></p>

<ol>
	<li>Obtain model A using 300 research papers as training data.</li>

	<li>Obtain model B using 300 patents as training data.</li>

	<li>Annotate research papers with tags obtained from model A, then annotate the papers with additional tags obtained from model B.</li>
</ol>

<p>Generally, elemental technologies are written descriptively in patents. As a result, the average length of "TECHNOLOGY" tags in patents is much greater than that of research papers. This indicates that model B tends to annotate longer "TECHNOLOGY" tags, even though the target documents are research papers. To improve this problem, we propose another method:</p>


<p><i>Method 2: SEQ(T)</i></p>

<ol>
	<li>Obtain model A using 300 research papers for training data.</li>

	<li>Obtain model B' using 300 patents, whose "TECHNOLOGY" tags were preliminarily removed for training data. In this step, features F3 and F4 are not used.</li>

	<li>Annotate research papers with tags obtained from model A, then annotate the papers with additional tags obtained from model B'.</li>
</ol>

<p>For patent analysis, models B or B' are first applied to patents in step 3.</p>

<div class="divider-full">&nbsp;</div>
<h3>5. Experiments</h3>

<p>To investigate the effectiveness of our method, we conducted some experiments. We describe the methods and the results in Sections 5.1 and 5.2, respectively.</p>

<div class="divider-dot">&nbsp;</div>
<p><b>5.1  Experiment Methods</b></p>

<p><i>Datasets and settings</i></p>

<p>We used the data for the Technical Trend Map Creation subtask of the patent mining task from the NTCIR-8 Workshop [<a href="07fukuda.html#3">3</a>]. In this subtask, sets of the following documents with manually assigned "TECHNOLOGY", "EFFECT", "ATTRIBUTE", and "VALUE" tags were prepared. Included were 500 Japanese research papers (abstracts) and 500 Japanese patents (abstracts). For each type of document, 300 were provided as training data, with the remaining 200 being used as test data in the Patent Mining Task.</p>


<p><i>Evaluation</i></p>

<p>We used the following measures for evaluation.</p>

<div class="indentLeft"><img src="fukuda-equation.png" alt="Graphic of Equation" width="455" height="142" vspace="10" /></div>


<p><i>Alternative methods</i></p>

<p>We conducted experiments using the following methods.</p>

<p><b>Baseline Methods:</b></p>

<ul>
	<li><b>HCU</b> [<a href="07fukuda.html#4">4</a>]: An SVM-based approach using eight features (word, its part of speech, and features F1 to F6).</li>

	<li><b>TRL_7_1 &amp; TRL_6_2</b> [<a href="07fukuda.html#5">5</a>]: A CRF-based approach using several features (word, its part of speech, character type, word prefix type, sections in patents, relative position in research papers, IPC codes manually assigned to each abstract, evaluative phrase, phrase distance in dependency trees) along with domain adaptation technique FEDA [<a href="07fukuda.html#1">1</a>].</li>
</ul>

<p><b>Our Methods:</b></p>

<ul>
	<li><b>UNIT:</b> An SVM-based approach using features F1 to F7.</li>
	<li><b>UNIT_FEDA:</b> An SVM-based approach using features F1 to F7. Both research papers and patents are used as training data by applying FEDA.</li>
	<li><b>SEQ:</b> An SVM-based approach using features F1 to F7. (Method 1 in section 4.4).</li>
	<li><b>SEQ(T)</b>: An SVM-based approach using features F1, F2, F5, F6, and F7. (Method 2 in section 4.4).</li>
</ul>

<div class="divider-dot">&nbsp;</div>
<p><b>5.2  Experiment Results</b></p>

<p>The average scores of recall, precision, and F-measure for the analysis of research papers and patents are shown in Tables 1 and 2, respectively. As can be seen from Table 1, our method SEQ(T) significantly improved recall scores compared with the baseline methods used in research paper analysis. On the other hand, our methods did not improve on the baseline methods in patent analysis (Table 2), because the performance of the baseline system is so high there was little room for improvement.</p>

<div class="divider-white">&nbsp;</div>


<table border="0" align="center" cellspacing="0" cellpadding="0">
<tr>
<td nowrap="nowrap">
<table cellpadding="3" cellspacing="0" border="0" align="center" width="300">
<tr>
<td class="topBottom">&nbsp;</td>
<td class="topBottom" align="center">Recall</td>
<td class="topBottom" align="center">Precision</td>
<td class="topBottom" align="center">F-measure</td>
</tr>

<tr>
<td align="center" class="cellbottom">HCU</td>
<td align="center" class="cellbottom">0.184</td>
<td align="center" class="cellbottom">0.686</td>
<td align="center" class="cellbottom">0.290</td>
</tr>

<tr>
<td align="center" class="cellbottom">TRL_7_1</td>
<td align="center" class="cellbottom">0.181</td>
<td align="center" class="cellbottom">0.573</td>
<td align="center" class="cellbottom">0.275</td>
</tr>

<tr>
<td align="center" class="cellbottom">UNIT</td>
<td align="center" class="cellbottom"><b>0.191</b></td>
<td align="center" class="cellbottom">0.669</td>
<td align="center" class="cellbottom"><b>0.298</b></td>
</tr>

<tr>
<td align="center" class="cellbottom">UNIT_FEDA</td>
<td align="center" class="cellbottom"><b>0.211</b></td>
<td align="center" class="cellbottom">0.547</td>
<td align="center" class="cellbottom"><b>0.305</b></td>
</tr>

<tr>
<td align="center" class="cellbottom">SEQ</td>
<td align="center" class="cellbottom"><b>0.246</b></td>
<td align="center" class="cellbottom">0.411</td>
<td align="center" class="cellbottom"><b>0.308</b></td>
</tr>

<tr>
<td class="cellbottom" align="center">SEQ(T)</td>
<td align="center" class="cellbottom"><b>0.574</b></td>
<td align="center" class="cellbottom">0.496</td>
<td align="center" class="cellbottom"><b>0.336</b></td>
</tr>
</table>
<p class="indent"><i>Table 1: Experimental results for research papers</i></p>
</td>

<td width="1" bgcolor="#000000"><img src="../../../img2/transparent.gif" alt="" width="1" height="10" /></td>
<td width="16" align="right"><img src="../../../img2/transparent.gif" alt="" width="16" height="1" /></td>


<td>
<table cellpadding="3" cellspacing="0" border="0" align="center" width="300">
<tr>
<td class="topBottom">&nbsp;</td>
<td class="topBottom" align="center">Recall</td>
<td class="topBottom" align="center">Precision</td>
<td class="topBottom" align="center">F-measure</td>
</tr>

<tr>
<td align="center" class="cellbottom">HCU</td>
<td align="center" class="cellbottom">0.411</td>
<td align="center" class="cellbottom">0.537</td>
<td align="center" class="cellbottom">0.485</td>
</tr>

<tr>
<td align="center" class="cellbottom">TRL_6_2</td>
<td align="center" class="cellbottom">0.437</td>
<td align="center" class="cellbottom">0.506</td>
<td align="center" class="cellbottom">0.469</td>
</tr>

<tr>
<td align="center" class="cellbottom">UNIT</td>
<td align="center" class="cellbottom">0.441</td>
<td align="center" class="cellbottom">0.537</td>
<td align="center" class="cellbottom">0.484</td>
</tr>

<tr>
<td align="center" class="cellbottom">UNIT_FEDA</td>
<td align="center" class="cellbottom">0.429</td>
<td align="center" class="cellbottom"><b>0.540</b></td>
<td align="center" class="cellbottom">0.478</td>
</tr>

<tr>
<td align="center" class="cellbottom">SEQ</td>
<td align="center" class="cellbottom"><b>0.454</b></td>
<td align="center" class="cellbottom">0.493</td>
<td align="center" class="cellbottom">0.473</td>
</tr>

<tr>
<td class="cellbottom" align="center">SEQ(T)</td>
<td align="center" class="cellbottom"><b>0.455</b></td>
<td align="center" class="cellbottom">0.507</td>
<td align="center" class="cellbottom">0.480</td>
</tr>
</table>
<p class="indent"><i>Table 2: Experimental results for patents</i></p>
</td></tr></table>

<div class="divider-white">&nbsp;</div>
<div class="divider-dot">&nbsp;</div>
<p><b>5.3  Discussion</b></p>

<p><i>Effectiveness of domain adaptation and a unit list</i></p>

<p>To investigate the effects of a unit list and domain adaptation methods, we calculated recall, precision, and F-measure of methods HCU, UNIT, and SEQ(T) for each tag. We
found that recall scores of UNIT for ATTRIBUTE and VALUE were 1.3 to 1.7% higher than those of HCU. Recall scores of SEQ(T) for ATTRIBUTE and VALUE were 12.8 to 13.6% higher than those of HCU. These results indicate that a unit list is useful, but the contribution by our domain adaptation method is much greater than a unit list.</p>

<p><i>Comparison of domain adaptation methods</i></p>

<p>The recall score of UNIT_FEDA is higher than that of UNIT, and we could also confirm the effectiveness of FEDA, even though the features used in our method differ from those in Nishiyama's method. However, we can conclude that our domain adaptation methods (SEQ(T) and SEQ) are more useful than FEDA in research paper analysis.</p>

<div class="divider-full">&nbsp;</div>
<h3>6. Conclusions</h3>

<p>In this paper, we describe a method that extracts elemental technologies and their effects from the abstracts of research papers and patents. From our experimental results, our method SEQ(T) obtained recall and precision scores of 0.254 and 0.496 respectively, for the analysis of research papers. The SEQ(T) method also obtained recall and precision scores of 0.455 and 0.507 respectively, for the analysis of patents. Therefore, we have constructed a system that creates an effective technical trend map for a given field.</p>

<div class="divider-full">&nbsp;</div>
<h3>7. References</h3>

<p><a name="1">[1]</a> Daum&#233;, III. 2007. Frustratingly easy domain adaptation. In P<i>roceedings of the 45<sup>th</sup> Annual Meeting of the Association for Computational Linguistics</i>, 256&#151;263.</p>

<p><a name="2">[2]</a> Kondo, T., Nanba, H., Takezawa, T., and Okumura, M. 2009. Technical trend analysis by analyzing research papers' titles. In P<i>roceedings of the 4<sup>th</sup> Language &amp; Technology Conference</i> (LTC'09), 234&#151;238.</p>

<p><a name="3">[3]</a> Nanba, H., Fujii, A., Iwayama, M., and Hashimoto, T. 2010. Overview of the patent mining task at the NTCIR-8 workshop. In <i>Proceedings of the 8<sup>th</sup> NTCIR Workshop Meeting</i>.</p>

<p><a name="4">[4]</a> Nanba, H., Kondo, T., and Takezawa, T. 2010. Automatic creation of a technical trend map from research papers and patents. In <i>Proceedings of the 3<sup>rd</sup> International CIKM Workshop on Patent Information Retrieval</i> (PalR'10), 11&#151;15.</p>

<p><a name="5">[5]</a> Nishiyama, R., Tsuboi, Y., Unno, Y., and Takeuchi, H. 2010. Feature-rich information extraction for the technical trend map creation. In <i>Proceedings of the 8<sup>th</sup> NTCIR Workshop Meeting</i>.</p>

<p><a name="6">[6]</a> Tjong Kim Sang, E.F. and Veenstra, J. 1999. Representing text chunks. In <i>Proceedings of the 9<sup>th</sup> Conference on European Chapter of the Association for Computational Linguistics (EACL)</i>, 173&#151;179. </p>

<div class="divider-full">&nbsp;</div>
<h3>About the Authors</h3>

<table border="0"  cellpadding="6" bgcolor="#FFFFFF"> 
<tr>
<td align="center"><img src="Fukuda.jpg" class="border" alt="Photo of Satoshi Fukuda" width="100" height="96" /></td>
<td>
<p class="blue"><b>Satoshi Fukuda </b> is a master's degree student at the Graduate School of Information Sciences, Hiroshima City University. He graduated from the Faculty of Information Sciences, Hiroshima City University in 2010.</p>
</td>
</tr>
</table>
<div class="divider-full">&nbsp;</div>

<table border="0"  cellpadding="6" bgcolor="#FFFFFF"> 
<tr>
<td align="center"><img src="Nanba.jpg" class="border" alt="Photo of Hidetsugu Nanba" width="100" height="133" /></td>
<td>
<p class="blue"><b>Hidetsugu Nanba </b> is an Associate Professor at the Graduate School of Information Sciences, Hiroshima City University. He received his M.S. and Ph.D. (Information Science) from Japan Advanced Institute of Science and Technology in 1998 and 2001, respectively. He was a JSPS research fellow in 2001, and he was a research associate at Precision and Intelligence Laboratory, Tokyo Institute of Technology in 2002. His research interests include natural language processing and information retrieval. </p>
</td>
</tr>
</table>
<div class="divider-full">&nbsp;</div>

<table border="0"  cellpadding="6" bgcolor="#FFFFFF"> 
<tr>
<td align="center"><img src="Takezawa.jpg" class="border" alt="Photo of Toshiyuki Takezawa" width="100" height="148" /></td>
<td>
<p class="blue"><b>Toshiyuki Takezawa </b> is a Professor at the Graduate School of Information Sciences, Hiroshima City University. He received his B.E., M.E., and D.Eng. degrees from Waseda University in 1984, 1986, and 1989. In 1987, he became a research associate at the Centre for Informatics, Waseda University. In 1989, he joined ATR Interpreting Telephony Research Laboratories. His research interests include natural language processing, speech recognition, translation and dialogue systems. </p>
</td>
</tr>
</table>
<div class="divider-full">&nbsp;</div>

 <!-- Standard Copyright line here  -->

<div class="center">


<p class="footer">Copyright &copy; 2012 Satoshi Fukuda, Hidetsugu Nanba and Toshiyuki Takezawa</p>  
  </div>
</td>
 </tr>
</table>

<table width="100%" border="0" align="center" cellpadding="0" cellspacing="0">
  <tr>
    <td height="1" bgcolor="#2b538e"><img src="../../../img2/transparent.gif" alt="transparent image" width="100" height="2" /></td>
  </tr>
</table>

</td></tr></table>
</td></tr></table>
</form>

</body>
</html>