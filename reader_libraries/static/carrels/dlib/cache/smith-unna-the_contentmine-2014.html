<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="DOI" content="10.1045/november14-smith-unna" />
<meta name="description" content="D-Lib Magazine" /> 
<meta name="keywords" content="ContentMine, Tools, Scraping, Mining Scholarly Literature" />
<link rel="metadata" href="11smith-unna.meta.xml" />
<link rel="metadata" href="../11bib.meta.bib" />
<link rel="metadata" href="../11ris.meta.ris" />
<link href="../../../style/style1.css" rel="stylesheet" type="text/css" />
<title>The ContentMine Scraping Stack: Literature-scale Content Mining with Community-maintained Collections of Declarative Scrapers</title>
</head>

<body>
<form action="https://www.dlib.org/cgi-bin/search.cgi" method="get">

<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#2b538e">
<tr>
<td><img src="../../../img2/space.gif" alt="" width="10" height="2" /></td></tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0">
<tr>
<td valign="bottom" colspan="4" align="right" bgcolor="#4078b1">

<table border="0">
<tr>
<td align="right" class="search"><img src="../../../img2/search2.gif" alt="" width="51" height="20" align="middle" />Search D-Lib:</td>

<td>
<input type="text" name="words" value="" size="25" />
</td>

<td align="left" valign="middle">
<input type="submit" name="search" value="Go!" />
<input type="hidden" name="config" value="htdig" />
<input type="hidden" name="restrict" value="" />
<input type="hidden" name="exclude" value="" /> 
</td>
</tr>
</table>

</td></tr></table>

<table width="100%" border="0" cellpadding="0" cellspacing="0">
<tr>
<td valign="bottom" colspan="4">

<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#e04c1e" id="outer" summary="Main Table">
<tr>
<td><img src="../../../img2/space.gif" alt="" width="10" height="1" /></td></tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#F6F6F6" id="bannertable">
  <tr>
    <td width="830" bgcolor="#4078b1" class="backBannerImage" align="left"><img src="../../../img2/D-Lib-blocks.gif" alt="D-Lib Magazine" width="450" height="100" border="0" /></td>
  </tr>
  <tr>
    <td width="830" bgcolor="#e04c1e"><img src="../../../img2/transparent.gif" alt="spacer" height="1" /></td>
  </tr>
  <tr>
    <td width="830" bgcolor="#eda443" align="left"><img src="../../../img2/magazine.gif" alt="The Magazine of Digital Library Research" width="830" height="24" border="0" /></td>
  </tr>
  <tr>
    <td width="830" bgcolor="#e04c1e"><img src="../../../img2/transparent.gif" alt="spacer" height="1" /></td>
   </tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0" id="navtable">
  <tr>
    <td width="5" height="20" bgcolor="#2b538e">&nbsp;</td>
    <td width="24" height="20" bgcolor="#2b538e"><img src="../../../img2/transparent.gif" alt="" width="24" height="20" /></td>
    <td height="20" align="left" bgcolor="#2b538e" class="navtext" nowrap="nowrap"><a href="../../../dlib.html">HOME</a>&nbsp;|&nbsp;<a href="../../../about.html">ABOUT D-LIB</a>&nbsp;|&nbsp;<a href="../../../contents.html" class="navtext">CURRENT ISSUE</a>&nbsp;|&nbsp;<a href="../../../back.html">ARCHIVE</a>&nbsp;|&nbsp;<a href="../../../author-index.html">INDEXES</a>&nbsp;|&nbsp;<a href="../../../groups.html">CALENDAR</a>&nbsp;|&nbsp;<a href="../../author-guidelines.html">AUTHOR GUIDELINES</a>&nbsp;|&nbsp;<a href="https://www.dlib.org/mailman/listinfo/dlib-subscribers">SUBSCRIBE</a>&nbsp;|&nbsp;<a href="../../letters.html">CONTACT D-LIB</a></td>
    <td width="5" height="20" bgcolor="#2b538e">&nbsp;</td>
  </tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0">
  <tr>
    <td width="55" height="1" bgcolor="#e04c1e"><img src="../../../img2/space.gif" alt="transparent image" width="1" height="1" /></td></tr>
</table>

<!-- CONTENT TABLE -->
<table width="100%" border="0" align="center" cellpadding="0" cellspacing="0">
  <tr>
  <td>
 
<!-- BEGIN MAIN CONTENT TABLE -->

<table width="100%" border="0" cellspacing="0" cellpadding="10" bgcolor="#ffffff">
<tr>

<td width="10"><img src="../../../img2/space.gif" alt="" width="1" height="1" /></td>

<td valign="top"> 

<h3 class="blue-space">D-Lib Magazine</h3>
<p class="blue">November/December 2014<br />
Volume 20, Number 11/12<br />
<a href="../11contents.html">Table of Contents</a>
</p> 

<div class="divider-full">&nbsp;</div>

<h3 class="blue-space">The ContentMine Scraping Stack: Literature-scale Content Mining with Community-maintained Collections of Declarative Scrapers</h3>

<p class="blue">
Richard Smith-Unna and Peter Murray-Rust<br /> 
University of Cambridge<br />
{rds45, pm286}&#064;cam.ac.uk


<br /><br />doi:10.1045/november14-smith-unna
 </p>

<div class="divider-full">&nbsp;</div>

<p class="blue"><a href="11smith-unna.print.html" class="fc">Printer-friendly Version</a></p>

<div class="divider-full">&nbsp;</div>

 <!-- Abstract or TOC goes here --> 

<h3 class="blue">Abstract</h3>

<p class="blue">
Successfully mining scholarly literature at scale is inhibited by technical and political barriers that have been only partially addressed by publishers' application programming interfaces (APIs). Many of those APIs have restrictions that inhibit data mining at scale, and while only some publishers actually provide APIs, almost all publishers make their content available on the web. Current web technologies should make it possible to harvest and mine the scholarly literature regardless of the source of publication, and without using specialised programmatic interfaces controlled by each publisher. Here we describe the tools developed to address this challenge as part of the ContentMine project.</p>

<p class="blue">Keywords: ContentMine, Tools, Scraping, Mining Scholarly Literature</p>


<!-- Article goes next --> 

<div class="divider-full">&nbsp;</div>
<h3>1. Introduction</h3>

<p>The need to mine content from the scholarly literature at scale is inhibited by technical and political barriers. Publishers have partially addressed this by providing application programming interfaces (APIs). However, we observe that many publisher APIs come with restrictions that inhibit data mining at scale, and that while only a few publishers provide APIs, almost all publishers make their content available on the web. With current web technologies it should be possible to harvest and mine the entire scholarly literature as it is published, regardless of the source of publication, and without using specialised programmatic interfaces controlled by each publisher. To address this challenge as part of the <a href="http://contentmine.org">ContentMine</a> project we developed the following tools:</p>

<ul>
	<li style="padding-bottom: .5em;"><b>scraperJSON</b>, a standard for defining reusable web scrapers as JSON objects</li>

	<li style="padding-bottom: .5em;"><b>thresher</b>, a web scraping library that uses scraperJSON scrapers and headless browsing to handle many idiosyncrasies of the modern web</li>

	<li style="padding-bottom: .5em;"><b>quickscrape</b>, a command-line web scraping suite that uses thresher</li>

	<li>a library of <b>scraperJSON scrapers</b> to extract data and metadata from many academic publishers</li>
</ul>

<p>We will demonstrate the use of this stack for scraping the scholarly literature.</p>

<div class="divider-full">&nbsp;</div>
<h3>2. Software description</h3>

<div class="divider-dot">&nbsp;</div>
<h3>2.1 scraperJSON</h3>

<p>We defined a new JSON schema, scraperJSON, to enable declarative scraping of structured data from many different sites without duplicating code. A scraperJSON scraper minimally defines the url(s) to which it applies and the set of elements that it can extract. Elements can be extracted by CSS, XPath, or using the Open Annotation standard, and can be post-processed using regular expression capture groups.</p>

<p><b>2.1.1 Implementation</b></p>

<p>We chose to use JSON as the data format because it is widely supported and is the natural data format of JavaScript, in which our web apps and scraping software are written (via Node.js).</p>

<p>The declarative approach enables building tools to enable non-programmers to create and maintain scrapers. This is prohibitively difficult with existing scraper collections, such as that used by the open source reference-manager Zotero.</p>

<p><b>2.1.2 Related work</b></p>

<p>To our knowledge, only one open-source declarative scraping system, <a href="https://github.com/KrakeIO/libkrake">libKrake</a>, exists. However, this software seems to no longer be maintained and did not support many of the features we require, such as simulated interactions and file downloads.</p>

<p><b>2.1.3 Simulating user interaction</b></p>

<p>Many modern websites do not specify their content in the raw HTML served when a URL is visited, but lazy-load the content, for example by making AJAX calls. These loading events are triggered by user interactions such as scrolling and clicking. To enable to scraping of full content, we therefore added to scraperJSON the ability to specify a list of predefined user interactions.</p>

<p><b>2.1.4 Structuring results</b></p>

<p>The structure of the scraperJSON <i>elements</i> object defines the structure of the results. Thus, each key in the elements of the scraper will be reflected as a key in the elements of the results. Elements can contain other elements, so that the results of the child elements are grouped into objects reflecting the structure of the parent element. This allows powerful capture of structured data representing entities and their properties, such as the name, affiliation and email of an author.</p>

<p><b>2.1.5 Downloads</b></p>

<p>When scraping the scholarly literature, it is usually of interest to download files such as fulltext PDFs and HTML, XML and RDF documents where available, and to capture full-sized figure images and supplementary materials. This feature is supported in scraperJSON: elements specifying URLs can have their target(s) downloaded and optionally renamed to a specified format.</p>

<p><b>2.1.6 Nested scrapers</b></p>

<p>Content associated with a page is often available in a more extensive form in a linked resource. This is observed with full figures on the websites of many academic publishers, where a thumbnail is displayed in the article and a link must be followed to expose the full image. This situation can be handled in scraperJSON: if an element targets a URL, the URL can be followed and one or more child elements extracted.</p>

<div class="divider-dot">&nbsp;</div>
<h3>2.2 Scraping with <i>thresher</i> &amp; <i>quickscrape</i></h3>

<p><b>2.2.1 thresher</b></p>

<p><b>Implementation</b></p>

<p><i><b>thresher</b></i> is a web scraping library that uses scraperJSON scrapers. It is written in Node.js, is cross-platform, is fully covered by tests, and is released under the MIT license.</p>

<p>JavaScript was selected as the development language because it is the de-facto language of the web. Node.js in particular allows seamless creation of APIs, command-line tools and GUI tools or webapps using the same codebase, and because the ecosystem of packages simplifies rapid cross-platform development.</p>

<p>Pages are rendered either using the Node.js package <b>jsdom</b>, or in a headless WebKit browser via PhantomJS. PhantomJS was selected over alternatives, such as Selenium WebDriver, due to the requirement that our scraping stack can run on servers without any X windowing system.</p>

<p>The output of each scraping operation is a JSON document that mirrors the structure of the scraper's scraperJSON definition. The format is bibJSON-compatible, allowing conversion to other scholarly metadata standards.</p>

<p><b>Automatic scraper selection</b></p>

<p>By harnessing the url specification in scraperJSON, thresher can process lists of URLs and automatically choose the best scraper for each from a provided collection of scrapers.</p>

<p><b>Headless rendering</b></p>

<p>Headless browsers are standalone web rendering engines with the GUI removed, commonly used for testing web-facing software. Thresher supports the optional user interaction feature in scraperJSON by rendering webpages and performing actions in PhantomJS if interactions are specified.</p>

<p><b>Rate-limiting</b></p>

<p>To encourage responsible scraping, and to avoid triggering denial of service by publishers, thresher implements a domain-based, customisable rate limit. A separate queue of URLs to be scraped is maintained for each domain, and the queues are processed in parallel.</p>

<p><b>Authentication</b></p>

<p>Because much of the scholarly literature is behind a pay-wall, thresher allows authentication by one of three commonly used methods:</p>

<ol>
	<li>using HTTP login/password authentication</li>

	<li>using a proxy</li>

	<li>by providing a set of session cookies</li>
</ol>

<p><b>2.2.2 quickscrape</b></p>

<p><i><b>quickscrape</b></i> is a cross-platform command-line suite that provides access to all the functionality of thresher. An overview of the interaction of the two systems is shown in Figure 1.</p>

<div align="center">
<img src="smith-unna-fig1.png" alt="smith-unna-fig1" width="300" height="706" vspace="10" />
<p class="indentLeft"><i>Figure 1: Schematic overview of the scraping stack. quickscrape provides a command-line interface to the thresher library, allowing users to specify inputs and settings and retrieve results. thresher drives the scraping process by matching URLs to those in the collection of scraperJSON scrapers, rendering the target, and capturing the specified elements.</i></p>
</div>

<div class="divider-dot">&nbsp;</div>
<h4>2.3 Journal scrapers</h4>

<p>We developed a collection of scraperJSON scrapers for major publishers. The scrapers collect article metadata, downloadable resources associated with the article, as well as citations and the full text of the article if available. Maintenance and expansion of the collection is done with the help of a community of volunteers. Each scraper is associated with a list of URLs for which the expected scraping results are known, and an automated testing system (using Github and Travis CI integration) checks that all the scrapers are functioning with perfect precision and recall (i.e. extracting the expected results) every day and after every change to a scraper. This allows a rapid community response to formatting changes by publishers. Accessory scripts are provided for automatically generating tests for scrapers, and for running the tests. The whole collection is released under the CC0 license.</p>

<div class="divider-full">&nbsp;</div>
<h3>3. Future work</h3>

<p>In future work we will enhance the ContentMine scraping stack by creating a layer of web tools enabling non-programmers to develop and maintain scrapers, and to conduct text and data mining directly from the browser using our resources.</p>

<p>All software is open source and available from the <a href="https://github.com/ContentMine">ContentMine Github</a> organisation.</p>

<div class="divider-full">&nbsp;</div>
<h3>Acknowledgements</h3>

<p>Peter Murray-Rust thanks the Shuttleworth Foundation for a fellowship and grant.</p>

<div class="divider-full">&nbsp;</div>
<h3>About the Authors</h3>

<table border="0"  cellpadding="6" bgcolor="#FFFFFF"> 
<tr>
<td>
<p class="blue"><b>Richard Smith-Unna</b> is a PhD student (Plant Sciences) at University of Cambridge and co-founder of Solvers.io.</p>
</td>
</tr>
</table>

<div class="divider-full">&nbsp;</div>

<table border="0"  cellpadding="6" bgcolor="#FFFFFF"> 
<tr>
<td align="center"><img src="murray-rust.png" class="border" alt="murray-rust" width="100" height="125" /></td>
<td>
<p class="blue"><b>Peter Murray-Rust </b> is Reader in Molecular Informatics at the University of Cambridge, and Senior Research Fellow of Churchill College, Cambridge. He was educated at Bootham School and Balliol College, Oxford and obtained a Doctor of Philosophy. His research interests have involved the automated analysis of data in scientific publications, creation of virtual communities and the Semantic Web. In 2002, Dr. Murray-Rust and his colleagues proposed an electronic repository for unpublished chemical data called the World Wide Molecular Matrix (WWMM). In 2014 he was awarded a Fellowship by the Shuttleworth Foundation to develop the automated mining of science from the literature. In addition to his work in chemistry, Murray-Rust is also known for his support of open access and open data.</p>
</td>
</tr>
</table>

<div class="divider-full">&nbsp;</div>


 <!-- Standard Copyright line here  -->

<div class="center">

<p class="footer">Copyright &copy; 2014 Richard Smith-Unna and Peter Murray-Rust</p>  
  </div>
</td>
 </tr>
</table>

<table width="100%" border="0" align="center" cellpadding="0" cellspacing="0">
  <tr>
    <td height="1" bgcolor="#2b538e"><img src="../../../img2/transparent.gif" alt="transparent image" width="100" height="2" /></td>
  </tr>
</table>

</td></tr></table>
</td></tr></table>
</form>

</body>
</html>