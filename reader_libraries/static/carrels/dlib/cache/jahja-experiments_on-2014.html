<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<meta name="DOI" content="10.1045/november14-jahja" />
<meta name="description" content="D-Lib Magazine" /> 
<meta name="keywords" content="Rating Conferences, CORE, DBLP, Automatic Conference Rating Algorithms" />
<link rel="metadata" href="11jahja.meta.xml" />
<link rel="metadata" href="../11bib.meta.bib" />
<link rel="metadata" href="../11ris.meta.ris" />
<link href="../../../style/style1.css" rel="stylesheet" type="text/css" />
<title>Experiments on Rating Conferences with CORE and DBLP</title>
</head>

<body>
<form action="https://www.dlib.org/cgi-bin/search.cgi" method="get">

<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#2b538e">
<tr>
<td><img src="../../../img2/space.gif" alt="" width="10" height="2" /></td></tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0">
<tr>
<td valign="bottom" colspan="4" align="right" bgcolor="#4078b1">

<table border="0">
<tr>
<td align="right" class="search"><img src="../../../img2/search2.gif" alt="" width="51" height="20" align="middle" />Search D-Lib:</td>

<td>
<input type="text" name="words" value="" size="25" />
</td>

<td align="left" valign="middle">
<input type="submit" name="search" value="Go!" />
<input type="hidden" name="config" value="htdig" />
<input type="hidden" name="restrict" value="" />
<input type="hidden" name="exclude" value="" /> 
</td>
</tr>
</table>

</td></tr></table>

<table width="100%" border="0" cellpadding="0" cellspacing="0">
<tr>
<td valign="bottom" colspan="4">

<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#e04c1e" id="outer" summary="Main Table">
<tr>
<td><img src="../../../img2/space.gif" alt="" width="10" height="1" /></td></tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0" bgcolor="#F6F6F6" id="bannertable">
  <tr>
    <td width="830" bgcolor="#4078b1" class="backBannerImage" align="left"><img src="../../../img2/D-Lib-blocks.gif" alt="D-Lib Magazine" width="450" height="100" border="0" /></td>
  </tr>
  <tr>
    <td width="830" bgcolor="#e04c1e"><img src="../../../img2/transparent.gif" alt="spacer" height="1" /></td>
  </tr>
  <tr>
    <td width="830" bgcolor="#eda443" align="left"><img src="../../../img2/magazine.gif" alt="The Magazine of Digital Library Research" width="830" height="24" border="0" /></td>
  </tr>
  <tr>
    <td width="830" bgcolor="#e04c1e"><img src="../../../img2/transparent.gif" alt="spacer" height="1" /></td>
   </tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0" id="navtable">
  <tr>
    <td width="5" height="20" bgcolor="#2b538e">&nbsp;</td>
    <td width="24" height="20" bgcolor="#2b538e"><img src="../../../img2/transparent.gif" alt="" width="24" height="20" /></td>
    <td height="20" align="left" bgcolor="#2b538e" class="navtext" nowrap="nowrap"><a href="../../../dlib.html">HOME</a>&nbsp;|&nbsp;<a href="../../../about.html">ABOUT D-LIB</a>&nbsp;|&nbsp;<a href="../../../contents.html" class="navtext">CURRENT ISSUE</a>&nbsp;|&nbsp;<a href="../../../back.html">ARCHIVE</a>&nbsp;|&nbsp;<a href="../../../author-index.html">INDEXES</a>&nbsp;|&nbsp;<a href="../../../groups.html">CALENDAR</a>&nbsp;|&nbsp;<a href="../../author-guidelines.html">AUTHOR GUIDELINES</a>&nbsp;|&nbsp;<a href="https://www.dlib.org/mailman/listinfo/dlib-subscribers">SUBSCRIBE</a>&nbsp;|&nbsp;<a href="../../letters.html">CONTACT D-LIB</a></td>
    <td width="5" height="20" bgcolor="#2b538e">&nbsp;</td>
  </tr>
</table>

<table width="100%" border="0" cellpadding="0" cellspacing="0">
  <tr>
    <td width="55" height="1" bgcolor="#e04c1e"><img src="../../../img2/space.gif" alt="transparent image" width="1" height="1" /></td></tr>
</table>

<!-- CONTENT TABLE -->
<table width="100%" border="0" align="center" cellpadding="0" cellspacing="0">
  <tr>
  <td>
 
<!-- BEGIN MAIN CONTENT TABLE -->

<table width="100%" border="0" cellspacing="0" cellpadding="10" bgcolor="#ffffff">
<tr>

<td width="10"><img src="../../../img2/space.gif" alt="" width="1" height="1" /></td>

<td valign="top"> 

<h3 class="blue-space">D-Lib Magazine</h3>
<p class="blue">November/December 2014<br />
Volume 20, Number 11/12<br />
<a href="../11contents.html">Table of Contents</a>
</p> 

<div class="divider-full">&nbsp;</div>

<h3 class="blue-space">Experiments on Rating Conferences with CORE and DBLP</h3>

<p class="blue">
Irvan Jahja, Suhendry Effendy and Roland H. C. Yap<br /> 
National University of Singapore<br /><br />

Corresponding Author: Roland H. C. Yap (ryap&#064;comp.nus.edu.sg)

<br /><br />doi:10.1045/november14-jahja
 </p>

<div class="divider-full">&nbsp;</div>

<p class="blue"><a href="11jahja.print.html" class="fc">Printer-friendly Version</a></p>

<div class="divider-full">&nbsp;</div>

 <!-- Abstract or TOC goes here --> 

<h3 class="blue">Abstract</h3>

<p class="blue">
Conferences are the lifeblood of research in computer science and they vary in reputation and perceived quality. One guidance for conferences is conference ratings which attempt to classify conferences by some quality measures. In this paper, we present a preliminary investigation on the possibility of systematic unbiased and automatic conference rating algorithms using random walks which give relatedness score for a conference relative to a reference conference (pivot). Our experiments show that just using a simple algorithm on the DBLP bibliographic database, it is possible to get ratings which correlate well with the CORE and CCF conference ratings.
</p>

<!-- Article goes next --> 

<div class="divider-full">&nbsp;</div>
<h3>1. Introduction</h3>

<p>Conferences are the lifeblood of research in computer science. There are thousands of conferences across a variety of areas. Conferences vary in reputation and perceived quality. One guidance for conferences is conference ratings which attempt to classify conferences by some quality measures. For example, <a href="http://core.edu.au/index.php">CORE</a> and <a href="http://www.ccf.org.cn/sites/ccf/paiming.jsp">CCF</a>, are two well known and widely used conference lists which give such ratings.</p>

<p>In this paper, we present a preliminary investigation on the possibility of systematic unbiased and automatic conference rating algorithms. While one can argue that conference rating cannot be done automatically, the advantage of unbiased methods is that they can inform on how one might determine such ratings. Starting with a measure using random walks which gives a relatedness score for a conference relative to a reference conference (pivot) [<a href="11jahja.html#4">4</a>], we make use of a set of suitable reference pivots to systematically rate conferences. We experiment to see how well this simple algorithm correlates with the CORE and CCF ratings using the DBLP bibliographic database (<a href="http://www.informatik.uni-trier.de/~ley/db/">DBLP</a>). While differences in ratings between the various rating methods should be expected, the preliminary results show that our conference rating correlate reasonably well with CORE and CCF. It is surprising that a simple method which does not use any semantics, essentially graph analysis, is effective. We believe this is due to the strong correlation between the collaboration network of computer scientists with how papers are published in conferences.</p>

<div class="divider-full">&nbsp;</div>
<h3>2. Related Work</h3>

<p>Scientific collaboration networks in computer science have been studied by many researchers [<a href="11jahja.html#1">1</a>, <a href="11jahja.html#2">2</a>, <a href="11jahja.html#3">3</a>], in particular, the DBLP author collaboration network by Elmacioglu, <i>et al</i>. [<a href="11jahja.html#2">2</a>]. However, there have not been many works which investigate or employ conference ratings.  Bird, <i>et al</i>., investigated the collaboration style between authors and areas in computer science [<a href="11jahja.html#1">1</a>]. They determined the areas of interest and manually selected several conferences to represent the areas. Although they did not explicitly consider rating in their conference selection, we noticed that most of the selected conferences have a high rating. Recently, we proposed several measures of relatedness and found that the top conferences with high relatedness scores (top 10-20) correlated well with A* and A rated conferences in the CORE list [<a href="11jahja.html#4">4</a>]. This paper extends this direction to predict conference ratings.</p>

<div class="divider-full">&nbsp;</div>
<h3>3. Background</h3>

<p>A scientific bibliography database, such as DBLP, contains information on paper title, authors, year of publication, publication venue (conference, journal, workshop, etc.), and many other details. Each conference <i>c</i> has a multiset of authors (as an author might publish many times in <i>c</i>), where &alpha;(<i>c</i>) represents the authors who publish in <i>c</i>. We build a conference graph from <i>c</i> and &alpha;(<i>c</i>) as follows: a conference graph <i>G = (V, E)</i> contains each conference <i>c</i> as a vertex, and each edge (<i>c<sub>i</sub></i>, <i>c<sub>j</sub></i>) has a weight <i>w<sub>i,j</sub></i> [<a href="11jahja.html#4">4</a>]. The weight <i>w<sub>i,j</sub></i> is meant to denote a relatedness measure between conferences <i>c<sub>i</sub></i> and <i>c<sub>j</sub></i> based on the authors publishing there and is defined by the <i>Jaccard index</i> on the multisets &alpha;(c<sub>i</sub>) and &alpha;(c<sub>j</sub>).The Jaccard index of sets <i>X</i> and <i>Y</i> is defined as |<i>X</i> &cap; <i>Y</i>| / |<i>X</i> &cup; <i>Y</i>|, i.e. the size of intersection over the size of union of the two sets.</p>

<p>The idea of relatedness measure is to relate conferences relative to a base conference called the <i>pivot</i> where the pivot is meant to be an influential conference, e.g., a rank A conference. In our earlier work [<a href="11jahja.html#4">4</a>], we proposed three different approaches to measure the relatedness between conferences through selected pivots, i.e. Direct Jaccard Index, Random Walk (RW), and Pivot Aggregation. It was observed that RW gives a better score for higher rated conferences if the pivot also has a high rating. Hence, in this paper we will focus on the RW measure.</p>

<p>In the RW approach, relatedness is measured by the expected number of times vertex <i>i</i> is visited in a random walk up to length <i>L</i>. The transition probability of visiting <i>j</i> from <i>i</i> is <i>w'<sub>i,j</sub></i> obtained by normalizing the Jaccard index as <i>w'<sub>i,j</sub> = w<sub>i,j</sub> / &omega;<sub>i</sub></i> for <i>i &ne; j</i>, otherwise 0, and <i>&omega;<sub>i</sub> = &Sigma;<sub>j&ne;i</sub> w<sub>i,j</sub></i> . Let the <i>i</i>-th element of vector <i>R<sup>p</sup></i> be the RW relatedness score of conference <i>c<sub>i</sub></i> to pivot <i>p</i>, and <i>W'</i> be a matrix with transition probabilities <i>w'<sub>i,j</sub></i> . <i>R<sup>p</sup></i> is computed as follows where <i>E<sup>p</sup>(s)</i> is a vector giving the probability for a walk of length <i>s</i> from <i>p</i> to reach a conference:</p>

<div align="center">
<img src="jahja-fig1.png" alt="jahja-fig1" width="300" height="142" vspace="30" />
</div>

<p>As we want to find conferences which are related (i.e. near) to a pivot, the random walk length should be short.</p>

<div class="divider-full">&nbsp;</div>
<h3>4. Conference Rating by Relatedness Ranking</h3>

<p>The preliminary experiments from [<a href="11jahja.html#4">4</a>] suggested that relatedness measure to a pivot correlated well with the A*/A conference rating in CORE for the top 10-20 conferences from several areas. We now propose how to predict conference ratings from the conference relatedness measure relative to a pivot. The assumption is that given a top rated pivot conference, conferences in the area whose relatedness measure is closer to the pivot are also expected to have a rating closer to the pivot. Correspondingly, conferences which are less related are expected to have a lower rating. Every conference can be scored by its rank relative to the pivot; a lower rank means that its relatedness score is closer to the pivot.</p>

<p>However using a single pivot is likely not sufficient. This is because the rating for a conference is typically connected to only particular areas or topics, e.g. the rating for conferences in the database area may not be meaningful in a graphics context. As such, we will consider a set of top rated pivot conferences, to give good coverage across a wide range of conferences. The relatedness ranking of a conference is defined as its minimum rank with respect to the pivots. In order to turn the relatedness ranking into a rating, we will assume that there is a target bound on how many conferences can have a given rating which seems a reasonable assumption.</p>

<p>We now describe our conference rating technique based on the above ideas. The following notation is used:</p>
<ul>
<li style="padding-bottom: .5em;"><i>C</i> is the set of all conferences;</li>
<li style="padding-bottom: .5em;"><i>P</i> is the chosen set of pivot conferences where <i>P</i> &sub; <i>C</i>;</li>
<li style="padding-bottom: .5em;"><i>R<sup>p</sup></i> is a vector containing relatedness score of all conferences to pivot <i>p</i> by RW as described in Sec. 2, and <i>R</i> is a set of the vectors <i>R<sup>p</sup></i> given pivots <i>P</i>;</li>
<li style="padding-bottom: .5em;"><i>S<sup>p</sup></i> is a vector derived from <i>R<sup>p</sup></i> giving the rank to pivot <i>p</i> described below, and <i>S</i> is a set of the vectors <i>S<sup>p</sup></i>;</li>
<li style="padding-bottom: .5em;"><i>L</i> is a sorted list of rating labels from high to low, e.g. &lsaquo; A, B, ... &rsaquo;;</li>
<li>and, <i>Y</i> is a vector giving the computed conference ratings.</li>
</ul>

<p>We define <i>N<sub>label</sub></i> as the maximum number of conferences which should be rated as <i>label</i> or better. For example, <i>N<sub>B</sub></i> = 100 means there should be at most 100 conferences which rated as A or B. Naturally, <i>N<sub>label</sub></i> is the total number of conferences when <i>label</i> is the lowest possible rating.</p>

<p>To simplify the description of the algorithm, we define another vector <i>S<sup>p</sup></i> which serves as a kind of inverted index of vector <i>R<sup>p</sup></i>. The value of <i>S<sup>p</sup></i> is its rank in a sorted version of <i>R<sup>p</sup></i>. Specifically, <i>S<sup>p</sup><sub style='position: relative; left: -.5em'>i</sub></i> = <i>x</i> if and only if <i>R<sup>p</sup><sub style='position: relative; left: -.5em'>i</sub></i> is the <i>x</i>-th highest score in <i>R<sup>p</sup></i>. As there are a set of pivots <i>P</i>, then <i>S</i> denotes a set of <i>S<sup>p</sup></i> for each <i>p</i> &isin; <i>P</i>.</p>

<div align="center">
<img src="jahja-fig2.png" alt="jahja-fig2" width="400" height="347" vspace="30" />
</div>

<p>The classification algorithm is given in Algorithm 1 (<small>CLASSIFY</small>). The algorithms are presented for ease of explanation rather than the actual implementation. Starting from the highest to the lowest labels, we first find a threshold <i>T</i> for each <i>label</i> satisfying <i>N<sub>label</sub></i>, denoting the rank threshold for a conference to be labeled as <i>label</i>. It uses <i>NConfRank</i> defined as how many conferences across all pivots are the <i>k</i>-th closest to a pivot, making use of the set of vectors <i>S</i>, more specifically,</p>

<div align="center">
<img src="jahja-fig3.png" alt="jahja-fig3" width="300" height="30" vspace="10" />
</div>

<p>As <i>NConfRank</i> is monotonic on <i>k</i>, a specific implementation of Algorithm 1 can use binary search on <i>S</i>. This algorithm uses the simple assumption that a single common threshold suffices for all areas/pivots but it is possible to have more complex variants with varying thresholds. Once the thresholds are found for each label, the label of each conference can be determined using Algorithm 2 (<small>GET</small>R<small>ATING</small>). Consider the threshold for the highest label which is also that of the pivots <i>P</i>, e.g. label A, all the conferences whose ranked distance based on the RW measure to some pivot in <i>P</i> which are less than the threshold are classified as label A. The next threshold is then used for conferences labeled as B and so on. In Algorithm 2, the notation <i>min<sub>p</sub>{S<sup>p</sup><sub style='position: relative; left: -.5em'>c</sub>}</i> gives the least value of <i>S</i> for conference <i>c</i> with any pivot.</p>

<div align="center">
<img src="jahja-fig4.png" alt="jahja-fig4" width="500" height="253" vspace="10" />
<p>Figure 1: Example of mapping from <i>R<sup>p</sup></i> to <i>S<sup>p</sup></i></p>
</div>

<p>Figure 1 shows an example mapping from <i>R<sup>p</sup></i> to <i>S<sup>p</sup></i> for 8 conferences and 3 pivots (recall that <i>S<sup>p</sup><sub style='position: relative; left: -.5em'>i</sub></i> contains the rank of <i>R<sup>p</sup><sub style='position: relative; left: -.5em'>i</sub></i> in vector <i>R<sup>p</sup></i>). Suppose we want to classify the conferences in Figure 1 into a rating list &lsaquo; A, B, C &rsaquo; with &lsaquo; 3, 3, 2 &rsaquo; number of conferences respectively in each category. In other words, <i>N<sub>A</sub></i> = 3, <i>N<sub>B</sub></i> = 6, and <i>N<sub>C</sub></i> = 8. Algorithm 1 will classify {<i>e</i>, <i>g</i>} as A, {<i>a</i>, <i>b</i>, <i>c</i>, <i>h</i>} as B, and {<i>d</i>, <i>f</i>} as C. The corresponding threshold <i>T</i>'s are <i>T<sub>A</sub></i> = 1 (i.e. conferences where the best rank is at most <i>T<sub>A</sub></i> = 1 will be classified as A), <i>T<sub>B</sub></i> = 3, and <i>T<sub>C</sub></i> = 4.</p>

<div class="divider-full">&nbsp;</div>
<h3>5. Dataset</h3>

<p><a href="http://www.informatik.uni-trier.de/~ley/db/">DBLP</a> is a public and widely used computer science bibliographic database. We downloaded the DBLP XML data on 23 December 2013 and extracted entries which correspond to paper published in conferences.</p>

<p>DBLP does not contain any conference ratings. For conference ratings, we used two well known and widely used conference rating lists, CORE and CCF. The Computer Research and Education Association of Australasia (<a href="http://core.edu.au/index.php">CORE</a>) gives conferences ratings in 4 categories (A*, A, B, C) where A* is meant to be the most reputable while C be the least (note that this is different from <a href="http://core.kmi.open.ac.uk">CORE project</a>). We encountered several issues when mapping the data from CORE to DBLP: (i) inconsistent conference naming; (ii) multiple entries for the same conference; and (iii) name-abbreviation mapping differences between CORE and DBLP. After some data cleanup, we obtained ratings for the conferences in DBLP which appear in CORE, which we call "<i>DBLP data</i>" (or simply DBLP when the context is clear).</p>

<p>We also employed the conference list from China Computer Federation (<a href="http://www.ccf.org.cn/sites/ccf/paiming.jsp">CCF</a>) in our experiments. The CCF list is used heavily in China. This list contains considerably fewer conferences compared to CORE. Out of 309 conferences, we found 255 matched our DBLP data. CCF rates each conference into 3 categories (A, B, C). Note that the CCF and CORE ratings differ, though there are common points of agreement. Table 1 shows the rating distribution for CORE and CCF.</p>

<p class="indent"><b>Table 1: CORE and CCF Rating Distribution</b></p>
<table align="center" border="0" cellpadding="12" cellspacing="0">

<tr>
<td class="topLeft"><b>Rating</b></td>
<td class="topLeft"><b>CORE</b></td>
<td class="topLeftRight"><b>CCF</b></td>
</tr>

<tr>
<td class="topLeft" align="center">A*</td>
<td class="topLeft" align="center">61</td>
<td class="topLeftRight" align="center">&#151;</td>
</tr>

<tr>
<td class="topLeft"  align="center">A</td>
<td class="topLeft" align="center">187</td>
<td class="topLeftRight" align="center">38</td>
</tr>

<tr>
<td class="topLeft"  align="center">B</td>
<td class="topLeft" align="center">319</td>
<td class="topLeftRight" align="center">108</td>
</tr>

<tr>
<td class="topLeft" align="center">C</td>
<td class="topLeft" align="center">378</td>
<td class="topLeftRight" align="center">109</td>
</tr>

<tr>
<td class="topLeftBottom" align="center">&Sigma;</td>
<td class="topLeftBottom" align="center">945</td>
<td class="all" align="center">255</td>
</tr> 
</table>

<p>Microsoft Academic Search (<a href="http://academic.research.microsoft.com">Libra</a>), which we will refer to as Libra, is an experimental search engine service developed by Microsoft Research for academic papers. We also experiment with the conferences found in Libra.</p>

<div class="divider-full">&nbsp;</div>
<h3>6. Experiments and Discussion</h3>

<p>We experiment with how our RW classification algorithm compares with actual conference ratings from CORE and CCF. We do not expect that it can match perfectly, which would be rather surprising. Nevertheless, it would be interesting to see how our simple algorithm can perform.</p>

<p>As RW uses pivots, we selected the following 15 conferences which have A* rating in CORE as individual pivots
for the RW algorithm (these abbreviations follow DBLP):</p>

<p class="indentLeft">POPL, ICSE, SIGMOD, ICML, KDD, CHI, SIGCOMM, SP, CRYPTO, SIGGRAPH, LICS, AAAI, STOC, ISCA, HUC</p>

<p>This choice was made manually but was intended to have broad coverage though may not be complete coverage. For example, POPL covers programming languages, ICSE for software engineering, SIGMOD for database, etc. The random walk length was set to be 4 following [<a href="11jahja.html#4">4</a>]. In order to make the result consistent and comparable between CORE and CCF, we have used the following settings for our classification algorithm:</p>

<ul>
<li style="padding-bottom: .5em;">All A* conferences in CORE are mapped to A as CCF does not have A* rating. This gives the label list <i>L</i> = &lsaquo; A, B, C &rsaquo;.</li>
<li style="padding-bottom: .5em;">The target upper bounds for N are based on the number of such conferences in CORE, specifically, <i>N<sub>A</sub></i> = 248, <i>N<sub>B</sub></i> = 567, and <i>N<sub>C</sub></i> = 945 (note that <i>N<sub>B</sub></i> = 248 + 319).</li>
</ul>

<div class="divider-dot">&nbsp;</div>
<h4>Rating Differences between CORE and CCF</h4>

<p>We first contrast the conference ratings in CORE and CCF. This is to illustrate that conference rating is inherently subjective and depend very much on how such lists are constructed. As far as we are aware, conference rating lists are usually manually constructed.</p>

<p>Given their manual construction, we should expect that the ratings will disagree on various conferences. In other words, we expect agreement for some conferences having common ratings and different ratings for others. We show this using a confusion matrix. Table 2 gives the confusion matrix between the ratings in CORE and CCF. Each row corresponds to the number of conference with such a rating in CORE, while each column corresponds to CCF's. For example, CORE and CCF agree on 37 conferences rated as A (or A* in CORE), however, there are 86 conferences rated A in CORE but rated B in CCF. From this table, we can see that CCF is more conservative compared to CORE; with the upper right triangle of the matrix having larger values. Note that perfect agreement in rating will result in all non-diagonal elements being zero in the confusion matrix.</p>

<p class="indent"><b>Table 2: Confusion Matrix between CORE and CCF</b></p>
<table align="center" border="0" cellpadding="12" cellspacing="0">

<tr>
<td class="topLeft"><b>&nbsp;</b></td>
<td class="topLeft"><b>A</b></td>
<td class="topLeft"><b>B</b></td>
<td class="topLeft"><b>C</b></td>
<td class="topLeftRight"><b>&Sigma;</b></td>
</tr>

<tr>
<td class="topLeft" align="center"><b>A</b></td>
<td class="topLeft" align="center">37</td>
<td class="topLeft" align="center">86</td>
<td class="topLeft" align="center">40</td>
<td class="topLeftRight" align="center">163</td>
</tr>

<tr>
<td class="topLeft"  align="center"><b>B</b></td>
<td class="topLeft" align="center">1</td>
<td class="topLeft" align="center">16</td>
<td class="topLeft" align="center">48</td>
<td class="topLeftRight" align="center">65</td>
</tr>

<tr>
<td class="topLeft"  align="center"><b>C</b></td>
<td class="topLeft" align="center">0</td>
<td class="topLeft" align="center">6</td>
<td class="topLeft" align="center">21</td>
<td class="topLeftRight" align="center">27</td>
</tr>

<tr>
<td class="topLeftBottom" align="center">&Sigma;</td>
<td class="topLeftBottom" align="center">38</td>
<td class="topLeftBottom" align="center">108</td>
<td class="topLeftBottom" align="center">109</td>
<td class="all" align="center">255</td>
</tr> 
</table>

<div class="divider-white">&nbsp;</div>
<div class="divider-dot">&nbsp;</div>
<h4>Classification based on RW</h4>

<p>We see from comparing CORE with CCF, Table 2, that we should expect that there will be differences and non-zero elements in the non-diagonal elements when comparing two conference rating lists. We now experiment with our RW classification to see how it compares to these two well known rating lists.</p>

<p>Table 3 shows the confusion matrix between conference ratings in CORE and the result of our classification algorithm. Each row corresponds to CORE's and each column corresponds to ours. We can see there is strong positive correlation between our result and CORE with the diagonal entries being dominant. We note that this is stronger than when comparing CORE with CCF. Moreover, in the event of disagreement for the A and C classifications, most conferences are classified to the neighboring rating (A to B, or C to B). One could argue that the more significant difference in conference rating lies between A and C where B is intermediate and perhaps some A and B conferences might interchange labels, and similarly B and C might interchange. Furthermore, such conference ratings which are manually constructed have a subjective element while our algorithm is unbiased and deterministic.</p>

<p class="indent"><b>Table 3: Confusion Matrix between CORE and RW</b></p>
<table align="center" border="0" cellpadding="12" cellspacing="0">

<tr>
<td class="topLeft"><b>&nbsp;</b></td>
<td class="topLeft"><b>A</b></td>
<td class="topLeft"><b>B</b></td>
<td class="topLeft"><b>C</b></td>
<td class="topLeftRight"><b>&Sigma;</b></td>
</tr>

<tr>
<td class="topLeft" align="center"><b>A</b></td>
<td class="topLeft" align="center">157</td>
<td class="topLeft" align="center">66</td>
<td class="topLeft" align="center">25</td>
<td class="topLeftRight" align="center">248</td>
</tr>

<tr>
<td class="topLeft"  align="center"><b>B</b></td>
<td class="topLeft" align="center">60</td>
<td class="topLeft" align="center">150</td>
<td class="topLeft" align="center">109</td>
<td class="topLeftRight" align="center">319</td>
</tr>

<tr>
<td class="topLeft"  align="center"><b>C</b></td>
<td class="topLeft" align="center">26</td>
<td class="topLeft" align="center">108</td>
<td class="topLeft" align="center">244</td>
<td class="topLeftRight" align="center">378</td>
</tr>

<tr>
<td class="topLeftBottom" align="center">&Sigma;</td>
<td class="topLeftBottom" align="center">243</td>
<td class="topLeftBottom" align="center">324</td>
<td class="topLeftBottom" align="center">378</td>
<td class="all" align="center">945</td>
</tr> 
</table>

<p>We will examine some of the conferences which are classified differently by our algorithm, namely, AMCIS and CSB are rated A in CORE but classified as C by our algorithm. AMCIS is a conference in the area of Information Systems. One reason for those differences is because we have been conservative in the choice of pivots. In this case, we have not chosen a pivot which is in the area of Information Systems. The same also applies to CSB (a conference in the area of Bioinformatics) we have not used a pivot which is in Bioinformatics or Computational Biology. On the other hand, there are conferences which are rated C by CORE but classified as A by our algorithm. For instance, TAPSOFT is rated as C in CORE, but our algorithm classifies it as A because it shows a close relation to two of the pivots, i.e. POPL and LICS. We remark that TAPSOFT is rated as B in CCF and has evolved to the ETAPS joint conferences of which many are rated as A in CORE.</p>

<p>We also evaluated our result against the CCF ratings. In order to do this, we have retained the same classification result as in Table 3, selected those conferences appearing in CORE and CCF, i.e. the DBLP data which maps to both CORE and CCF, giving 255 conferences in total. Table 4 shows the confusion matrix. Similar to the CORE result, we also observed a positive correlation between CCF and our classification. Although there are disagreements between CORE and CCF, our algorithm still has a positive correlation to both.</p>

<p class="indent"><b>Table 4: Confusion Matrix between CCF and RW</b></p>
<table align="center" border="0" cellpadding="12" cellspacing="0">

<tr>
<td class="topLeft"><b>&nbsp;</b></td>
<td class="topLeft"><b>A</b></td>
<td class="topLeft"><b>B</b></td>
<td class="topLeft"><b>C</b></td>
<td class="topLeftRight"><b>&Sigma;</b></td>
</tr>

<tr>
<td class="topLeft" align="center"><b>A</b></td>
<td class="topLeft" align="center">22</td>
<td class="topLeft" align="center">13</td>
<td class="topLeft" align="center">3</td>
<td class="topLeftRight" align="center">38</td>
</tr>

<tr>
<td class="topLeft"  align="center"><b>B</b></td>
<td class="topLeft" align="center">6</td>
<td class="topLeft" align="center">68</td>
<td class="topLeft" align="center">34</td>
<td class="topLeftRight" align="center">108</td>
</tr>

<tr>
<td class="topLeft"  align="center"><b>C</b></td>
<td class="topLeft" align="center">2</td>
<td class="topLeft" align="center">30</td>
<td class="topLeft" align="center">77</td>
<td class="topLeftRight" align="center">109</td>
</tr>

<tr>
<td class="topLeftBottom" align="center">&Sigma;</td>
<td class="topLeftBottom" align="center">30</td>
<td class="topLeftBottom" align="center">111</td>
<td class="topLeftBottom" align="center">114</td>
<td class="all" align="center">255</td>
</tr> 
</table>

<p>We also experimented with the conferences in Libra, mapping the DBLP data to conferences in CORE and Libra. Libra does not have any conference ratings, thus, we only use it to select conferences from CORE. Table 5a shows the confusion matrix for the CORE conferences in Libra (CORE &cap; Libra) from DBLP data with our algorithm. Table 5b shows the confusion matrix for the remaining CORE conferences which are not in Table 5a. In both cases, we have used the labels classified from our algorithm running on CORE, i.e. the subset of conferences with labels from Table 3. Again, we observed positive correlation.</p>

<table align="center" cellspacing="3" cellpadding="3">

<tr>
<td colspan="3">
<p class="indent"><b>Table 5: Confusion Matrix for Libra</b></p>
</td>
</tr>

<tr>
<td>
<table align="center" border="0" cellpadding="12" cellspacing="0">
<tr>
<td class="topLeft"><b>&nbsp;</b></td>
<td class="topLeft"><b>A</b></td>
<td class="topLeft"><b>B</b></td>
<td class="topLeft"><b>C</b></td>
<td class="topLeftRight"><b>&Sigma;</b></td>
</tr>

<tr>
<td class="topLeft" align="center"><b>A</b></td>
<td class="topLeft" align="center">123</td>
<td class="topLeft" align="center">54</td>
<td class="topLeft" align="center">17</td>
<td class="topLeftRight" align="center">194</td>
</tr>

<tr>
<td class="topLeft"  align="center"><b>B</b></td>
<td class="topLeft" align="center">46</td>
<td class="topLeft" align="center">117</td>
<td class="topLeft" align="center">82</td>
<td class="topLeftRight" align="center">245</td>
</tr>

<tr>
<td class="topLeft"  align="center"><b>C</b></td>
<td class="topLeft" align="center">20</td>
<td class="topLeft" align="center">78</td>
<td class="topLeft" align="center">166</td>
<td class="topLeftRight" align="center">264</td>
</tr>

<tr>
<td class="topLeftBottom" align="center">&Sigma;</td>
<td class="topLeftBottom" align="center">189</td>
<td class="topLeftBottom" align="center">249</td>
<td class="topLeftBottom" align="center">265</td>
<td class="all" align="center">703</td>
</tr> 
</table>
</td>

<td><img width="20" height="1" src="../../../img2/spacer.gif" alt="spacer" /></td>

<td>
<table align="center" border="0" cellpadding="12" cellspacing="0">

<tr>
<td class="topLeft"><b>&nbsp;</b></td>
<td class="topLeft"><b>A</b></td>
<td class="topLeft"><b>B</b></td>
<td class="topLeft"><b>C</b></td>
<td class="topLeftRight"><b>&Sigma;</b></td>
</tr>

<tr>
<td class="topLeft" align="center"><b>A</b></td>
<td class="topLeft" align="center">30</td>
<td class="topLeft" align="center">18</td>
<td class="topLeft" align="center">6</td>
<td class="topLeftRight" align="center">64</td>
</tr>

<tr>
<td class="topLeft"  align="center"><b>B</b></td>
<td class="topLeft" align="center">13</td>
<td class="topLeft" align="center">29</td>
<td class="topLeft" align="center">32</td>
<td class="topLeftRight" align="center">74</td>
</tr>

<tr>
<td class="topLeft"  align="center"><b>C</b></td>
<td class="topLeft" align="center">8</td>
<td class="topLeft" align="center">29</td>
<td class="topLeft" align="center">77</td>
<td class="topLeftRight" align="center">114</td>
</tr>

<tr>
<td class="topLeftBottom" align="center">&Sigma;</td>
<td class="topLeftBottom" align="center">51</td>
<td class="topLeftBottom" align="center">76</td>
<td class="topLeftBottom" align="center">115</td>
<td class="all" align="center">242</td>
</tr> 
</table>
</td>
</tr>

<tr>
<td align="center">
<p>(a) CORE &cap; Libra vs. RW</p>
</td>
<td><img width="20" height="1" src="../../../img2/spacer.gif" alt="spacer" /></td>
<td align="center">
<p>(a) CORE &cap; &not;Libra vs. RW</p>
</td>
</tr>
</table>

<p>The preliminary experimental results show that a simple unbiased algorithm can give results with good correlation to existing conference rating lists. Clearly it would be impossible to get very high accuracy since such lists are usually manually constructed and thus is unlikely to be replicated by any algorithm. Still it is somewhat surprising that the RW classification gets good correlation with both CORE and CCF. We believe the results show that there is a good correlation between the social graph of authors in computer science with how authors publish in conferences. Our classification does depend on the pivot selection. We intend to experiment with more and varied pivots including automatic selection. We also intend to investigate variations which are less conservative than the conservative thresholds here.</p>

<div class="divider-full">&nbsp;</div>
<h3>Acknowledgements</h3>

<p>This work has been supported by grant T1 251RES1207.</p>

<div class="divider-full">&nbsp;</div>
<h3>References</h3>

<p><a name="1">[1]</a> C. Bird, E. Barr, A. Nash, P. Devanbu, V. Filkov, and Z. Su. (2009). "Structure and dynamics of research collaboration in computer science", International Conference on Data Mining (SDM '09). <a href="https://doi.org/10.1137/1.9781611972795.71">http://doi.org/10.1137/1.9781611972795.71</a></p>

<p><a name="2">[2]</a> E. Elmacioglu, D. Lee. (2005). "On six degrees of separation in DBLP-DB and more", ACM SIGMOD Conference. <a href="https://doi.org/10.1145/1083784.1083791">http://doi.org/10.1145/1083784.1083791</a></p>

<p><a name="3">[3]</a> M. Biryukov, C. Dong. (2010). "Analysis of computer science communities based on DBLP", 14th European conference on Research and advanced technology for digital libraries (ECDL '10). <a href="https://doi.org/10.1007/978-3-642-15464-5_24">http://doi.org/10.1007/978-3-642-15464-5_24</a></p>

<p><a name="4">[4]</a> S. Effendy, I. Jahja, R. H. C. Yap. (2014). "Relatedness measures between conferences in computer science - a preliminary study based on DBLP", WWW Workshop on Big Scholarly Data: Towards the Web of Scholars. <a href="https://doi.org/10.1145/2567948.2579035">http://doi.org/10.1145/2567948.2579035</a></p>

<p><a name="5">[5]</a>  M. Biryukov, C. Dong. Analysis of computer science communities based on DBLP. In ECDL, 2010.</p>

<div class="divider-full">&nbsp;</div>
<h3>About the Authors</h3>

<table border="0"  cellpadding="6" bgcolor="#FFFFFF"> 
<tr>
<td align="center"><img src="ijahja.png" class="border" alt="ijahja" width="100" height="133" /></td>
<td>
<p class="blue"><b>Irvan Jahja</b> is a PhD student in the School of Computing at the National University of Singapore.</p>
</td>
</tr>
</table>

<div class="divider-full">&nbsp;</div>

<table border="0"  cellpadding="6" bgcolor="#FFFFFF"> 
<tr>
<td align="center"><img src="seffendy.jpg" class="border" alt="seffendy" width="100" height="134" /></td>
<td>
<p class="blue"><b>Suhendry Effendy</b> is a PhD student in the School of Computing at the National University of Singapore. His research is mainly in social networks.</p>
</td>
</tr>
</table>

<div class="divider-full">&nbsp;</div>

<table border="0"  cellpadding="6" bgcolor="#FFFFFF"> 
<tr>
<td align="center"><img src="ryap.jpg" class="border" alt="ryap" width="100" height="136" /></td>
<td>
<p class="blue"><b>Roland H. C. Yap</b> is an associate professor in the School of Computing at the National University of Singapore. His research interests are in artificial intelligence, constraints, programming languages, security and social networks. </p>
</td>
</tr>
</table>

<div class="divider-full">&nbsp;</div>

 <!-- Standard Copyright line here  -->

<div class="center">

<p class="footer"><i>On December 16, 2014 this article was edited to correct the corresponding author's email address.</i><br /><br />
Copyright &copy; 2014 Irvan Jahja, Suhendry Effendy and Roland H. C. Yap</p>  
  </div>
</td>
 </tr>
</table>

<table width="100%" border="0" align="center" cellpadding="0" cellspacing="0">
  <tr>
    <td height="1" bgcolor="#2b538e"><img src="../../../img2/transparent.gif" alt="transparent image" width="100" height="2" /></td>
  </tr>
</table>

</td></tr></table>
</td></tr></table>
</form>

</body>
</html>