<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

		<title>The Code4Lib Journal &#8211; Searching for Meaning Rather Than Keywords and Returning Answers Rather Than Links</title>

		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta name="generator" content="WordPress 6.8.1" /> <!-- leave this for stats -->
    <link rel="shortcut icon" href="../wp-content/themes/c4lj-theme/images/favicon.ico" />
		<link rel="stylesheet" href="../wp-content/themes/c4lj-theme/style.css" type="text/css" media="screen, print" />
		<!--[if lte IE 7]>
		<link rel="stylesheet" href="https://journal.code4lib.org/wp-content/themes/c4lj-theme/fix-ie7.css" type="text/css" media="screen" />
		<![endif]-->
		<!--[if lte IE 6]>
		<link rel="stylesheet" href="https://journal.code4lib.org/wp-content/themes/c4lj-theme/fix-ie6.css" type="text/css" media="screen" />
		<![endif]-->
		<link rel="stylesheet" href="../wp-content/themes/c4lj-theme/print.css" type="text/css" media="print" />
		<link rel="alternate" type="application/rss+xml" title="The Code4Lib Journal Syndication Feed" href="http://feeds.feedburner.com/c4lj" />
		<link rel="pingback" href="https://journal.code4lib.org/xmlrpc.php" />

<!-- Google Scholar Stuff -->
	<meta name="citation_title" content="Searching for Meaning Rather Than Keywords and Returning Answers Rather Than Links">
 <meta name="citation_author" content="Kent Fitch">
<meta name="citation_publication_date" content="2023/08/29">
	<meta name="citation_journal_title" content="Code4Lib Journal">
		<meta name="citation_issue" content="57">
<!-- end  Google Scholar Stuff -->

<meta name='robots' content='max-image-preview:large' />
	<style>img:is([sizes="auto" i], [sizes^="auto," i]) { contain-intrinsic-size: 3000px 1500px }</style>
	<link rel="alternate" type="application/rss+xml" title="The Code4Lib Journal &raquo; Searching for Meaning Rather Than Keywords and Returning Answers Rather Than Links Comments Feed" href="17443/feed" />
<script type="text/javascript">
/* <![CDATA[ */
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.1.0\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.1.0\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/journal.code4lib.org\/wp-includes\/js\/wp-emoji-release.min.js?ver=6.8.1"}};
/*! This file is auto-generated */
!function(i,n){var o,s,e;function c(e){try{var t={supportTests:e,timestamp:(new Date).valueOf()};sessionStorage.setItem(o,JSON.stringify(t))}catch(e){}}function p(e,t,n){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);var t=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data),r=(e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(n,0,0),new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data));return t.every(function(e,t){return e===r[t]})}function u(e,t,n){switch(t){case"flag":return n(e,"\ud83c\udff3\ufe0f\u200d\u26a7\ufe0f","\ud83c\udff3\ufe0f\u200b\u26a7\ufe0f")?!1:!n(e,"\ud83c\uddfa\ud83c\uddf3","\ud83c\uddfa\u200b\ud83c\uddf3")&&!n(e,"\ud83c\udff4\udb40\udc67\udb40\udc62\udb40\udc65\udb40\udc6e\udb40\udc67\udb40\udc7f","\ud83c\udff4\u200b\udb40\udc67\u200b\udb40\udc62\u200b\udb40\udc65\u200b\udb40\udc6e\u200b\udb40\udc67\u200b\udb40\udc7f");case"emoji":return!n(e,"\ud83d\udc26\u200d\ud83d\udd25","\ud83d\udc26\u200b\ud83d\udd25")}return!1}function f(e,t,n){var r="undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?new OffscreenCanvas(300,150):i.createElement("canvas"),a=r.getContext("2d",{willReadFrequently:!0}),o=(a.textBaseline="top",a.font="600 32px Arial",{});return e.forEach(function(e){o[e]=t(a,e,n)}),o}function t(e){var t=i.createElement("script");t.src=e,t.defer=!0,i.head.appendChild(t)}"undefined"!=typeof Promise&&(o="wpEmojiSettingsSupports",s=["flag","emoji"],n.supports={everything:!0,everythingExceptFlag:!0},e=new Promise(function(e){i.addEventListener("DOMContentLoaded",e,{once:!0})}),new Promise(function(t){var n=function(){try{var e=JSON.parse(sessionStorage.getItem(o));if("object"==typeof e&&"number"==typeof e.timestamp&&(new Date).valueOf()<e.timestamp+604800&&"object"==typeof e.supportTests)return e.supportTests}catch(e){}return null}();if(!n){if("undefined"!=typeof Worker&&"undefined"!=typeof OffscreenCanvas&&"undefined"!=typeof URL&&URL.createObjectURL&&"undefined"!=typeof Blob)try{var e="postMessage("+f.toString()+"("+[JSON.stringify(s),u.toString(),p.toString()].join(",")+"));",r=new Blob([e],{type:"text/javascript"}),a=new Worker(URL.createObjectURL(r),{name:"wpTestEmojiSupports"});return void(a.onmessage=function(e){c(n=e.data),a.terminate(),t(n)})}catch(e){}c(n=f(s,u,p))}t(n)}).then(function(e){for(var t in e)n.supports[t]=e[t],n.supports.everything=n.supports.everything&&n.supports[t],"flag"!==t&&(n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&n.supports[t]);n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&!n.supports.flag,n.DOMReady=!1,n.readyCallback=function(){n.DOMReady=!0}}).then(function(){return e}).then(function(){var e;n.supports.everything||(n.readyCallback(),(e=n.source||{}).concatemoji?t(e.concatemoji):e.wpemoji&&e.twemoji&&(t(e.twemoji),t(e.wpemoji)))}))}((window,document),window._wpemojiSettings);
/* ]]> */
</script>
<style id='wp-emoji-styles-inline-css' type='text/css'>

	img.wp-smiley, img.emoji {
		display: inline !important;
		border: none !important;
		box-shadow: none !important;
		height: 1em !important;
		width: 1em !important;
		margin: 0 0.07em !important;
		vertical-align: -0.1em !important;
		background: none !important;
		padding: 0 !important;
	}
</style>
<link rel='stylesheet' id='wp-block-library-css' href='../wp-includes/css/dist/block-library/style.min.css%3Fver=6.8.1.css' type='text/css' media='all' />
<style id='classic-theme-styles-inline-css' type='text/css'>
/*! This file is auto-generated */
.wp-block-button__link{color:#fff;background-color:#32373c;border-radius:9999px;box-shadow:none;text-decoration:none;padding:calc(.667em + 2px) calc(1.333em + 2px);font-size:1.125em}.wp-block-file__button{background:#32373c;color:#fff;text-decoration:none}
</style>
<style id='global-styles-inline-css' type='text/css'>
:root{--wp--preset--aspect-ratio--square: 1;--wp--preset--aspect-ratio--4-3: 4/3;--wp--preset--aspect-ratio--3-4: 3/4;--wp--preset--aspect-ratio--3-2: 3/2;--wp--preset--aspect-ratio--2-3: 2/3;--wp--preset--aspect-ratio--16-9: 16/9;--wp--preset--aspect-ratio--9-16: 9/16;--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--x-large: 42px;--wp--preset--spacing--20: 0.44rem;--wp--preset--spacing--30: 0.67rem;--wp--preset--spacing--40: 1rem;--wp--preset--spacing--50: 1.5rem;--wp--preset--spacing--60: 2.25rem;--wp--preset--spacing--70: 3.38rem;--wp--preset--spacing--80: 5.06rem;--wp--preset--shadow--natural: 6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep: 12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp: 6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined: 6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp: 6px 6px 0px rgba(0, 0, 0, 1);}:where(.is-layout-flex){gap: 0.5em;}:where(.is-layout-grid){gap: 0.5em;}body .is-layout-flex{display: flex;}.is-layout-flex{flex-wrap: wrap;align-items: center;}.is-layout-flex > :is(*, div){margin: 0;}body .is-layout-grid{display: grid;}.is-layout-grid > :is(*, div){margin: 0;}:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}
:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}
:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}
:root :where(.wp-block-pullquote){font-size: 1.5em;line-height: 1.6;}
</style>
<style id='akismet-widget-style-inline-css' type='text/css'>

			.a-stats {
				--akismet-color-mid-green: #357b49;
				--akismet-color-white: #fff;
				--akismet-color-light-grey: #f6f7f7;

				max-width: 350px;
				width: auto;
			}

			.a-stats * {
				all: unset;
				box-sizing: border-box;
			}

			.a-stats strong {
				font-weight: 600;
			}

			.a-stats a.a-stats__link,
			.a-stats a.a-stats__link:visited,
			.a-stats a.a-stats__link:active {
				background: var(--akismet-color-mid-green);
				border: none;
				box-shadow: none;
				border-radius: 8px;
				color: var(--akismet-color-white);
				cursor: pointer;
				display: block;
				font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen-Sans', 'Ubuntu', 'Cantarell', 'Helvetica Neue', sans-serif;
				font-weight: 500;
				padding: 12px;
				text-align: center;
				text-decoration: none;
				transition: all 0.2s ease;
			}

			/* Extra specificity to deal with TwentyTwentyOne focus style */
			.widget .a-stats a.a-stats__link:focus {
				background: var(--akismet-color-mid-green);
				color: var(--akismet-color-white);
				text-decoration: none;
			}

			.a-stats a.a-stats__link:hover {
				filter: brightness(110%);
				box-shadow: 0 4px 12px rgba(0, 0, 0, 0.06), 0 0 2px rgba(0, 0, 0, 0.16);
			}

			.a-stats .count {
				color: var(--akismet-color-white);
				display: block;
				font-size: 1.5em;
				line-height: 1.4;
				padding: 0 13px;
				white-space: nowrap;
			}
		
</style>
<link rel="https://api.w.org/" href="../wp-json/index.html" /><link rel="alternate" title="JSON" type="application/json" href="../wp-json/wp/v2/posts/17443" /><link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://journal.code4lib.org/xmlrpc.php?rsd" />
<link rel="canonical" href="../index.html%3Fp=17443.html" />
<link rel='shortlink' href='../index.html%3Fp=17443.html' />
<link rel="alternate" title="oEmbed (JSON)" type="application/json+oembed" href="../wp-json/oembed/1.0/embed%3Furl=https:%252F%252Fjournal.code4lib.org%252Farticles%252F17443" />
<link rel="alternate" title="oEmbed (XML)" type="text/xml+oembed" href="../wp-json/oembed/1.0/embed%3Furl=https:%252F%252Fjournal.code4lib.org%252Farticles%252F17443&amp;format=xml" />
<style>
@media all and (max-width : 768px) {
.syntaxhighlighter a, .syntaxhighlighter div, .syntaxhighlighter code, .syntaxhighlighter table, .syntaxhighlighter table td, .syntaxhighlighter table tr, .syntaxhighlighter table tbody, .syntaxhighlighter table thead, .syntaxhighlighter table caption, .syntaxhighlighter textarea
{
	font-size: 0.95em !important;
}
}
</style>
	</head>
	<body>
		<div id="page">
			<div id="header">
				<div id="headerbackground">
					<h1><a href="../index.html"><img src="../wp-content/themes/c4lj-theme/images/logo.png" alt="The Code4Lib Journal" /></a></h1>
				</div>
				<div id="about">
					<ul>
						<li class="page_item page-item-5"><a href="../index.html%3Fp=5.html">Mission</a></li>
<li class="page_item page-item-6"><a href="../editorial-committee/index.html">Editorial Committee</a></li>
<li class="page_item page-item-8"><a href="../process/index.html">Process and Structure</a></li>
						<li><a href="http://code4lib.org/">Code4Lib</a></li>
					</ul>
				</div>
				<div class="mobile-search">
					<form method="get" id="searchform" action="../index.html">
						<div>
							<input type="text" value="" aria-labelledby="searchsubmit" name="s" id="s" />
							<input type="submit" value="Search" id="searchsubmit" />
						</div>
					</form>
				</div>
			</div>

			<div id="content">
								<div class="article" id="post-17443">
					<p id="issueDesignation"><a href="../issues/issues/issue57.html">Issue 57, 2023-08-29</a></p>
					<h1 class="articletitle">Searching for Meaning Rather Than Keywords and Returning Answers Rather Than Links</h1>
					<div class="abstract">
						<p>Large language models (LLMs) have transformed the largest web search engines: for over ten years, public expectations of being able to search on meaning rather than just keywords have become increasingly realised. Expectations are now moving further: from a search query generating a list of &#8220;ten blue links&#8221; to producing an answer to a question, complete with citations.</p>
<p>This article describes a proof-of-concept that applies the latest search technology to library collections by implementing a semantic search across a collection of 45,000 newspaper articles from the National Library of Australia&#8217;s Trove repository, and using OpenAI&#8217;s ChatGPT4 API to generate answers to questions on that collection that include source article citations. It also describes some techniques used to scale semantic search to a collection of 220 million articles.</p>
					</div>
					<div class="entry">
						<p>by Kent Fitch</p>
<h2>Motivation and goals</h2>
<p>Our expectation of how search works changed ten years ago when Google introduced their &#8220;Hummingbird&#8221; search algorithm [<a id="ref1" href="../index.html%3Fp=17443.html#note1">1</a>].  Our experience until then was that you needed to formulate your search query using some exact words that appear in the document you were seeking. Ten years later, we&#8217;ve been further trained by &#8220;OK Google&#8221; and continuous improvement in search algorithms to find what we need by issuing a query defining our intent rather than exactly matching words, and increasingly, by using natural language to simply ask a question.</p>
<p>Google&#8217;s Knowledge Graph [<a id="ref2" href="../index.html%3Fp=17443.html#note2">2</a>] introduced in 2012 taught us to expect direct, summarised answers to queries, and now, Bing and Google are both anticipating a near future where the traditional &#8220;ten blue links&#8221; of a search result are superseded by text built by extracting information from the documents found by the search, enriched by the context provided by a knowledge graph and presented as an answer to the question inferred from the search query.</p>
<p>The gap in capability between the almost universally used search engines that set community expectations and libraries and related repositories has continued to grow. That gap, opened by PageRank [<a id="ref3" href="../index.html%3Fp=17443.html#note3">3</a>] 25 years ago, is now a chasm.</p>
<p>To provide modern and efficient discovery services for their communities rather than risking mutual abandonment by becoming perceived as baffling backwaters of byzantine business-rules redolent of a bygone era, libraries must adopt the same core search technologies used by the commercial services: semantic search, knowledge graphs and generative AI to support summarisation and chat.</p>
<p>Ten years ago, even semantic search would have been an impractical goal: the technology was specialised and expensive to implement and wasn&#8217;t well advanced. Just two years ago, attempting to implement much of this capability was more like a &#8220;science project&#8221; than a routine information technology undertaking. But as quickly and surely as public expectations have changed, so has the accessibility of the technology:  the use of AI has spread from university labs and tech leviathans and is now becoming commonplace, further increasing the pressure on the library community to embrace its effectiveness for the benefit of the public.</p>
<p>This article does not provide definitive answers to the many questions raised when introducing semantic search and chat-based question answering across a digital repository. Rather, it presents findings from a preliminary exploration in the hope of encouraging widespread experimentation with this rapidly evolving technology so that the benefits it will deliver to library communities can be expedited and shared.</p>
<p>The context for the technologies investigated and described in this article is a repository of newspaper articles. The main investigations were performed on a very small subset of the National Library of Australia (NLA) digitised newspaper repository. This study examined the feasibility and effectiveness of:</p>
<ul>
<li style="list-style-type: none;">
<ul>
<li aria-level="1">Semantic search</li>
<li aria-level="1">Named entity identification and disambiguation of entities described by Wikipedia</li>
<li aria-level="1">Question answering, summarisation and a &#8220;chat&#8221; interface</li>
</ul>
</li>
</ul>
<p>A further investigation of the feasibility of scaling semantic search to the full NLA digitised newspaper repository (about 220 million articles) was also performed.</p>
<h2>Context</h2>
<p>The National Library of Australia (NLA) digitised newspaper repository forms the most accessed component of their Trove service [<a id="ref4" href="../index.html%3Fp=17443.html#note4">4</a>]. It contains over 220 million newspaper articles published since 1804 in over 1,800 separate newspaper and gazette titles. During May 2023, Google Analytics recorded about 1.2M searches on Trove, and 3.8M pageviews of newspaper articles. The repository is popular with family researchers looking for information using personal names and place names (particularly in birth/death/marriage notices) and is also widely used by academics and general researchers. An indicator of Trove&#8217;s popularity is that when government budget measures threatened the ongoing sustainability of Trove in 2022, a petition calling for adequate funding received almost 34,000 signatures [<a id="ref5" href="../index.html%3Fp=17443.html#note5">5</a>].</p>
<p>From the Trove newspaper corpus, 45,494 news articles from the title<i> The Canberra Times </i>from the year 1994 were extracted. Articles of type advertising (display and classifieds) were excluded as the intent of this prototype was to focus on the benefits of semantic searching on news rather than on &#8220;births, deaths and marriages&#8221; searches which tend to be more &#8220;known item&#8221; searches on people and places. There were many uncorrected OCR errors in the extracted text.</p>
<p>To evaluate the practicality of name disambiguation using Wikipedia articles, 38,359 articles about organisations and 155,781 articles about people were extracted from an August 2022 Wikipedia dump, limiting the extract to articles that contained the word &#8220;Australia&#8221; or contained at least 3,000 words. The goal was to attempt to include all Australian people and organisations, and all &#8220;prominent&#8221; (worthy of 3,000 words) people and organisations even if non-Australian. Heuristics were used to classify articles as being of interest to this trial (ie, about people or organisations, not place names).</p>
<h2>Approach and results</h2>
<h3>Named entity identification</h3>
<p>The news articles were processed using the Stanford NLP library [<a id="ref6" href="../index.html%3Fp=17443.html#note6">6</a>] to identify entities of type PERSON, LOCATION, ORGANIZATION and MISC. Common non-specific entities such as &#8220;he&#8221;, &#8220;his&#8221;, &#8220;she&#8221;, &#8220;her&#8221;, &#8220;hers&#8221; were dropped. Minor processing to encourage better uniformity (particularly in the face of OCR artefacts) was performed, eg, &#8220;NSW&#8221; was changed to &#8220;NEW SOUTH WALES&#8221;, &#8220;MEL BOURNE CUP&#8221; was changed to &#8220;MELBOURNE CUP&#8221;.</p>
<h3>Embedding creation</h3>
<p>The text from each news article was used to create an &#8220;embedding&#8221;, which is a vector characterising the text as a point in a high-dimensional space in such a way that texts which have similar meanings have vector representations which are &#8220;close&#8221; in that multi-dimensional space.</p>
<p>Embeddings can be created in many ways, but those created by transformer-based large language models (LLMs) have been successfully used as a foundation for semantic search. The better the embedding represents the meaning of text, the better a semantic search using a set of those embeddings is likely to be. As well as characterising text, many models can also create embeddings for images, hence mapping images and text to a common high-dimensional space and enabling searches across both images and text. Although not the immediate focus of this experiment, the Trove newspaper repository contains millions of newspaper images (such as photos published in newspapers) and other sub-repositories in Trove contain tens of millions of other images, so an embedding model able to characterise images and text is of interest.</p>
<p>Although it is impossible to visualise points in a high-dimensional space, this simplification may help:</p>
<p>&nbsp;</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/Fitch1.png" /><br />
<strong>Figure 1. </strong>Simplified representation of embeddings<strong>.<br />
</strong></p>
<p>Here, four input documents (three of text, one image) are each separately processed by an embedding model to produce four embedding vectors. If we just look at the first three values and treat them as coordinates in three-dimensional space, we can imagine how the four points represented by those coordinates could be compared for proximity, and how, if the embedding is successful in representing texts or images as vectors, it would be possible to find things similar to some other thing.</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/FitchX.png" /><br />
<strong>Figure 2. </strong>Searching data flow<strong>.<br />
</strong></p>
<p>A semantic search engine accepts a query (a text string, or possibly an image) and creates an embedding for that query, just as it had previously created an embedding for the documents it stores. It then finds the closest document embeddings to that query embedding in the high-dimensional space of the embedding model, and returns the closest documents, ranked by the proximity of their embeddings to the query embedding.</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/FitchY.png" /><br />
<strong>Figure 3. </strong>Simplified representation of embeddings<strong>.<br />
</strong></p>
<p>It must be noted that the language model used to create the document embeddings must be also used to create the query embeddings. For example, if OpenAI&#8217;s ada-002 was used to create the document embeddings, ada-002 must also be available and used when creating the embedding of the query to be compared to those document embeddings. For large or long-lived repositories, the ongoing availability of the language model and associated software required to create embeddings is a key driver of the choice of the embedding to be used.</p>
<p>Embeddings map text to a vector space of hundreds or even thousands of dimensions. A higher-dimensional space derived from a very large language model, well-trained and well-aligned with human understanding, can typically represent more subtly of meaning than a lower-dimensional space. However, more dimensions are more expensive to store, index and compare.</p>
<p>For the proof-of-concept, three embeddings were tried, derived from:</p>
<ul>
<li style="list-style-type: none;">
<ul>
<li aria-level="1">The BERT language model &#8211; a very early transformer LLM.</li>
<li aria-level="1">CLIP ViT L-14 [<a id="ref7" href="../index.html%3Fp=17443.html#note7">7</a>]: maps image and text to a single vector space. Although not the &#8220;best in class&#8221; sentence transformer, CLIP ViT L-14 performs well, is quick, and importantly for this proof-of-concept was very easy to set-up using &#8220;CLIP as a service&#8221; [<a id="ref8" href="../index.html%3Fp=17443.html#note8">8</a>].</li>
<li aria-level="1">ada-002:  a recent and highly-regarded embedding provided as a commercial cloud service by OpenAI.</li>
</ul>
</li>
</ul>
<p>Evaluation of BERT was discontinued because qualitative performance was inferior to CLIP. CLIP produces embeddings of 768 dimensions, and  ada-002 produces 1,536 dimensions. Qualitatively, ada-002 embeddings are more useful for semantic search, but its extra resource requirements, cost and non-open nature raise concerns for a repository with lots of data and a long life.</p>
<p>Each language model has a maximum size of text it is able to process when creating an embedding. Text beyond that maximum is ignored and does not contribute to that embedding. The  ada-002 embedding can accept about 6,000 words which is more than almost all newspaper articles, and even for longer articles, probably perfectly adequate (it is hard to imagine an article that buries the lede that deep).</p>
<p>However, CLIP, like many other embedding-generating sentence transformers, is limited to about 70 words. It is not obvious how best to process longer text. One approach is to generate and index multiple embeddings per article. For example, for an average-length news article of, say, 500 words, generate 8 embeddings and index all of them with the article and somehow at query time, combine the similarity scores for each embedding to generate an article match score. However, it is important to &#8220;break&#8221; the embedding text on sentence boundaries, as sentence transformers are trained on sentences, not arbitrary extents of text. Also, it isn&#8217;t obvious that splitting &#8220;meaning&#8221; across separate embeddings will allow that meaning to be represented by summing or otherwise combining the match scores of the component embeddings. And finally, such splitting greatly increases the size of the embedding index and will negatively affect performance.</p>
<p>Another approach is to somehow merge the multiple embeddings: that is, in the above example, generate perhaps 10 sentence embeddings then add the vectors together (and then &#8220;normalise&#8221; this summed vector to have a unit length required for best performance by embedding index engines).</p>
<p>This proof-of-concept uses a &#8220;hybrid&#8221; approach for CLIP: it generates an embedding for the first (up to) 70 words honouring sentence boundaries which is indexed separately, and also produces a summed embedding from multiple sentence embeddings from the entire text. Qualitatively, the use of the first 70 word embedding was unnecessary &#8211; summing the embeddings did not seem to &#8220;blur&#8221; or &#8220;smear&#8221; the meaning as much as was feared.</p>
<p>The choice of embedding model is an important decision with significant consequences discussed below.</p>
<h3>Embedding indexing</h3>
<p>Embeddings can be indexed and searched using one of several specialist &#8220;vector&#8221; databases (such as Pinecone, Milvus and Weaviate) or by using vector extensions to existing relational databases (such as the pgVector extension for Postgres) and document stores (such as Elastic and Lucene/Solr). As Lucene/Solr is an existing key component of NLA&#8217;s technology stack, it was chosen for this proof-of-concept and performed well and &#8220;out of the box&#8221; with the CLIP embeddings. Lucene implements &#8220;Approximate Nearest Neighbour&#8221; (ANN) search using the Hierarchical Navigable Small Worlds (HNSW) search graph [<a id="ref9" href="../index.html%3Fp=17443.html#note9">9</a>].</p>
<p>The ada-002 embeddings however are longer than Lucene&#8217;s current vector limit of 1,024 dimensions, and so required a trivial source-code change to enable.</p>
<h3>Semantic searching</h3>
<p>The proof-of-concept Lucene/Solr index of 45,494 newspaper articles contains two separate types of indices:</p>
<ul>
<li style="list-style-type: none;">
<ul>
<li aria-level="1">&#8220;traditional&#8221; free text index able to support keyword and phrase searching and ranking based on relative occurrences of search terms in the index, the document and the document&#8217;s length</li>
<li aria-level="1">vector index able to match a query embedding against all document embeddings to generate a similarity score used for ranking</li>
</ul>
</li>
</ul>
<p>Each type of index has strengths and weaknesses. Free text indices are great for exact matches on the words in the supplied query, and for &#8220;boosting&#8221; the ranking of documents where those words are found in more important fields (perhaps a title or abstract), or are found together as a phrase. Vector indices are great for finding documents that are &#8220;like&#8221; the query &#8211; not necessarily due to word matches, but by matching the meaning of the query.</p>
<p>An example of a good candidate for a text search is a name search &#8211; if you search for <i>John Smith</i>, you are almost certainly searching for a document containing those words very close together. It is unlikely there are useful &#8220;semantics&#8221; beyond that &#8211; that is, there are probably not &#8220;John Smith&#8221;-like documents that don&#8217;t contain those words close together.</p>
<p>Another possible example is a single-word search, such as <i>kamikaze</i>: it is likely that the searcher is looking for that exact word rather than content that does not contain that word but somehow represents the &#8220;spirit&#8221; or meaning of that word.</p>
<p>However,<i> train crash</i> is less clear cut, as the searcher is possibly interested in <i>railway accidents </i>or <i>derailment</i> or <i>level-crossing smash</i> or <i>train smashes into bus</i> or any other number of other slightly broader, narrower or related concepts.</p>
<p>As queries get longer, they tend to convey even more &#8220;semantic intent&#8221; that is impossible to honour with simple keywords, even when attempting to automatically expand the search with keywords.</p>
<p>For example, a search for articles using <i>the fall of John Major</i> (a prominent conservative British Prime Minister during the 1990s) conveys a clear semantic intent. If a patron walked up to a librarian and asked for content using that phrase, the librarian would have a good idea about what they were after. Performing this search on Trove (limited to 1993 Canberra Times articles to enable a comparison) produces results a user familiar with Google does not expect: many results have nothing to do with &#8220;John Major&#8221; &#8211; they just happen to have each of those 5 words somewhere in the article. Perhaps worse, many directly relevant articles are not found: perhaps they use <i>ousting</i> or <i>downfall</i> or <i>undoing</i> or <i>unravelling</i> or <i>humiliation</i> or <i>collapse of support</i> rather than <i>fall</i>, perhaps they describe the political pressure he was under in other ways.</p>
<p>The proof-of-concept explores a blended keyword and semantic search. It does this by:</p>
<ol>
<li aria-level="1">Issuing a &#8220;standard&#8221; keyword and phrase text search with standard keyword ranking.</li>
<li aria-level="1">For each of the top-10 keyword-ranked results, fetch that document&#8217;s embedding and use this to issue a semantic search to find other documents with embeddings most similar to it. The intent is to &#8220;enrich&#8221; the result candidates by including documents very similar to those best keyword results but which may not contain all the keywords.</li>
<li aria-level="1">Creating an embedding of the original search query and issuing a semantic search to find documents with embedding most similar to it.</li>
</ol>
<p>Each of these searches produces a ranked list of documents with a search score (in this case, generated by Lucene). The score for the first search (the &#8220;standard&#8221; keyword and phrase search) is calculated by Lucene based on its default BM25 [<a id="ref10" href="../index.html%3Fp=17443.html#note10">10</a>] ranking using keyword repository and document frequencies with a boost applied if all keywords were found nearby (ie, a phrase-like boost). The scores for the second and third searches are also calculated by Lucene based on the distance in vector-space between the embeddings of the search embeddings and the document embeddings. The proof-of-concept then applies a separate weight to the scores produced by the three types of searches and adds the results across all three searches to generate a document result set for ranking.</p>
<p>The three weights are referred to by the proof-of-concept user interface as:</p>
<ol>
<li aria-level="1">Keyword boost</li>
<li aria-level="1">Keyword-found doc similarity boost</li>
<li aria-level="1">Query similarity boost</li>
</ol>
<p>Hence to perform a ranking purely on keyword score, set the second and third weights to zero, and to perform a ranking purely on semantic similarity to the query, set the first and second weights to zero.</p>
<p>&nbsp;</p>
<p class="caption"><img decoding="async" src="../media/issue57/fitch/Fitch2.png" /><br />
<strong>Figure 4. </strong>Search results for <em>the fall of John Major</em>: pure keyword results on the left, blended keyword and semantic results on the right.</p>
<p>&nbsp;</p>
<p>It is not obvious how to set the three weights. Empirically, a search using the CLIP embeddings benefits more from a lower relative query similarity boost than a search using the ada-002 embeddings, probably because the &#8220;semantics&#8221; captured in the ada-002 embedding of both query and documents are much better. Also empirically, a search on just one word, or on a &#8220;known item&#8221; such as a person&#8217;s name often gives better results when the scoring is dominated by keyword boost (but only when that word exists in the repository &#8211; see <i>cybersecurity</i> counter-example below)</p>
<p>Ranking of results is subjective and no formal recall/precision analysis has yet been performed.</p>
<p>It must be noted that the vector search algorithm used by Lucene, HNSW, does not guarantee to find the &#8220;most relevant&#8221; results, that is, the documents whose embedding vectors are closest to the query vector: HNSW is an &#8220;Approximate Nearest Neighbour&#8221; algorithm that trades accuracy against resource consumption (CPU, storage, I/O). However, if reasonable care is taken with HNSW index construction parameters and query parameters, HNSW is highly accurate.</p>
<p>A version of the proof-of-concept is available for demonstration here: <a href="http://nla-overproof.projectcomputing.com/knnBlend">http://nla-overproof.projectcomputing.com/knnBlend</a></p>
<p>A typical search demonstrating the comparison of keyword and blended results is this search using the query <i>the fall of John Major</i>: <a href="http://nla-overproof.projectcomputing.com/knnBlend?set=1994&amp;embedding=ada-002&amp;stxt=the%20fall%20of%20John%20Major">http://nla-overproof.projectcomputing.com/knnBlend?set=1994&amp;embedding=ada-002&amp;stxt=the%20fall%20of%20John%20Major</a></p>
<p>The following table is a summary of the first 50 results of each search using the ada-002 embeddings, showing how keyword results were &#8220;demoted&#8221; (rendered as with pink background in the blended results), &#8220;promoted&#8221; (rendered with light green background), and shows results only found by the semantic search (dark green background, designated as &#8220;knn&#8221;).</p>
<p>For example, the first keyword result (about the war in Bosnia containing comments by John Major) was demoted to rank 48; the second keyword result (about a fall in regional bank values that includes many occurrences of the query keywords but is not at all about the intent of the query) is demoted to rank 50; the fourth keyword result is promoted to rank 1; the 6th through 17th blended results were only found by a semantic search.</p>
<p>&nbsp;</p>
<div class="caption">
<p><strong>Table 1. </strong>Showing how keyword results where re-ranked after blending with semantic results</p>
<table>
<thead>
<tr>
<th style="background-color: #eeeeee;" colspan="3"><b>Keyword search</b></th>
<th style="background-color: #eeeeff;" colspan="3"><b>Blended search</b></th>
</tr>
</thead>
<tbody>
<tr>
<td style="background-color: #eeeeee;"><b>Rank</b></td>
<td style="background-color: #eeeeee;"><b>Article</b></td>
<td style="background-color: #eeeeee;"><b>Re-ranked to</b></td>
<td style="background-color: #eeeeff;"><b>Rank</b></td>
<td style="background-color: #eeeeff;"><b>Article</b></td>
<td style="background-color: #eeeeff;"><b>Re-ranked from</b></td>
</tr>
<tr>
<td style="background-color: #eeeeee;">1</td>
<td>INTERNATIONAL Bihac still under siege By KURT SCHORK SARAJEVO, Friday: Bosnian S</td>
<td>48</td>
<td style="background-color: #eeeeff;">1</td>
<td style="background-color: #eeffee;">Jig is up for UK Conservatives BILL MANDLE THE BRITISH local-govern ment electio</td>
<td>4</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">2</td>
<td>BUSINESS AND INVESTMENT Regional banks shed value after rates increase By MICHAE</td>
<td>50</td>
<td style="background-color: #eeeeff;">2</td>
<td style="background-color: #eeffee;">Major problems for unpopular PM &#8216;There is not much point in spending money on a</td>
<td>5</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">3</td>
<td>INTERNATIONAL Major warns Tories: EU yes or a poll &#8211; By RICHARD MEARES LONDON, F</td>
<td>18</td>
<td style="background-color: #eeeeff;">3</td>
<td style="background-color: #eeffee;">Momentum against Major The administration of Britain&#8217;s Prime Minister, John Majo</td>
<td>9</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">4</td>
<td>Jig is up for UK Conservatives BILL MANDLE THE BRITISH local-government electio</td>
<td>1</td>
<td style="background-color: #eeeeff;">4</td>
<td style="background-color: #eeffee;">The undoing of John Major Malcolm Booker THE CONSERVATIVE lead ers in Britain, a</td>
<td>13</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">5</td>
<td>Major problems for unpopular PM &#8216;There is not much point in spending money on a</td>
<td>2</td>
<td style="background-color: #eeeeff;">5</td>
<td style="background-color: #eeffee;">Major&#8217;s fortunes turn around JOHN MAJOR must be almost unable, to believe his lu</td>
<td>23</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">6</td>
<td>Brambles 313m loss due to big abnormal SYDNEY: A huge abnormal loss and poor per</td>
<td>72</td>
<td style="background-color: #eeeeff;">6</td>
<td style="background-color: #bbffbb;">[Foreign Major&#8217;s moral crusade nauseating: Lamont LONDON: A bitter attack by Nor</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">7</td>
<td>Hopes now ride on future of Falcon COMMENT By PETER BREWER By axing the Capri co</td>
<td>73</td>
<td style="background-color: #eeeeff;">7</td>
<td style="background-color: #bbffbb;">Seeds sown for Conservative uprising Major facing mutiny after EU climb-down LON</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">8</td>
<td>Community services the key to more jobs JOHN QUIGGIN says only a major policy re</td>
<td>74</td>
<td style="background-color: #eeeeff;">8</td>
<td style="background-color: #bbffbb;">INTERNATIONAL Conservative crisis after Budget defeat By ALAN WHEATLEY of Reuter</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">9</td>
<td>Momentum against Major The administration of Britain&#8217;s Prime Minister, John Majo</td>
<td>3</td>
<td style="background-color: #eeeeff;">9</td>
<td style="background-color: #bbffbb;">Major crushes Eurorebels for now LONDON, Saturday: The Brit ish Prime Minister,</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">10</td>
<td>Market hits 14-month low as base metals bear brunt MELBOURNE: The Australian sha</td>
<td>75</td>
<td style="background-color: #eeeeff;">10</td>
<td style="background-color: #bbffbb;">INTERNATIONAL MPs expelled in row over EU Key win saves Maior Bv ALAN WHFATLFY L</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">11</td>
<td>BUSINESS AND INVESTMENT Steel leads BHP to record profit MELBOURNE: Steel, backe</td>
<td>discard</td>
<td style="background-color: #eeeeff;">11</td>
<td style="background-color: #bbffbb;">INTERNATIONAL Major humiliated In local elections LONDON: In a humiliating setba</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">12</td>
<td>BUSINESS AND INVESTMENT Coal &#8216;rabble&#8217; irks union Australia&#8217;s coal industry was b</td>
<td>discard</td>
<td style="background-color: #eeeeff;">12</td>
<td style="background-color: #bbffbb;">INTERNATIONAL Major may face leadership test within weeks By DON WOOLFORD of AAP</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">13</td>
<td>The undoing of John Major Malcolm Booker THE CONSERVATIVE lead ers in Britain, a</td>
<td>4</td>
<td style="background-color: #eeeeff;">13</td>
<td style="background-color: #bbffbb;">By-election disaster kicks off Major&#8217;s dice with voters LONDON: Prime Minister J</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">14</td>
<td>State Bank sale to be finalised soon SYDNEY: The sale of the State Bank of NSW h</td>
<td>discard</td>
<td style="background-color: #eeeeff;">14</td>
<td style="background-color: #bbffbb;">INTERNATIONAL Tories sink into a moral stew Three MPs exposed . more rattling in</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">15</td>
<td>Fears for peace push DUBLIN, Sunday: A dispute in the Irish Republic&#8217;s Coalition</td>
<td>discard</td>
<td style="background-color: #eeeeff;">15</td>
<td style="background-color: #bbffbb;">Unravelling not due to Major alone From HUGO YOUNG in London THE SLOW disintegra</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">16</td>
<td>IN BRIEF QDL expands into Victoria BRISBANE: Pharmaceutical whole- &#8211; salcr and d</td>
<td>discard</td>
<td style="background-color: #eeeeff;">16</td>
<td style="background-color: #bbffbb;">British Tories rally behind their leader LONDON: John Major is expected to secur</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">17</td>
<td>Elated RSL wins battle for Wake&#8217;s war medals By MARION FRITH A jubilant Returned</td>
<td>discard</td>
<td style="background-color: #eeeeff;">17</td>
<td style="background-color: #bbffbb;">Blair rides high on eve of conference as Major founders in sea of &#8216;sleaze&#8217; By AL</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">18</td>
<td>IRC decision gives all workers on federal awards 11 public holidays a year By MI</td>
<td>discard</td>
<td style="background-color: #eeeeff;">18</td>
<td style="background-color: #ffeeee;">INTERNATIONAL Major warns Tories: EU yes or a poll &#8211; By RICHARD MEARES LONDON, F</td>
<td>3</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">19</td>
<td>HISTORY Highlights in history on May 8: 1704: British forces under Duke of Marlb</td>
<td>discard</td>
<td style="background-color: #eeeeff;">19</td>
<td style="background-color: #eeffee;">INTERNATIONAL Major on the ropes as bribery claim Minister resigns By MICHAEL WH</td>
<td>22</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">20</td>
<td>AIDS patients &#8216;incorrectly&#8217; diagnosed By CATRIONA BONFIGLIOLI, AAP Medical Corre</td>
<td>discard</td>
<td style="background-color: #eeeeff;">20</td>
<td style="background-color: #bbffbb;">INTERNATIONAL Major stakes Govt on high-risk Bill By DONALD MACINTYRE LONDON, Th</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">21</td>
<td>BUSINESS AND INVESTMENT ; i, Colonial Mutual rejects State Bank sell-off claims</td>
<td>discard</td>
<td style="background-color: #eeeeff;">21</td>
<td style="background-color: #bbffbb;">INTERNATIONAL EU vote revolt poses poll threat to Tories By RICHARD MEARES LONDO</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">22</td>
<td>INTERNATIONAL Major on the ropes as bribery claim Minister resigns By MICHAEL WH</td>
<td>19</td>
<td style="background-color: #eeeeff;">22</td>
<td style="background-color: #bbffbb;">MPs cold on heat tax LONDON, Tuesday: The Brit ish Prime Minister, John Ma jor,</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">23</td>
<td>Major&#8217;s fortunes turn around JOHN MAJOR must be almost unable, to believe his lu</td>
<td>5</td>
<td style="background-color: #eeeeff;">23</td>
<td style="background-color: #bbffbb;">Fresh row adds to Tory woes BOURNEMOUTH, England, Wednesday: Britain&#8217;s ruling Co</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">24</td>
<td>COLLECTABLES Dion Skinner Logos of old still capture the mind IN THE world of ad</td>
<td>discard</td>
<td style="background-color: #eeeeff;">24</td>
<td style="background-color: #bbffbb;">INTERNATIONAL PM fancies a return to banking The British Prime Minister, John Ma</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">25</td>
<td>PM backs rate rise as recovery tool By, IAN HENDERSON Prime Minister Paul Keatin</td>
<td>discard</td>
<td style="background-color: #eeeeff;">25</td>
<td style="background-color: #bbffbb;">Elections Speaking in the House of Commons, Labour leader John Smith described t</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">26</td>
<td>US economy like banker&#8217;s dream THE United States economy is showing signs of tur</td>
<td>discard</td>
<td style="background-color: #eeeeff;">26</td>
<td style="background-color: #bbffbb;">Heseltine front-runner to take over as leader The president of the British Board</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">27</td>
<td>Norman struggles, six shots off pace PITTSBURGH, Pennsylvania: A disappointed Gr</td>
<td>discard</td>
<td style="background-color: #eeeeff;">27</td>
<td style="background-color: #bbffbb;">Cloud is cast over Major&#8217;s morals drive LONDON: Two forced re- signations from t</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">28</td>
<td>Saints fall victim to resurgent Hawks MELBOURNE: Pundits proclaiming an end to H</td>
<td>discard</td>
<td style="background-color: #eeeeff;">28</td>
<td style="background-color: #bbffbb;">Heseltine rises from the political grave LONDON: Michael Heseltine, once the gol</td>
<td>knn47</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">29</td>
<td>, . , BUSINESS AND INVESTMENT Sale of State Bank still not a sure thing SYDNEY:</td>
<td>discard</td>
<td style="background-color: #eeeeff;">29</td>
<td style="background-color: #bbffbb;">Avoid a security over-reaction THE ATTACK on Prince Charles has set off the pred</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">30</td>
<td>B U SIN E SSL AN D INVESTMENT Bargain hunters help market rally Shares recover a</td>
<td>discard</td>
<td style="background-color: #eeeeff;">30</td>
<td style="background-color: #bbffbb;">WORLD BRIEFS Major hangs on as party leader LONDON: Prime Minister John Major es</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">31</td>
<td>INTERNATIONAL Villagers flee enclave, rebel onslaught Serbs move on UN safe have</td>
<td>76</td>
<td style="background-color: #eeeeff;">31</td>
<td style="background-color: #bbffbb;">They said it It could only have been expected &#8230; it&#8217;s the fault of the Governme</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">32</td>
<td>Fawcett quits in rift over FAL plans PERTH: David Fawcett, the man viewed by man</td>
<td>discard</td>
<td style="background-color: #eeeeff;">32</td>
<td style="background-color: #bbffbb;">Racist row over speech LONDON: A high-flying minister has forced Prime Minister</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">33</td>
<td>Adsteam shares take a dive SYDNEY: Shares in the debt laden Adsteam group plunge</td>
<td>discard</td>
<td style="background-color: #eeeeff;">33</td>
<td style="background-color: #bbffbb;">RICHARD FARMER Liberals cannot merely sit back THE LIBERAL Party&#8217;s election stra</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">34</td>
<td>Power plays of men and women Oleanna, a piece about sexual harassment, is meant</td>
<td>discard</td>
<td style="background-color: #eeeeff;">34</td>
<td style="background-color: #bbffbb;">Richard Farmer Economic news of no help to Libs AT FIRST blush an Oppo sition ma</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">35</td>
<td>US turnaround leads Australian market recovery SYDNEY: The Australian stock mark</td>
<td>discard</td>
<td style="background-color: #eeeeff;">35</td>
<td style="background-color: #bbffbb;">All hope is not gone for Liberals Peter Cole-Adams I.F THE Liberal Party was in</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">36</td>
<td>Bannister and friends remember a record OXFORD, England: After 40 years, about t</td>
<td>discard</td>
<td style="background-color: #eeeeff;">36</td>
<td style="background-color: #bbffbb;">Beggars &#8216;offend&#8217; British leader LONDON: The British Prime Minis ter, John Major,</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">37</td>
<td>BOOKS Discouraging view of the journalism scene in US WHO STOLE THE NEWS? Why we</td>
<td>discard</td>
<td style="background-color: #eeeeff;">37</td>
<td style="background-color: #bbffbb;">Domestic issues dominate THE ELECTIONS for the Euro pean Parliament now under wa</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">38</td>
<td>1 Head of GATT to quit this year . J From JOHN ZAROCOSTAS In Geneva PETER SUTHER</td>
<td>discard</td>
<td style="background-color: #eeeeff;">38</td>
<td style="background-color: #bbffbb;">UK Budget freezes spending . LONDON, Wednesday: Brit ain&#8217;s unpopular Conservativ</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">39</td>
<td>Book Two Decaying belief in the dentist J UST what I need: another reason not to</td>
<td>discard</td>
<td style="background-color: #eeeeff;">39</td>
<td style="background-color: #bbffbb;">NZ teeters towards poll Resignation hits shaky majority WELLINGTON: New Zea land</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">40</td>
<td>Rates rise &#8216;not justified&#8217; By KEITH SCOTT The managing director of BHP, John Pre</td>
<td>discard</td>
<td style="background-color: #eeeeff;">40</td>
<td style="background-color: #bbffbb;">Saturday FORUM The conjurers around the Cabinet table FEDERAL POLITICS Ross Peak</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">41</td>
<td>TJDGET &#8217;94 Strategy depends on business investment recovery in 1994-95 Budget cl</td>
<td>discard</td>
<td style="background-color: #eeeeff;">41</td>
<td style="background-color: #bbffbb;">PM told to spend on the jobless By TOM CONNORS, Economics Writer The Prime Minis</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">42</td>
<td>Accused MP vows to fight to e nd SYDNEY: The embattled State Liberal MP Barry Mo</td>
<td>discard</td>
<td style="background-color: #eeeeff;">42</td>
<td style="background-color: #bbffbb;">A good test of Downer&#8217;s mettle ALEXANDER Downer had another bad day yesterday. H</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">43</td>
<td>Test tickets now prized possessions ! ROO TOUR I [NOTEBOOK! By Bevan Hannan Seat</td>
<td>discard</td>
<td style="background-color: #eeeeff;">43</td>
<td style="background-color: #bbffbb;">PM keeps Budget tax rise on cards By ROSS PEAKE, Political Correspondent Prime M</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">44</td>
<td>Forestry protesters chip directly at PM Australian Federal Police officers remov</td>
<td>discard</td>
<td style="background-color: #eeeeff;">44</td>
<td style="background-color: #bbffbb;">Crystal ball starts to get a bit hazy around 1999 POLITICAL punditry is a risky</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">45</td>
<td>Brave-faced Lions view the future MELBOURNE: The victim of the AFL&#8217;s five-year p</td>
<td>discard</td>
<td style="background-color: #eeeeff;">45</td>
<td style="background-color: #bbffbb;">Crushing swing rocks British Conservatives BRIERLEY HILL, England, Friday: The B</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">46</td>
<td>Southern Cross Woden&#8217;s talent exodus leaves Stephens with shaky foundation ON TH</td>
<td>discard</td>
<td style="background-color: #eeeeff;">46</td>
<td style="background-color: #bbffbb;">Kohl to call Major&#8217;s bluff BONN: Chancellor Helmut Kohl and his European allies</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">47</td>
<td>Magic bullets&#8217; take their aim on disease POSSIBLE breakthrough in treating rheu</td>
<td>discard</td>
<td style="background-color: #eeeeff;">47</td>
<td style="background-color: #bbffbb;">A state loss is a federal gain John Black analyses the figures and concludes tha</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">48</td>
<td>Loggers ready to blockade Woodchip warning to Govt By PAUL CHAMBERLIN The loggin</td>
<td>discard</td>
<td style="background-color: #eeeeff;">48</td>
<td style="background-color: #ffeeee;">INTERNATIONAL Bihac still under siege By KURT SCHORK SARAJEVO, Friday: Bosnian S</td>
<td>1</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">49</td>
<td>Small business confidence slips back a notch By IAN HENDERSON, Economics Writer</td>
<td>discard</td>
<td style="background-color: #eeeeff;">49</td>
<td style="background-color: #bbffbb;">Downer faces possible revolt on two fronts By PETER COLE-ADAMS, Political Editor</td>
<td>knn</td>
</tr>
<tr>
<td style="background-color: #eeeeee;">50</td>
<td>Senior Libs consulted on Hewson&#8217;s fall By JACK WATERFORD The four most senior of</td>
<td>discard</td>
<td style="background-color: #eeeeff;">50</td>
<td style="background-color: #ffeeee;">BUSINESS AND INVESTMENT Regional banks shed value after rates increase By MICHAE</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
<p>&nbsp;</p>
<p>The blended result is a great improvement, but it is far from perfect: blended results 33 to 44 are almost all about Australian or New Zealand rather than British politics, and results 63 and 67 are quite relevant to the search. Result 63 (&#8220;Death is another Major setback LONDON: The death of a Conservative legislator in what police call suspicious circumstances adds to the troubles of Prime Minister John Major&#8221;) ranks highly (at 21) on a &#8220;pure&#8221; semantic search, so it is being pushed down the rankings by keyword and similar-to-keyword results. Result 67 (&#8220;Harold Macmillan&#8217;s fate haunts modern Tories&#8221;) is not improved by a pure semantic search using the ada-002 embeddings, but is ranked at 34 using the CLIP embeddings. This result demands further investigation.</p>
<p>The value of a semantic search is particularly evident when performing a search most naturally expressed as a question or phrase rather than a set of keywords, and it does indeed seem that the ada-002 embedding (and to a lesser extent, the CLIP embedding) manages to encode a commonly understood intent behind the phrase and match it successfully with articles, for example, this search on <i>infiltration of ASIO by communist spies</i> which returns zero keyword matches yet many relevant semantic matches:</p>
<p>&nbsp;</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/Fitch3.png" /><br />
<strong>Figure 5. </strong>Search results for <em>infiltration of ASIO by communist spies</em>: there are no pure keyword results on the left, but many relevant semantic results on the right.</p>
<p>&nbsp;</p>
<p>Another interesting example is the ability of a semantic search to find articles using current-day terminology that was not used at the time the article was written. <i>Cybersecurity</i> was not a term much used in 1994, and a keyword search finds zero matches, but the semantic search finds many relevant articles:</p>
<p>&nbsp;</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/Fitch4.png" /><br />
<strong>Figure 6. </strong>Search results for <em>cybersecurity</em>: there are no pure keyword results on the left, but many relevant semantic results on the right.</p>
<p>&nbsp;</p>
<h3>Named entity resolution</h3>
<p>When navigating very large repositories and trying to understand the context of their contents and how they relate to other resources, being able to see the people, organisations, places and events mentioned in articles and result summaries can be very useful. People especially can be referred to in many ways, often with name variations that sometimes are contextual (eg, Joe Biden, Joseph Biden, President Biden, the President, J.R. Biden Jr (1942-), Joe, Mr Biden, Senator Biden, ..), and identifying what is a name and the &#8220;real-world&#8221; entity it refers to is both useful and non-trivial.</p>
<p>The proof-of-concept used the Stanford Natural Language library [<a id="ref6" href="../index.html%3Fp=17443.html#note6">6</a>] to identify named entities which were then indexed with their containing articles. This allowed displaying names in article and result summaries (the latter as facets which can be used as search filters).</p>
<p>However, this is just the first step in providing a useful entity identification and context capability. It is valuable for an entity&#8217;s names (including their many variants) to be linked to a real-world entity. In the above example, it is probable, dependent on context, that Joe Biden, Senator Biden, President Biden, etc. should all be linked to the same &#8220;real world&#8221; person represented by the Wikipedia page <a href="https://en.wikipedia.org/wiki/Joe_Biden">https://en.wikipedia.org/wiki/Joe_Biden</a>.</p>
<p>This linking has not been attempted in the proof-of-concept, but an experiment in trying to resolve people and organisation names to Wikipedia entities has been implemented which uses a combination of name matching and semantic similarity between the article containing the entity and Wikipedia text describing the entity to generate a similarity score between people and organisation entities identified by the Stanford Natural Language library and Wikipedia articles.</p>
<p>Some examples from the first semantic match on<i> the fall of John Major</i> search:</p>
<p>&nbsp;</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/Fitch5.png" /><br />
<strong>Figure 7. </strong>Entities found in a newspaper article about <em>the fall of John Major</em><strong>.<br />
</strong></p>
<p>&nbsp;</p>
<p>The first entity listed is<i> MR MAJOR</i>.</p>
<p>&nbsp;</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/Fitch6.png" /><br />
<strong>Figure 8. </strong>Wikipedia articles matching the entity <em>MR MAJOR </em>ranked by textual and semantic similarity.</p>
<p>&nbsp;</p>
<p>Although not the closest name match amongst at least 10 name matches on <i>MR MAJOR</i>, the Wikipedia article on John Major is the closest semantic match to the article, and reasonably confidently disambiguates the name.</p>
<p>This article also contains a named entity for <i>JOHN MAJOR</i>:</p>
<p>&nbsp;</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/Fitch7.png" /><br />
<strong>Figure 9. </strong>Wikipedia articles matching the entity <em>JOHN MAJOR </em>ranked by textual and semantic similarity.</p>
<p>&nbsp;</p>
<p>Unsurprisingly, the better name match and same article/Wikipedia similarity match very confidently disambiguates this name.</p>
<p>The next entity is <i>KENNETH CLARKE</i> (in this context, another prominent conservative British politician):</p>
<p>&nbsp;</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/Fitch8.png" /><br />
<strong>Figure 10. </strong>Wikipedia articles matching the entity <em>KENNETH CLARKE </em>ranked by textual and semantic similarity.</p>
<p>&nbsp;</p>
<p>Semantic similarity confidently disambiguates this entity reference, as it also does for Michael Portillo,  Michael Heseltine (both conservative British politicians) and Paul Keating (Australian Prime Minister, 1991-1996). However, it fails with <i>JOHN SMITH</i>:</p>
<p>&nbsp;</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/Fitch9.png" /><br />
<strong>Figure 11. </strong>Wikipedia articles matching the entity <em>JOHN SMITH </em>ranked by textual and semantic similarity.</p>
<p>&nbsp;</p>
<p>The English politician is not even in the top 10, which is perhaps not surprising given the <a href="https://en.wikipedia.org/wiki/John_Smith">dozens of John Smiths on Wikipedia</a>. However, this is a good illustration of the difficulties in linking named entities, particularly of common names, to Wikipedia entities.</p>
<p>These attempts at resolving named entities are encouraging but further work is needed to refine matching using names, contextual dates, semantic similarity and common co-appearing entities.</p>
<h3>Scaling experiments</h3>
<p>Although performance on this small repository of 45,494 documents is very good, the complete Trove newspaper repository contains 220 million articles. A notable characteristic of the HNSW algorithm used by many vector search engines (including Lucene/Solr) is that it is a hierarchical graph search in which most nodes (representing the embedding vectors of documents) are highly connected, typically to 16 &#8211; 128 other nodes, and the algorithm proceeds by following many promising parallel paths of increasing similarity through the graph. This causes effectively random access to the data in the graph, and as a consequence, very high IO rates unless the graph is held in memory. Even when in memory, the number of &#8220;probes&#8221; performed navigating the graph requires very high bandwidth between memory and the processor.  For example, finding the &#8220;top 10&#8221; semantically-near results to a query with a HNSW graph storing about 200 million vectors, each linked with 60 nearest neighbours, requires about 45,000 &#8220;probes&#8221; distributed across the graph.</p>
<p>Hence, the size of the graph is of utmost importance. The maximum number of other nodes each node can be connected to has a moderate influence on the graph size, but the biggest factor is likely to be the size of the embedding vector. The ada-002 vector contains 1,536 floating point values, which equates to 6,144 bytes per vector. This uses less than 300MB for the small proof-of-concept repository, but for 220 million articles, requires over 1,300GB of memory. Even on a machine with such a large memory, the need to randomly access this memory into the processor&#8217;s memory caches suggests undesirable processing bottlenecks which will reduce query performance.</p>
<p>If the ada-002 vector contained vector positions (&#8220;dimensions&#8221;) which were highly correlated with each other or had near-constant values, then it would be possible to drop some dimensions whilst leaving search results unaffected. However, an analysis of the ada-002 vectors created for the newspaper and Wikipedia articles showed there was no correlation between vector positions and no near-constant values: that is, all the dimensions seem to add discriminatory value to the embedding.</p>
<p>However, this analysis did note that all but five of the 1,536 dimensions have almost all of their values in a narrow range between -0.1 to +0.1, whereas the five &#8220;outlier&#8221; dimensions had characteristic different narrow ranges (for example, dimension 194&#8217;s range is almost entirely -0.7 to -0.6). This suggested a low-error way to quantise the 4 byte float into a 1 byte value using six quantisations: one for each of the five outliers and one for the central &#8220;clump&#8221;, by choosing quantisation values to minimise total error.</p>
<p>With this approach, the 1,536 float values are quantized and stored as 1,536 bytes, and at &#8220;query time&#8221;, when calculating the vector similarity, the quantisation tables are used to &#8220;expand&#8221; each byte to a float having a value close to the original float value.</p>
<p>The proof-of-concept implements this approach as the &#8220;ada-002quant&#8221; embedding, and qualitatively, the results are equivalent to the unquantized values.</p>
<p>This approach reduces storage/memory/memory-cache-shuffling by 75%, but still requires 340MB of memory for 220M records just to represent the embeddings.</p>
<p>Further compression can be achieved using Product Quantization (PQ) coding [<a id="ref11" href="../index.html%3Fp=17443.html#note11">11</a>]. With this approach, some number of vector positions, typically between 2 and 10, is represented by a single quantised value stored as typically between 4 and 16 bits.</p>
<p>Experimentation using the ada-002 embeddings gave good results using PQ coding on 3 vector positions and representing them with 1 byte quantisation. This required 1,536 / 3 = 512 separate PQ tables, each with 256 values, which represented 3 floats as 1 byte. Each of the 512 PQ tables were built using k-means clustering to minimise total error.</p>
<p>PQ coding hence reduces memory requirements by a further two-thirds, storing 512 bytes per embedding (down from the original 6,144 bytes), and allows the embeddings for 220M records to be stored in just 113GB.</p>
<p>The proof-of-concept implements this approach as the &#8220;ada-002pqcode&#8221; embedding, and qualitatively, the results are similar to the unquantized values.</p>
<h3>Chatting with a newspaper repository</h3>
<p>After the OpenAI chat API was released in March 2023, a simple chat proof-of-concept was implemented as follows:</p>
<ul>
<li style="list-style-type: none;">
<ul>
<li aria-level="1">An embedding is created for the initial user chat input (exactly as it would be for a search).</li>
<li aria-level="1">A semantic search on the newspaper repository (Canberra Times news articles from 1994) is issued to find the 8 documents with embeddings closest to that of the user input.</li>
<li aria-level="1">Up to ~3,000 words are extracted in total from the 8 documents, in proportion to their relative similarity score and are used as context for the OpenAI chat request.</li>
<li aria-level="1">A system prompt and the user input are added to the article context and passed to OpenAI&#8217;s chat API endpoint.</li>
<li aria-level="1">The response is received, article references changed to hyperlinks (to the original article on Trove) and shown to the user.</li>
<li aria-level="1">Any followup response from the user is appended to the previous inputs and sent again to OpenAI&#8217;s chat API.</li>
</ul>
</li>
</ul>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/FitchZ.png" /><br />
<strong>Figure 12. </strong>Chatting data flow<strong>.<br />
</strong></p>
<p>As an example, a user input of “What problems did John Major have in 1994?” generates the following request to OpenAI&#8217;s chat API:</p>
<pre class="brush: plain; title: ; notranslate" title="">
&quot;messages&quot;:&#x5B;

{&quot;role&quot;:&quot;system&quot;,&quot;content&quot;:&quot;You answer questions factually based on the context provided. The context consists of newspaper articles, each within their own article tag (&lt;article&gt;) which starts with their article number and date of issue formatted as yyyy-mm-dd. Reference every article you used to construct your answer by starting every use of text derived from an article or articles by the source articles' sequence in the context, for example  &#x5B;From article 2] answer text derived from article 2... &#x5B;From article 8, 3] answer text derived from articles 8 and 3&quot;},;

{&quot;role&quot;:&quot;system&quot;,&quot;name&quot;:&quot;context&quot;,&quot;content&quot;:

&quot;&lt;article&gt; Article 1. Date of issue: 1994-05-07. Content: INTERNATIONAL Major humiliated In local elections LONDON: In a humiliating setback for Prime Minister John Major, Britain's ruling party yesterday suffered its worst defeat in local elections as traditional Conservative voters deserted his crisis plagued Government. Diehard Conservatives throughout Britain switched allegiance or stayed &#x5B;content removed for brevity …] Norman Fowler&lt;/article&gt;

&lt;article&gt; Article 2. Date of issue: 1994-02-16. Content: Momentum against Major The administration of Britain's Prime Minister, John Major, has been deeply hurt by the continuing sex scandals which have seen &#x5B;content removed for brevity …] too long in the political game-to,fall into that trap&lt;/article&gt;

&lt;article&gt; Article 3. Date of issue: 1994-04-01. Content: Seeds sown for Conservative uprising Major facing mutiny after EU climb-down LONDON: British Prime Minister John Major faced the growing threat of a mutiny against his leadership yesterday..

And so on for 8 articles, concluding with:

.. David Ashby, had shared a hotel bedroom in France with a male friend&lt;/article&gt; &quot;},

{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;What problems did John Major have in 1994?&quot;}

]
</pre>
<p>ChatGPTv3 frequently failed to cite the source article as requested, but ChatGPTv4 routinely cites correctly, resulting in output such as:</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/Fitch10.png" /><br />
<strong>Figure 13. </strong>Response from OpenAI&#8217;s ChatGPTv4 to the query <em>What problems did John Major have in 1994? </em>given a context of 8 semantically-searched documents from <em>The Canberra Times, </em>1994.</p>
<p>&nbsp;</p>
<p>Followup questions are kept within the provided context:</p>
<p>&nbsp;</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/Fitch11.png" /><br />
<strong>Figure 14. </strong>Response from a followup question, showing effective preservation of the original question&#8217;s context.</p>
<p>&nbsp;</p>
<p>Chat provides an interesting way to summarise a large amount of content from different articles quickly and will be attractive for many people wanting to get an overview whilst still being able to &#8220;dig in&#8221; to the source material via the citation links to the original articles.</p>
<h2>Further work</h2>
<p>The proof-of-concept has merely scratched the surface of possibilities and briefly explored just some of the approaches for improving discoverability of resources held in large digital repositories.</p>
<p>Some of the areas identified for further investigation are listed below.</p>
<h3>Named Entity identification and linking</h3>
<p>Large language models have been successfully used to identify named entities and informal experiments with some newspaper articles and appropriate prompts using GPT3.5 and GPT4 are extremely encouraging, although the cost currently greatly exceeds running traditional Named Entity Recognition libraries.</p>
<p>It is possible that large language models, perhaps fine-tuned with content from Wikipedia, will be able to accurately identify and disambiguate people, organisations and places.</p>
<p>Many of the people and organisations described in newspaper articles will not be prominent enough to appear in Wikipedia. They will, however, be of great interest to some communities. How to identify and represent these, and for example, not link an unrelated &#8220;John Major&#8221; to the Wikipedia entry of the former British Prime Minister is unresolved.</p>
<p>Assuming reasonably accurate identification of named entities and linking to Wikipedia/Wikibase entities can be achieved, there exist great opportunities for representing content containing those entities in a broader context. For example, it would be possible to search for articles about <i> British conservative leaders visiting Ireland</i> without needing to specify names of people or places in Ireland, by using an equivalent of Google&#8217;s Knowledgebase (possibly based on Wikidata [<a id="ref12" href="../index.html%3Fp=17443.html#note12">12</a>]) to find the appropriate people and places, and then using named entity indices on articles to find the intersections and finally using a semantic filter to identify those describing an appropriate named entity (British conservative leader) <i>visiting</i> another appropriate named entity (place in Ireland).</p>
<h3>Appropriate embeddings</h3>
<p>This proof-of-concept compares the CLIP ViT-L-14 and ada-002 embeddings, but there are many other options for embeddings. CLIP ViT is no longer considered state of the art, but was used because it was very easy to try and because the ability to map text and images to a single high-dimensional space is very attractive, providing as it does a simple, unified way to retrieve images as well as text.</p>
<p>Although ada-002 qualitatively gave better results, it has several downsides. The cost is non-trivial. NLA&#8217;s newspaper archive contains around 140 billion words, which equates to around 190 billion tokens. OpenAI has reduced the cost of embeddings dramatically, and at the time of writing (June 2023), ada-002 embeddings cost $US0.0001 per 1K tokens, giving an embedding cost of $US19,000 for the NLA newspaper repository. However, the NLA&#8217;s web archive repository contains about two magnitudes more text.</p>
<p>Another concern with a proprietary embedding approach is that should the vendor ever deprecate then discontinue support, the existing embeddings are unusable: incoming queries need to be converted to the same embedding model as the documents they are going to search. Similarly, a future decision to increase the price of obtaining an embedding could greatly increase search costs.</p>
<p>The Hugging Face Massive Text Embedding Benchmark Leaderboard [<a id="ref13" href="../index.html%3Fp=17443.html#note13">13</a>] compares the attributes of embedding models, providing a starting point to investigate a balance between attributes such as cost, openness, speed, ability to characterise long runs of text, ability to characterise various languages, ability to characterise images, effectiveness, vector length and compressibility.</p>
<h3>Vector search</h3>
<p>This proof-of-concept only examined the performance of Lucene/Solr&#8217;s HNSW Approximate Nearest Neighbour search engine. This is a good &#8220;fit&#8221; for NLA&#8217;s technology stack, and it performed well and was demonstrated to scale to the size of the entire current newspaper repository. However, there may be better options, and vector search is a rapidly developing field.</p>
<p>The time taken and space used to construct the HNSW graph is affected by two tunable parameters:</p>
<ul>
<li style="list-style-type: none;">
<ul>
<li aria-level="1">The number of &#8220;nearest&#8221; neighbours each node should be connected to (although the HNSW algorithm seeks to have somewhat diverse rather than strictly nearest but extremely similar neighbours).</li>
<li aria-level="1">How exhaustively the algorithm should search for nearest neighbours.</li>
</ul>
</li>
</ul>
<p>Searching the HNSW graph is also affected by a parameter that specifies how exhaustively the algorithm should search for best matches.</p>
<p>Optimising these HNSW parameters is likely to be rewarding.</p>
<p>Compressing the embedding representation (to smaller storage values such as a byte rather than a float, and by reducing the number of values by techniques such as PQ coding) are likely to be necessary for large repositories. For a repository the size of NLA&#8217;s web archive (15 billion documents and counting), tradeoffs between embedding accuracy and performance seem inevitably required, at least with current hardware.</p>
<h3>User Interface and search logic</h3>
<p>Many searches are intended to be exact keyword searches. A searcher issuing <i>Paul Keating Bankstown</i> is very likely only interested in results about the named entity <i>Paul Keating</i> (the former Australian Prime Minister) and the named entity <i>Bankstown </i>(the Sydney suburb and his birthplace), and &#8220;blending in&#8221; semantic results may result in more annoyance than joy. Preprocessing the query to identify those entities will be valuable (for example, <i>Paul Keating</i> may be referenced in a sought article as <i>Prime Minister Keating</i>, <i>PJ Keating</i>, <i>Mr Keating</i>, or depending on the article&#8217;s context, just<i> Prime Minister)</i>.</p>
<p>However, a search for <i>Paul Keating immigration policy</i> will benefit from semantic searching, at least on the <i>immigration policy</i> part. Perhaps a starting approach would be to attempt to identify named entities in the query and treat them as requiring a keyword match, or, if a knowledge graph is available, a known-entity match.</p>
<p>A last resort (admitting failure) would be to rely on a mechanism such as &#8220;Advanced Search&#8221; in many library systems, which basically puts the onus on the searcher to devise and communicate the search strategy. But for those trained by Google, such an approach is likely to be as mystifying as it is inadequate.</p>
<p>Where to terminate a semantic result set is another interesting problem. Unlike keyword searches which always have a hard cutoff (e.g., for an &#8220;ANDed&#8221; search, only documents with all the keywords can be returned), similarity has no obvious hard boundary: all documents are similar to all others to some degree.</p>
<p>Previously, &#8220;documents like this&#8221; capabilities operated on a &#8220;bag of words&#8221; approach, typically favouring rarer words. Semantic similarity offers the potential to make &#8220;documents like this&#8221; more useful.</p>
<p>Named entities combined with a knowledge graph will allow the search interface to suggest more &#8220;abstract&#8221; facets that are not directly contained in the text of the search result. For example, a search for <i>train crashes</i> may usefully present facets of country or state as higher-level groupings of place-names mentioned in the found articles. A search for <i>Paul Keating immigration policy </i>may usefully present facets such as political parties or factions as higher-level groupings of people mentioned in the found articles.</p>
<h3>OCR text correction</h3>
<p>Semantic search is more robust than keyword search in the presence of OCR errors because it relies on more than specific occurrences of keywords to create an embedding representing the meaning of text. However, as described above, many searches will still rely on exact keywords (e.g., searching for a particular person and place), and removing as many OCR errors as possible prior to creating embeddings and keyword indices, whether by crowd-sourcing (as successfully implemented by Trove [<a id="ref14" href="../index.html%3Fp=17443.html#note14">14</a>]), by automated OCR correction (such as OverProof [<a id="ref15" href="../index.html%3Fp=17443.html#note15">15</a>], also used by Trove) or using generative AI, may allow materially more accurate searching and summarisation.</p>
<h3>Summarisation and &#8220;chat&#8221; interface</h3>
<p>Moving away from the traditional search result of hyperlinks, each with a document title and brief context of found keywords, will be a big change for library systems, but following the design lead of large public search engines may minimise surprise.</p>
<p>At the time of writing (June 2023), Microsoft&#8217;s Bing search results show a feature search result (with section headings and summaries taken from that feature web page) followed by traditional-looking links in a left hand column, and a AI generated summary response that starts filling the right hand column and &#8220;learn more&#8221; citations used by the summary, common explanatory follow-up questions, and an invitation to &#8220;Let&#8217;s Chat&#8221;:</p>
<p>&nbsp;</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/Fitch12.png" /><br />
<strong>Figure 15. </strong>Bing&#8217;s AI-augmented search results as of June 2023.</p>
<p>&nbsp;</p>
<p>(Note that the actual search results returned will be informed by many factors such as the searcher&#8217;s location, IP address, search history, cookies associated with them and their browser and its settings.)</p>
<p>The citations can be followed to show the &#8220;source&#8221; web page. Alternatively, &#8220;Let&#8217;s chat&#8221; or one of the &#8220;pre-canned&#8221; follow-up questions can be clicked to continue the chat:</p>
<p>&nbsp;</p>
<p class="caption"><img decoding="async" class="aligncenter" src="../media/issue57/fitch/Fitch13.png" /><br />
<strong>Figure 16. </strong>Follow-up chat on Bing&#8217;s AI augmented search portal.</p>
<p>&nbsp;</p>
<p>The summarisation/chat is very effective at presenting an overview derived from multiple source documents with mechanisms to dive deeper, and this hybrid of traditional search results shown alongside summarisation/chat allows the introduction of this new capability in a gentle way that&#8217;s unlikely to disorientate people seeing it for the first time.</p>
<p>While the choice of which model to use for creating embeddings has long-term implications, the choice of which model to use for chat/summarisation is &#8220;tactical&#8221;, and can be relatively easily changed as technology develops. Although at the time of writing (June 2023) OpenAI&#8217;s GPT4 chat API has by far the best capabilities, it is expensive to scale (a single chat interaction with a provided context of 4,000 words generating a 500 word response costs about $US0.20) and some institutions may not be happy with resources and patron questions being supplied to a commercial service. Highly capable open-source models trained for chat are appearing, such as the  Falcon-40B [<a id="ref16" href="../index.html%3Fp=17443.html#note16">16</a>] and MPT-30B  [<a id="ref17" href="../index.html%3Fp=17443.html#note17">17</a>] models. MPT-30B appears particularly attractive because its size allows it to run comfortably on a single (very) high-end graphics processor and because it offers an 8K maximum token context, the same as the base GPT4 API. A larger context means more &#8220;source&#8221; text can be provided for the chat engine to summarise or use as a base for question-answering.</p>
<p>The proof-of-concept always selects chat context from the 8 best articles, and always selects text from the start of the articles. It is unlikely this naive strategy is optimal. Instead, it is probable that sometimes more, sometimes fewer articles should be selected, and that the most relevant context won&#8217;t always be confined to some fixed amount of text at the start of the article. For web pages (rather than newspaper articles) in particular, text at the start of a page is often boilerplate and hence normally irrelevant to the chat.</p>
<p>The proof-of-concept persists with the same context as the chat progresses, simply adding questions and the chat-engine&#8217;s response to that context. This is unlikely to be optimal: followup questions in the chat probably change the original best-choice of articles selected to provide the context.</p>
<p>Finally, a production service may need to ensure that the chat remains within the context provided, regardless of follow-up questions which may, adversarially, attempt to elicit responses that the hosting institution would rather not appear under its banner.</p>
<h2>References</h2>
<p>[<a id="note1" href="../index.html%3Fp=17443.html#ref1">1</a>] Google Hummingbird. Wikipedia [accessed 2023 June 28] <a href="https://en.wikipedia.org/wiki/Google_Hummingbird">https://en.wikipedia.org/wiki/Google_Hummingbird</a></p>
<p>[<a id="note2" href="../index.html%3Fp=17443.html#ref2">2</a>] Introducing the Knowledge Graph: things, not strings, Amit Singhal, Google [accessed 2023 June 28] <a href="https://blog.google/products/search/introducing-knowledge-graph-things-not/">https://blog.google/products/search/introducing-knowledge-graph-things-not/</a></p>
<p>[<a id="note3" href="../index.html%3Fp=17443.html#ref3">3</a>] PageRank. Wikipedia [accessed 2023 June 28] <a href="https://en.wikipedia.org/wiki/PageRank">https://en.wikipedia.org/wiki/PageRank</a></p>
<p>[<a id="note4" href="../index.html%3Fp=17443.html#ref4">4</a>] Trove. National Library of Australia [accessed 2023 June 28] <a href="https://trove.nla.gov.au">https://trove.nla.gov.au</a></p>
<p>[<a id="note5" href="../index.html%3Fp=17443.html#ref5">5</a>] #FullyFundTrove. Change.org [accessed 2023 June 28] <a href="https://www.change.org/p/fully-fund-trove">https://www.change.org/p/fully-fund-trove</a></p>
<p>[<a id="note6" href="../index.html%3Fp=17443.html#ref6">6</a>] Stanford Natural Language Processing Core [accessed 2023 June 28] <a href="https://stanfordnlp.github.io/CoreNLP/">https://stanfordnlp.github.io/CoreNLP/</a></p>
<p>[<a id="note7" href="../index.html%3Fp=17443.html#ref7">7</a>] CLIP ViT-L-14. Hugging Face [accessed 2023 June 28] <a href="https://huggingface.co/sentence-transformers/clip-ViT-L-14">https://huggingface.co/sentence-transformers/clip-ViT-L-14</a></p>
<p>[<a id="note8" href="../index.html%3Fp=17443.html#ref8">8</a>] CLIP as a service. Jina [accessed 2023 June 28] https://clip-as-service.jina.ai/</p>
<p>[<a id="note9" href="../index.html%3Fp=17443.html#ref9">9</a>]  Hierarchical Navigable Small Worlds (HNSW) search graph. Pinecone [accessed 2023 June 28] <a href="https://www.pinecone.io/learn/hnsw/">https://www.pinecone.io/learn/hnsw/</a></p>
<p>[<a id="note10" href="../index.html%3Fp=17443.html#ref10">10</a>] Okapi BM25. Wikipedia [accessed 2023 June 28] <a href="https://en.wikipedia.org/wiki/Okapi_BM25">https://en.wikipedia.org/wiki/Okapi_BM25</a></p>
<p>[<a id="note11" href="../index.html%3Fp=17443.html#ref11">11</a>] Product quantization. Pinecone [accessed 2023 June 28] <a href="https://www.pinecone.io/learn/product-quantization/">https://www.pinecone.io/learn/product-quantization/</a></p>
<p>[<a id="note12" href="../index.html%3Fp=17443.html#ref12">12</a>] Wikidata [accessed 2023 June 28] <a href="https://www.wikidata.org/wiki/Wikidata:Main_Page">https://www.wikidata.org/wiki/Wikidata:Main_Page</a></p>
<p>[<a id="note13" href="../index.html%3Fp=17443.html#ref13">13</a>] Massive Text Embedding Benchmark (MTEB) Leaderboard. Hugging Face [accessed 2023 June 28] <a href="https://huggingface.co/spaces/mteb/leaderboard">https://huggingface.co/spaces/mteb/leaderboard</a></p>
<p>[<a id="note14" href="../index.html%3Fp=17443.html#ref14">14</a>] &#8216;Singing for their supper’: Trove, Australian newspapers, and the crowd. Marie-Louise Ayres  [accessed 2023 June 28] <a href="https://library.ifla.org/id/eprint/245/1/153-ayres-en.pdf">https://library.ifla.org/id/eprint/245/1/153-ayres-en.pdf</a></p>
<p>[<a id="note15" href="../index.html%3Fp=17443.html#ref15">15</a>] Report on comparative search results following overProof correction of 10 million NLA newspaper articles. Project Computing [accessed 2023 June 28] <a href="http://nla-overproof.projectcomputing.com/">http://nla-overproof.projectcomputing.com/</a></p>
<p>[<a id="note16" href="../index.html%3Fp=17443.html#ref16">16</a>] Falcon-40B. Hugging Face [accessed 2023 June 28] <a href="https://huggingface.co/tiiuae/falcon-40b">https://huggingface.co/tiiuae/falcon-40b</a></p>
<p>[<a id="note17" href="../index.html%3Fp=17443.html#ref17">17</a>] MPT-30B: Raising the bar for open-source foundation models. MosaicML  [accessed 2023 June 28] <a href="https://www.mosaicml.com/blog/mpt-30b">https://www.mosaicml.com/blog/mpt-30b</a></p>
<h2 class="abouttheauthor">About the author</h2>
<p><em><a href="mailto:kent.fitch@projectcomputing.com">Kent Fitch</a></em> has worked as a computer programmer since 1980 and been a partner in Project Computing Pty Ltd since 1982. He was the system architect and lead programmer of the National Library of Australia&#8217;s Newspaper Digitisation system, then of Trove, and recently added pageRank to Trove&#8217;s huge web archive full text index. He also develop systems at University of New South Wales and for Project Computing.</p>
<p>He does not speak for the NLA and the work described in this article was performed independently of the NLA.</p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by/3.0/us/">Creative Commons Attribution 3.0 United States License</a>.</p>
					</div>
														</div>
				<!-- You can start editing here. -->

<div class="comments">
	<p class="subscriptionlinks">Subscribe to comments: <a href="17443/feed">For this article</a> | <a href="http://feeds.feedburner.com/c4lj/comments">For all articles</a></p>

			<!-- If comments are open, but there are no comments. -->

	 

<h3 id="respond">Leave a Reply</h3>


<form action="https://journal.code4lib.org/wp-comments-post.php" method="post" id="commentform">


<p><input type="text" name="author" id="author" value=""/>
<label for="author">Name (required)</label></p>

<p><input type="text" name="email" id="email" value="" />
<label for="email">Mail (will not be published) (required)</label></p>

<p><input type="text" name="url" id="url" value="" />
<label for="url">Website</label></p>


<p><textarea autocomplete="new-password"  aria-label="Comment box" id="f127f6ccbe"  name="f127f6ccbe"   cols="50" rows="10"></textarea><textarea id="comment" aria-label="hp-comment" aria-hidden="true" name="comment" autocomplete="new-password" style="padding:0 !important;clip:rect(1px, 1px, 1px, 1px) !important;position:absolute !important;white-space:nowrap !important;height:1px !important;width:1px !important;overflow:hidden !important;" tabindex="-1"></textarea><script data-noptimize>document.getElementById("comment").setAttribute( "id", "a7a2663ded83ffdfed91395b09ed3a3e" );document.getElementById("f127f6ccbe").setAttribute( "id", "comment" );</script></p>

<p><input name="submit" type="submit" id="submit"  value="Submit Comment" />
<input type="hidden" name="comment_post_ID" value="17443" />
</p>
<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="7b4a96cf2d" /></p><p style="display: none !important;" class="akismet-fields-container" data-prefix="ak_"><label>&#916;<textarea name="ak_hp_textarea" cols="45" rows="8" maxlength="100"></textarea></label><input type="hidden" id="ak_js_1" name="ak_js" value="160"/><script>document.getElementById( "ak_js_1" ).setAttribute( "value", ( new Date() ).getTime() );</script></p>
</form>


</div>
							</div>

			<div id="meta">
				<div id="issn">
					<p>ISSN 1940-5758</p>
				</div>
				<div class="search-sidebar">
				<form method="get" id="searchform" action="../index.html">
					<div>
						<input type="text" value="" aria-labelledby="searchsubmit" name="s" id="s" />
						<input type="submit" value="Search" id="searchsubmit"/>
					</div>
				</form>
				</div>
				<div id="archives">
					<h2>Current Issue</h2>
						<ul>
							<li><a href="../issues/issues/issue60.html">Issue 60, 2025-04-14</a></li>
						</ul>

					<h2>Previous Issues</h2>
						<ul>
              <li><a href="../issues/issues/issue59.html">Issue 59, 2024-10-07</a></li><li><a href="../issues/issues/issue58.html">Issue 58, 2023-12-04</a></li><li><a href="../issues/issues/issue57.html">Issue 57, 2023-08-29</a></li><li><a href="../issues/issues/issue56.html">Issue 56, 2023-04-21</a></li>              <li><a href="../index.html%3Fp=2476.html">Older Issues</a></li>
						</ul>
				</div>
				<div id="forauthors">
					<h2>For Authors</h2>
					<ul>
						<li class="page_item page-item-4"><a href="../index.html%3Fp=4.html">Call for Submissions</a></li>
<li class="page_item page-item-7"><a href="../index.html%3Fp=7.html">Article Guidelines</a></li>
					</ul>
				</div>
			</div>
						<div id="footer">
				<p id="login"><a href="../wp-login.php.html">Log in</a></p>
				<p id="copyright">This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/3.0/us/">Creative Commons Attribution 3.0 United States License</a>.<br /><a rel="license" href="http://creativecommons.org/licenses/by/3.0/us/"><img alt="Creative Commons License" src="http://i.creativecommons.org/l/by/3.0/us/80x15.png" /></a></p>
			</div>
			<script type="speculationrules">
{"prefetch":[{"source":"document","where":{"and":[{"href_matches":"\/*"},{"not":{"href_matches":["\/wp-*.php","\/wp-admin\/*","\/wp-content\/uploads\/*","\/wp-content\/*","\/wp-content\/plugins\/*","\/wp-content\/themes\/c4lj-theme\/*","\/*\\?(.+)"]}},{"not":{"selector_matches":"a[rel~=\"nofollow\"]"}},{"not":{"selector_matches":".no-prefetch, .no-prefetch a"}}]},"eagerness":"conservative"}]}
</script>
<script type="text/javascript" src="../wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shCore.js%3Fver=3.0.9b" id="syntaxhighlighter-core-js"></script>
<script type="text/javascript" src="../wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shBrushPlain.js%3Fver=3.0.9b" id="syntaxhighlighter-brush-plain-js"></script>
<script type='text/javascript'>
	(function(){
		var corecss = document.createElement('link');
		var themecss = document.createElement('link');
		var corecssurl = "https://journal.code4lib.org/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shCore.css?ver=3.0.9b";
		if ( corecss.setAttribute ) {
				corecss.setAttribute( "rel", "stylesheet" );
				corecss.setAttribute( "type", "text/css" );
				corecss.setAttribute( "href", corecssurl );
		} else {
				corecss.rel = "stylesheet";
				corecss.href = corecssurl;
		}
		document.head.appendChild( corecss );
		var themecssurl = "https://journal.code4lib.org/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shThemeDefault.css?ver=3.0.9b";
		if ( themecss.setAttribute ) {
				themecss.setAttribute( "rel", "stylesheet" );
				themecss.setAttribute( "type", "text/css" );
				themecss.setAttribute( "href", themecssurl );
		} else {
				themecss.rel = "stylesheet";
				themecss.href = themecssurl;
		}
		document.head.appendChild( themecss );
	})();
	SyntaxHighlighter.config.strings.expandSource = '+ expand source';
	SyntaxHighlighter.config.strings.help = '?';
	SyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\n\n';
	SyntaxHighlighter.config.strings.noBrush = 'Can\'t find brush for: ';
	SyntaxHighlighter.config.strings.brushNotHtmlScript = 'Brush wasn\'t configured for html-script option: ';
	SyntaxHighlighter.defaults['pad-line-numbers'] = false;
	SyntaxHighlighter.defaults['toolbar'] = false;
	SyntaxHighlighter.all();

	// Infinite scroll support
	if ( typeof( jQuery ) !== 'undefined' ) {
		jQuery( function( $ ) {
			$( document.body ).on( 'post-load', function() {
				SyntaxHighlighter.highlight();
			} );
		} );
	}
</script>
<script defer type="text/javascript" src="../wp-content/plugins/akismet/_inc/akismet-frontend.js%3Fver=1748382734" id="akismet-frontend-js"></script>
		</div>
	</body>
</html>
