<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

		<title>The Code4Lib Journal &#8211; Large Language Models for Machine-Readable Citation Data: Towards an Automated Metadata Curation Pipeline for Scholarly Journals</title>

		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta name="generator" content="WordPress 6.8.1" /> <!-- leave this for stats -->
    <link rel="shortcut icon" href="../wp-content/themes/c4lj-theme/images/favicon.ico" />
		<link rel="stylesheet" href="../wp-content/themes/c4lj-theme/style.css" type="text/css" media="screen, print" />
		<!--[if lte IE 7]>
		<link rel="stylesheet" href="https://journal.code4lib.org/wp-content/themes/c4lj-theme/fix-ie7.css" type="text/css" media="screen" />
		<![endif]-->
		<!--[if lte IE 6]>
		<link rel="stylesheet" href="https://journal.code4lib.org/wp-content/themes/c4lj-theme/fix-ie6.css" type="text/css" media="screen" />
		<![endif]-->
		<link rel="stylesheet" href="../wp-content/themes/c4lj-theme/print.css" type="text/css" media="print" />
		<link rel="alternate" type="application/rss+xml" title="The Code4Lib Journal Syndication Feed" href="http://feeds.feedburner.com/c4lj" />
		<link rel="pingback" href="https://journal.code4lib.org/xmlrpc.php" />

<!-- Google Scholar Stuff -->
	<meta name="citation_title" content="Large Language Models for Machine-Readable Citation Data: Towards an Automated Metadata Curation Pipeline for Scholarly Journals">
 <meta name="citation_author" content="Aerith Y. Netzer">
<meta name="citation_publication_date" content="2025/04/14">
	<meta name="citation_journal_title" content="Code4Lib Journal">
		<meta name="citation_issue" content="60">
<!-- end  Google Scholar Stuff -->

<meta name='robots' content='max-image-preview:large' />
	<style>img:is([sizes="auto" i], [sizes^="auto," i]) { contain-intrinsic-size: 3000px 1500px }</style>
	<link rel="alternate" type="application/rss+xml" title="The Code4Lib Journal &raquo; Large Language Models for Machine-Readable Citation Data: Towards an Automated Metadata Curation Pipeline for Scholarly Journals Comments Feed" href="18368/feed" />
<script type="text/javascript">
/* <![CDATA[ */
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.1.0\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.1.0\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/journal.code4lib.org\/wp-includes\/js\/wp-emoji-release.min.js?ver=6.8.1"}};
/*! This file is auto-generated */
!function(i,n){var o,s,e;function c(e){try{var t={supportTests:e,timestamp:(new Date).valueOf()};sessionStorage.setItem(o,JSON.stringify(t))}catch(e){}}function p(e,t,n){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);var t=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data),r=(e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(n,0,0),new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data));return t.every(function(e,t){return e===r[t]})}function u(e,t,n){switch(t){case"flag":return n(e,"\ud83c\udff3\ufe0f\u200d\u26a7\ufe0f","\ud83c\udff3\ufe0f\u200b\u26a7\ufe0f")?!1:!n(e,"\ud83c\uddfa\ud83c\uddf3","\ud83c\uddfa\u200b\ud83c\uddf3")&&!n(e,"\ud83c\udff4\udb40\udc67\udb40\udc62\udb40\udc65\udb40\udc6e\udb40\udc67\udb40\udc7f","\ud83c\udff4\u200b\udb40\udc67\u200b\udb40\udc62\u200b\udb40\udc65\u200b\udb40\udc6e\u200b\udb40\udc67\u200b\udb40\udc7f");case"emoji":return!n(e,"\ud83d\udc26\u200d\ud83d\udd25","\ud83d\udc26\u200b\ud83d\udd25")}return!1}function f(e,t,n){var r="undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?new OffscreenCanvas(300,150):i.createElement("canvas"),a=r.getContext("2d",{willReadFrequently:!0}),o=(a.textBaseline="top",a.font="600 32px Arial",{});return e.forEach(function(e){o[e]=t(a,e,n)}),o}function t(e){var t=i.createElement("script");t.src=e,t.defer=!0,i.head.appendChild(t)}"undefined"!=typeof Promise&&(o="wpEmojiSettingsSupports",s=["flag","emoji"],n.supports={everything:!0,everythingExceptFlag:!0},e=new Promise(function(e){i.addEventListener("DOMContentLoaded",e,{once:!0})}),new Promise(function(t){var n=function(){try{var e=JSON.parse(sessionStorage.getItem(o));if("object"==typeof e&&"number"==typeof e.timestamp&&(new Date).valueOf()<e.timestamp+604800&&"object"==typeof e.supportTests)return e.supportTests}catch(e){}return null}();if(!n){if("undefined"!=typeof Worker&&"undefined"!=typeof OffscreenCanvas&&"undefined"!=typeof URL&&URL.createObjectURL&&"undefined"!=typeof Blob)try{var e="postMessage("+f.toString()+"("+[JSON.stringify(s),u.toString(),p.toString()].join(",")+"));",r=new Blob([e],{type:"text/javascript"}),a=new Worker(URL.createObjectURL(r),{name:"wpTestEmojiSupports"});return void(a.onmessage=function(e){c(n=e.data),a.terminate(),t(n)})}catch(e){}c(n=f(s,u,p))}t(n)}).then(function(e){for(var t in e)n.supports[t]=e[t],n.supports.everything=n.supports.everything&&n.supports[t],"flag"!==t&&(n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&n.supports[t]);n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&!n.supports.flag,n.DOMReady=!1,n.readyCallback=function(){n.DOMReady=!0}}).then(function(){return e}).then(function(){var e;n.supports.everything||(n.readyCallback(),(e=n.source||{}).concatemoji?t(e.concatemoji):e.wpemoji&&e.twemoji&&(t(e.twemoji),t(e.wpemoji)))}))}((window,document),window._wpemojiSettings);
/* ]]> */
</script>
<style id='wp-emoji-styles-inline-css' type='text/css'>

	img.wp-smiley, img.emoji {
		display: inline !important;
		border: none !important;
		box-shadow: none !important;
		height: 1em !important;
		width: 1em !important;
		margin: 0 0.07em !important;
		vertical-align: -0.1em !important;
		background: none !important;
		padding: 0 !important;
	}
</style>
<link rel='stylesheet' id='wp-block-library-css' href='../wp-includes/css/dist/block-library/style.min.css%3Fver=6.8.1.css' type='text/css' media='all' />
<style id='classic-theme-styles-inline-css' type='text/css'>
/*! This file is auto-generated */
.wp-block-button__link{color:#fff;background-color:#32373c;border-radius:9999px;box-shadow:none;text-decoration:none;padding:calc(.667em + 2px) calc(1.333em + 2px);font-size:1.125em}.wp-block-file__button{background:#32373c;color:#fff;text-decoration:none}
</style>
<style id='global-styles-inline-css' type='text/css'>
:root{--wp--preset--aspect-ratio--square: 1;--wp--preset--aspect-ratio--4-3: 4/3;--wp--preset--aspect-ratio--3-4: 3/4;--wp--preset--aspect-ratio--3-2: 3/2;--wp--preset--aspect-ratio--2-3: 2/3;--wp--preset--aspect-ratio--16-9: 16/9;--wp--preset--aspect-ratio--9-16: 9/16;--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--x-large: 42px;--wp--preset--spacing--20: 0.44rem;--wp--preset--spacing--30: 0.67rem;--wp--preset--spacing--40: 1rem;--wp--preset--spacing--50: 1.5rem;--wp--preset--spacing--60: 2.25rem;--wp--preset--spacing--70: 3.38rem;--wp--preset--spacing--80: 5.06rem;--wp--preset--shadow--natural: 6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep: 12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp: 6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined: 6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp: 6px 6px 0px rgba(0, 0, 0, 1);}:where(.is-layout-flex){gap: 0.5em;}:where(.is-layout-grid){gap: 0.5em;}body .is-layout-flex{display: flex;}.is-layout-flex{flex-wrap: wrap;align-items: center;}.is-layout-flex > :is(*, div){margin: 0;}body .is-layout-grid{display: grid;}.is-layout-grid > :is(*, div){margin: 0;}:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}
:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}
:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}
:root :where(.wp-block-pullquote){font-size: 1.5em;line-height: 1.6;}
</style>
<style id='akismet-widget-style-inline-css' type='text/css'>

			.a-stats {
				--akismet-color-mid-green: #357b49;
				--akismet-color-white: #fff;
				--akismet-color-light-grey: #f6f7f7;

				max-width: 350px;
				width: auto;
			}

			.a-stats * {
				all: unset;
				box-sizing: border-box;
			}

			.a-stats strong {
				font-weight: 600;
			}

			.a-stats a.a-stats__link,
			.a-stats a.a-stats__link:visited,
			.a-stats a.a-stats__link:active {
				background: var(--akismet-color-mid-green);
				border: none;
				box-shadow: none;
				border-radius: 8px;
				color: var(--akismet-color-white);
				cursor: pointer;
				display: block;
				font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen-Sans', 'Ubuntu', 'Cantarell', 'Helvetica Neue', sans-serif;
				font-weight: 500;
				padding: 12px;
				text-align: center;
				text-decoration: none;
				transition: all 0.2s ease;
			}

			/* Extra specificity to deal with TwentyTwentyOne focus style */
			.widget .a-stats a.a-stats__link:focus {
				background: var(--akismet-color-mid-green);
				color: var(--akismet-color-white);
				text-decoration: none;
			}

			.a-stats a.a-stats__link:hover {
				filter: brightness(110%);
				box-shadow: 0 4px 12px rgba(0, 0, 0, 0.06), 0 0 2px rgba(0, 0, 0, 0.16);
			}

			.a-stats .count {
				color: var(--akismet-color-white);
				display: block;
				font-size: 1.5em;
				line-height: 1.4;
				padding: 0 13px;
				white-space: nowrap;
			}
		
</style>
<link rel="https://api.w.org/" href="../wp-json/index.html" /><link rel="alternate" title="JSON" type="application/json" href="../wp-json/wp/v2/posts/18368" /><link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://journal.code4lib.org/xmlrpc.php?rsd" />
<link rel="canonical" href="../index.html%3Fp=18368.html" />
<link rel='shortlink' href='../index.html%3Fp=18368.html' />
<link rel="alternate" title="oEmbed (JSON)" type="application/json+oembed" href="../wp-json/oembed/1.0/embed%3Furl=https:%252F%252Fjournal.code4lib.org%252Farticles%252F18368" />
<link rel="alternate" title="oEmbed (XML)" type="text/xml+oembed" href="../wp-json/oembed/1.0/embed%3Furl=https:%252F%252Fjournal.code4lib.org%252Farticles%252F18368&amp;format=xml" />
<style>
@media all and (max-width : 768px) {
.syntaxhighlighter a, .syntaxhighlighter div, .syntaxhighlighter code, .syntaxhighlighter table, .syntaxhighlighter table td, .syntaxhighlighter table tr, .syntaxhighlighter table tbody, .syntaxhighlighter table thead, .syntaxhighlighter table caption, .syntaxhighlighter textarea
{
	font-size: 0.95em !important;
}
}
</style>
	</head>
	<body>
		<div id="page">
			<div id="header">
				<div id="headerbackground">
					<h1><a href="../index.html"><img src="../wp-content/themes/c4lj-theme/images/logo.png" alt="The Code4Lib Journal" /></a></h1>
				</div>
				<div id="about">
					<ul>
						<li class="page_item page-item-5"><a href="../index.html%3Fp=5.html">Mission</a></li>
<li class="page_item page-item-6"><a href="../editorial-committee/index.html">Editorial Committee</a></li>
<li class="page_item page-item-8"><a href="../process/index.html">Process and Structure</a></li>
						<li><a href="http://code4lib.org/">Code4Lib</a></li>
					</ul>
				</div>
				<div class="mobile-search">
					<form method="get" id="searchform" action="../index.html">
						<div>
							<input type="text" value="" aria-labelledby="searchsubmit" name="s" id="s" />
							<input type="submit" value="Search" id="searchsubmit" />
						</div>
					</form>
				</div>
			</div>

			<div id="content">
								<div class="article" id="post-18368">
					<p id="issueDesignation"><a href="../issues/issues/issue60.html">Issue 60, 2025-04-14</a></p>
					<h1 class="articletitle">Large Language Models for Machine-Readable Citation Data: Towards an Automated Metadata Curation Pipeline for Scholarly Journals</h1>
					<div class="abstract">
						<p>Northwestern University spent far too much time and effort curating citation data by hand. Here, we show that large language models can be an efficient way to convert plain-text citations to BibTeX for use in machine-actionable metadata. Further, we prove that these models can be run locally, without cloud compute cost. With these tools, university-owned publishing operations can increase their operating efficiency which, when combined with human review, has no effect on quality.</p>
					</div>
					<div class="entry">
						<p>By Aerith Y. Netzer</p>
<h2>Background and Motivation</h2>
<p>Northwestern University Libraries publishes two peer-reviewed journals, <em>The Bulletin of Applied Transgender Studies</em>, and <em>Studies in Russian Philosophy, Literature, and Religious Thought</em>. Northwestern’s journal publishing operates under tight economic constraints—direct and opportunity—and therefore must solve the same problems of corporate academic publishers with a fraction of the resources available [<a id="ref1" href="../index.html%3Fp=18368.html#note1">1</a>][<a id="ref2" href="../index.html%3Fp=18368.html#note2">2</a>]. One of these problems is reference metadata, i.e., machine-actionable references that are then used to count citations of articles. The act of capturing, counting, and using citations accurately enables funding agencies, universities, and publishers to make data-driven decisions for funding allocation, reviewers to validate the research of a manuscript, and faster literature review.</p>
<h2>An Example</h2>
<p>The workflow for our university—a medium-size, elite university in the Midwest United States—consists of receiving manuscripts from authors in a Microsoft Word file format. We then use pandoc [<a id="ref3" href="../index.html%3Fp=18368.html#note3">3</a>] to transform this Word document to a markdown file format, from which we can build PDF and Web versions from a single source. But due to manuscript author’s primarily writing their manuscript in Microsoft Word, this meant looking up each source, adding them to a Zotero [<a id="ref4" href="../index.html%3Fp=18368.html#note4">4</a>] library, and then exporting the BibTeX file for use as metadata in the web version of the article. As Northwestern Libraries’ journal-publishing operation is a one-woman show and quickly growing in complexity and scope, we found it necessary to find a way to find a faster way.</p>
<p>There have been many projects aimed to converting plain-text to BibTeX using programmatic means, but are often limited to certain languages [<a id="ref5" href="../index.html%3Fp=18368.html#note5">5</a>] or are dependent upon external data [<a id="ref6" href="../index.html%3Fp=18368.html#note6">6</a>]. As Northwestern’s journal submissions often use non-latin languages, such as sources in <em>Studies in Russian Philosophy</em>, this is a limitation that precludes many of the sources necessary for us to translate into machine-readable text [<a id="ref7" href="../index.html%3Fp=18368.html#note7">7</a>]. As Large Language Models grew popular, we originally reached for the most popular option — the GPT-3 and 3.5 API. However, due to these popular options being paid, using this method would not be scalable to many journals with hundreds of citations to process per volume/issue. Further, we as an organization prefer transparency and replicability in our tools. As GPT is managed by OpenAI, access to the model can be closed at any time, forcing us to move to a new system. While with open-source systems, we can upgrade or downgrade as needs arise, and we need not pay. Thus, we reached for another, more open, tool—Ollama [<a id="ref8" href="../index.html%3Fp=18368.html#note8">8</a>].</p>
<h2>Limitations &amp; Concerns</h2>
<p>This analysis is limited to works that appear in the crossref API, creating a bias in the dataset against older works and academic monographs. While this limits the usefulness of this analysis to publishers whose specialty lies within fields where citations are limited to recent works (such as the physical sciences), future work can and should include plain-text citations of historical, non-digital, and non-academic works.</p>
<p>Along with the rapid growth in users of Large-Language models, so have concerns over the ecological sustainability of LLM technology [<a id="ref9" href="../index.html%3Fp=18368.html#note9">9</a>][<a id="ref10" href="../index.html%3Fp=18368.html#note10">10</a>]. Most of these concerns, however, can be alleviated with the use of &#8220;small&#8221; models such as those provided by Ollama. Further, there are concerns about the validity of Large-Language models, especially concerning their propensity to hallucinate. However, in combination with validity checkers such as bibtexparser and human review, we are confident enough in this system to be used in production of our journals [<a id="ref11" href="../index.html%3Fp=18368.html#note11">11</a>]. Future work in this area should include building scalable, verifiable workflows that require less human oversight.</p>
<h2>Methodology</h2>
<p>Data [<a id="ref12" href="../index.html%3Fp=18368.html#note12">12</a>] was collected via the CrossRef API. Using the sample function of the crossref API, we retrieved a random DOI. Then, using the Crossref content negotiation endpoint, we were able to retrieve a plain-text formatted citation from a randomly selected citation style from the following:</p>
<ol>
<li>Chicago Author-Date</li>
<li>Elsevier-Harvard</li>
<li>Ecoscience</li>
<li>APA</li>
<li>MLA</li>
<li>IEEE</li>
<li>Council of Science Editors</li>
</ol>
<p>Using the CrossRef API, we pulled the BibTex Citation, the Plaintext Citation, and the DOI to create a dataset for our analysis. Table 1 presents the variables and their descriptions.</p>
<table>
<caption>Table 1: Citation Metadata</caption>
<tbody>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
<tr>
<td>DOI</td>
<td>The Digital Object Identifier of the requisite work.</td>
</tr>
<tr>
<td>BibTeX Citation</td>
<td>Metadata about the work in BibTeX format.</td>
</tr>
<tr>
<td>Plain Text Citation</td>
<td>The cited work in a given citation style.</td>
</tr>
<tr>
<td>Plain Text Citation Style</td>
<td>The style in which the plain text citation is given.</td>
</tr>
</tbody>
</table>
<p>Using this random assignment of citation formats, we achieved a roughly even distribution of each citation style in the dataset (see Figure 1).</p>
<p class="caption"><img decoding="async" src="../media/issue60/netzer/style_props.png" /><br />
<strong>Figure 1.</strong> Pie Chart demonstrating the proportion of each citation style present in the dataset.</p>
<h2>Analysis</h2>
<p>All language models (see Table 2) were tested using the Ollama toolkit using the Quest [<a id="ref13" href="../index.html%3Fp=18368.html#note13">13</a>] supercomputing cluster at Northwestern University, running in a singularity container [<a id="ref14" href="../index.html%3Fp=18368.html#note14">14</a>][<a id="ref15" href="../index.html%3Fp=18368.html#note15">15</a>]. Testing of all models took 14 hours to complete on two NVIDIA A100 Graphical Processing Units, one node with eight cores, and 128 gigabytes of memory[<a id="ref16" href="../index.html%3Fp=18368.html#note16">16</a>][<a id="ref17" href="../index.html%3Fp=18368.html#note17">17</a>]. All code was written in python using an Anaconda environment to aid in reproducible deployments of this code [<a id="ref18" href="../index.html%3Fp=18368.html#note18">18</a>].  We used the plain text citation given by the CrossRef content negotiation API as a ground truth to which the model would aspire. We prompted each model with the same text:</p>
<blockquote><p><em>You are a professional citation parser. Given the following plain text citation: {plain_text_citation} Please convert this citation into a structured BibTeX entry. Include all relevant fields such as author, title, journal, volume, pages, year, etc. Output only the BibTeX entry, and nothing else. Do not include any explanations, preambles, or additional text.</em></p></blockquote>
<p>The following models were prompted in this analysis</p>
<ul>
<li><a href="https://huggingface.co/google/codegemma-2b">codegemma:2b</a></li>
<li><a href="https://huggingface.co/google/codegemma-7b">codegemma:7b</a></li>
<li><a href="https://huggingface.co/meta-llama/Llama-2-7b">llama2:7b</a></li>
<li><a href="https://huggingface.co/meta-llama/Meta-Llama-3-70B">llama3.3:70b</a></li>
<li><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B">llama3:8b</a></li>
<li><a href="https://huggingface.co/mistralai/Mistral-7B-v0.1">mistral:7b</a></li>
<li><a href="https://huggingface.co/bigcode/starcoder2-3b">starcoder2:3b</a></li>
<li><a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0">tinyllama</a></li>
</ul>
<p>These models were chosen to represent a range of model sizes and training methods. The following variables were saved to the output file of the model:</p>
<table>
<caption>Table 3: Study Variables</caption>
<tbody>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
<tr>
<td>Model</td>
<td>The model being tested. One of the eight models listed above.</td>
</tr>
<tr>
<td>PlainTextCitation</td>
<td>Maps to plain text citation field table 1.</td>
</tr>
<tr>
<td>TimeToGeneration</td>
<td>Time taken to generate the entry.</td>
</tr>
<tr>
<td>ActualBibTeX</td>
<td>BibTeX entry retrieved from Crossref.</td>
</tr>
<tr>
<td>TotalFields</td>
<td>The number of BibTeX fields being compared in generated and “ground truth” entries.</td>
</tr>
<tr>
<td>Matching Fields</td>
<td>The number of fields that have a match in both the generated and “ground truth” entries.</td>
</tr>
<tr>
<td>Percentage Match (Overall Accuracy)</td>
<td>Matching Fields / Total Fields</td>
</tr>
</tbody>
</table>
<p>This generated 8 CSV files of approximately 3,000 lines each. Each row corresponds to a single DOI. These files were used for analyzing the efficiency and effectiveness of each model.</p>
<h2>Model Effectiveness</h2>
<p class="caption"><img decoding="async" src="../media/issue60/netzer/overall_accuracy_comparison.png" /><br />
<strong>Figure 2.</strong> Per Field Accuracy and Valid BibTeX of the Model.</p>
<p>Unsurprisingly, llama3.3:70b, the most advanced and largest model of the chosen models, performed the best. Further, starcoder2:3b failed to create any valid BibTeX entries, whereas every other model created valid BibTeX for every citation.</p>
<p class="caption"><img decoding="async" src="../media/issue60/netzer/time_to_generation_boxplot.png" /><br />
<strong>Figure 3.</strong>Box and Whisker Plot for TimeToGeneration by Model</p>
<table>
<caption>Model time to Generation</caption>
<tbody>
<tr>
<th>Model</th>
<th>Median Time to Generation (seconds)</th>
<th>Standard Deviation (seconds)</th>
</tr>
<tr>
<td>codegemma:2b</td>
<td>1.18</td>
<td>3.24</td>
</tr>
<tr>
<td>codegemma:7b</td>
<td>1.08</td>
<td>0.29</td>
</tr>
<tr>
<td>llama2:7b</td>
<td>0.96</td>
<td>0.27</td>
</tr>
<tr>
<td>llama3.3:70b</td>
<td>5.45</td>
<td>1.90</td>
</tr>
<tr>
<td>llama3:8b</td>
<td>1.00</td>
<td>0.31</td>
</tr>
<tr>
<td>mistral</td>
<td>1.00</td>
<td>0.27</td>
</tr>
<tr>
<td>starcoder2:3b</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>tinyllama</td>
<td>0.57</td>
<td>0.64</td>
</tr>
</tbody>
</table>
<p>As every model except for starcoder2:3b created valid BibTeX perfectly, we are primarily concerned with the accuracy of the fields. In this discussion, the <em>validity</em> of the BibTeX simply means that if the BibTeX can be parsed without errors, then the BibTeX is valid. However, a well-formed BibTeX entry can be <em>valid</em> but <em>incorrect</em>. Meaning that the entry can be parsed, but the data in the entry is wrong. llama3.3:70b generated the most <em>accurate</em> BibTeX entries. However, we should not assume that the model was necessarily <em>incorrect</em>, but was just different from how Crossref represented the field. Mistral and Codegemma, though, are very close behind, especially with their parameter sizes (and therefore cost of compute) much lower than llama3.3:70b, it may be economical for some publishing operations to use smaller models, decreasing their cost, while keeping parity with the accuracy of the model. Trading a .2% reduction in overall accuracy for, on average, a 5x faster computation is an effective strategy for this use case.</p>
<p class="caption"><img decoding="async" src="../media/issue60/netzer/per_field_accuracy_comparison.png" /><br />
<strong>Figure 4.</strong>Per-field accuracy by model</p>
<p>All models were very accurate in producing volume, year, and journal entries in BibTeX, while author, publisher, and school were the least accurate fields. This is because there is greater freedom and flexibility in how these fields are entered, and thus a correct and valid generated BibTeX need not be exactly the same as Crossref’s representation of the same data. Future work should include creating a validator to identify equivalent author, publisher, and school names.<br />
For example, consider the following BibTeX entries:</p>
<pre> {National Academy of Sciences, The}
{The National Academy of Sciences}</pre>
<p>While these entries refer to the same entity, they cannot be identified as the same programmatically, and are thus penalized as “inaccurate.” Thus, the results in Figure 2 should be interpreted as the models’ accuracy when using Crossref as the metric of accuracy. This analysis is useful because it shows which models are better-suited for this task, rather than the concluding 50% of the fields to be incorrect.</p>
<h2>Recreating Results on Consumer Hardware</h2>
<p>While we ran these models on a supercomputer to aid in analysis, models with parameter sizes of less than 10 billion can be run on current consumer hardware. We recommend a computer with a dedicated, modern GPU (verified to work on the author’s personal Nvidia 3080Ti, AMD Ryzen 6-core CPU, and 32 GB of RAM; and an Apple M3 Max with 36 GB of memory)<br />
For UNIX Systems, use cURL to install Ollama with one command:</p>
<pre> curl -fsSL https://ollama.com/install.sh | sh</pre>
<p>Then pull the model (we recommend mistral):</p>
<pre> ollama pull mistral</pre>
<p>Then, start an Ollama server listening on port 11434:</p>
<pre> def generate_text_with_ollama(model_name, prompt):
    url = 'http://localhost:11434/api/generate'
    payload = {
        "model": model_name,
        "prompt": prompt,
        "temperature": 0,  # Make output more deterministic
        "stop": ["\n\n"]
    }
    headers = {
        "Content-Type": "application/json"
    }
    response = requests.post(url, headers=headers,
                             data=json.dumps(payload), stream=True)

    # Handle streaming response
    generated_text = ''
    for line in response.iter_lines():
        if line:
            data = json.loads(line)
            if data.get('done', False):
                break
            else:
                generated_text += data.get('response', '')

    return generated_text.strip()
</pre>
<p>You can then pass any input you like to this function and return a generated BibTex Key. Full code sample is in the author’s GitHub repo.</p>
<h2>Conclusion</h2>
<p>For university-owned publishers, small, locally-available LLMs are capable of producing well-formed BibTeX. These models can be used to create machine-actionable citation metadata, automating a step in the publishing process. As of the publication of this paper, 7 billion parameter models, especially mistral, are capable of running on the latest laptops, and provide acceptable performance at the least cost.</p>
<h2>Data and Code Availability</h2>
<p>The author strives to adhere to the FAIR guiding principles. Code and data used for this analysis is <a href="https://github.com/aerithnetzer/biblatex-transformer.">available on GitHub</a>.</p>
<h2>References and Notes</h2>
<p>[<a id="note1" href="../index.html%3Fp=18368.html#ref1">1</a>] Association of College &amp; Research Libraries. &#8220;The State of U.S. Academic Libraries: Findings from the ACRL 2023 Annual Survey.&#8221; Chicago: <em>Association of College &amp; Research Libraries, 2024</em>. Retrieved from <a href="https://www.ala.org/sites/default/files/2024-10/2023%20State%20of%20Academic%20Libraries%20Report.pdf">https://www.ala.org/sites/default/files/2024-10/2023%20State%20of%20Academic%20Libraries%20Report.pdf</a></p>
<p>[<a id="note2" href="../index.html%3Fp=18368.html#ref2">2</a>] RELX. 2023. “Market Segments.” RELX.</p>
<p>[<a id="note3" href="../index.html%3Fp=18368.html#ref3">3</a>] “Pandoc &#8211; Index.” n.d. <a href="https://pandoc.org/">https://pandoc.org/</a>. Accessed March 8, 2024.</p>
<p>[<a id="note4" href="../index.html%3Fp=18368.html#ref4">4</a>] “Zotero Your Personal Research Assistant.” n.d. https://www.zotero.org/. Accessed December 31, 2024.</p>
<p>[<a id="note5" href="../index.html%3Fp=18368.html#ref5">5</a>] “Makino Takaki’s Page &#8211; Writings &#8211; Technical Tips &#8211; Generate BiBTeX Entry from Plain Text (.en).” n.d. <a href="https://www.snowelm.com/~t/doc/tips/makebib.en.html">https://www.snowelm.com/~t/doc/tips/makebib.en.html</a>. Accessed March 9, 2024.</p>
<p>[<a id="note6" href="../index.html%3Fp=18368.html#ref6">6</a>] “Text2bib.” n.d. <a href="https://text2bib.economics.utoronto.ca/index.php/index.">https://text2bib.economics.utoronto.ca/index.php/index.</a> Accessed March 9, 2024.</p>
<p>[<a id="note7" href="../index.html%3Fp=18368.html#ref7">7</a>] Williams, Rowan. 2024. “Sergeii Bulgakov, Socialism, and the Church.” <em>Northwestern University Studies in Russian Philosophy, Literature, and Religious Thought</em>.</p>
<p>[<a id="note8" href="../index.html%3Fp=18368.html#ref8">8</a>] “Ollama/Ollama.” 2025. Ollama.</p>
<p>[<a id="note9" href="../index.html%3Fp=18368.html#ref9">9</a>] Ding, Yi, and Tianyao Shi. 2024. “Sustainable LLM Serving: Environmental Implications, Challenges, and Opportunities : Invited Paper.” In <em>2024 IEEE 15th International Green and Sustainable Computing Conference</em> (IGSC), 37–38. <a href="https://doi.org/10.1109/IGSC64514.2024.00016.">https://doi.org/10.1109/IGSC64514.2024.00016.</a></p>
<p>[<a id="note10" href="../index.html%3Fp=18368.html#ref10">10</a>] (Chien, Andrew A, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. 2023. “Reducing the Carbon Impact of Generative AI Inference (Today and in 2035).” In <em>Proceedings of the 2nd Workshop on Sustainable Computer Systems,</em> 1–7. Boston MA USA: ACM. <a href="https://doi.org/10.1145/3604930.3605705.">https://doi.org/10.1145/3604930.3605705.</a></p>
<p>[<a id="note11" href="../index.html%3Fp=18368.html#ref11">11</a>] &#8220;About the Journal &#8211; Bulletin of Applied Transgender Studies.” n.d. <a href="https://bulletin.appliedtransstudies.org/about/">https://bulletin.appliedtransstudies.org/about/</a>. Accessed March 8, 2024.</p>
<p>[<a id="note12" href="../index.html%3Fp=18368.html#ref12">12</a>] [<a id="note18" href="../index.html%3Fp=18368.html#ref18">18</a>] Netzer, Aerith. “aerithnetzer/Biblatex-Transformer.” 2025. <a href="https://github.com/aerithnetzer/biblatex-transformer">https://github.com/aerithnetzer/biblatex-transformer</a>.</p>
<p>[<a id="note13" href="../index.html%3Fp=18368.html#ref13">13</a>] “Quest High-Performance Computing Cluster: Information Technology &#8211; Northwestern University.” n.d. <a href="https://www.it.northwestern.edu/departments/it-services-support/research/computing/quest/">https://www.it.northwestern.edu/departments/it-services-support/research/computing/quest/</a>. Accessed January 1, 2025</p>
<p>[<a id="note14" href="../index.html%3Fp=18368.html#ref14">14</a>] Kurtzer, Gregory M., Vanessa Sochat, and Michael W. Bauer. 2017. “Singularity: Scientific Containers for Mobility of Compute.” <em>PLOS ONE</em> 12 (5): e0177459. <a href="https://doi.org/10.1371/journal.pone.0177459.">https://doi.org/10.1371/journal.pone.0177459.</a></p>
<p>[<a id="note15" href="../index.html%3Fp=18368.html#ref15">15</a>] Singularity containers allow for reproducible environments for analysis. The singularity definition file used in this analysis can be found in the GitHub repository. As LLMs are typically stochastic in nature, one can expect to have reasonably similar, but not exactly the same, results as presented in this paper. The singularity definition file primarily serves as a resource for readers to deploy this system in their home institution.</p>
<p>[<a id="note16" href="../index.html%3Fp=18368.html#ref16">16</a>]The script used to create the SLURM job can be found in the GitHub repository <a href="https://github.com/aerithnetzer/biblatex-transformer.">https://github.com/aerithnetzer/biblatex-transformer.</a></p>
<p>[<a id="note17" href="../index.html%3Fp=18368.html#ref17">17</a>] Thank you to Kat Nykiel at Purdue University for her assistance in building and deploying the singularity container.</p>
<h2>About the Author</h2>
<p>Aerith Y. Netzer is the Digital Publishing and Repository Librarian at Northwestern University in Evanston, Illinois.</p>
					</div>
														</div>
				<!-- You can start editing here. -->

<div class="comments">
	<p class="subscriptionlinks">Subscribe to comments: <a href="18368/feed">For this article</a> | <a href="http://feeds.feedburner.com/c4lj/comments">For all articles</a></p>

			<!-- If comments are open, but there are no comments. -->

	 

<h3 id="respond">Leave a Reply</h3>


<form action="https://journal.code4lib.org/wp-comments-post.php" method="post" id="commentform">


<p><input type="text" name="author" id="author" value=""/>
<label for="author">Name (required)</label></p>

<p><input type="text" name="email" id="email" value="" />
<label for="email">Mail (will not be published) (required)</label></p>

<p><input type="text" name="url" id="url" value="" />
<label for="url">Website</label></p>


<p><textarea autocomplete="new-password"  aria-label="Comment box" id="f127f6ccbe"  name="f127f6ccbe"   cols="50" rows="10"></textarea><textarea id="comment" aria-label="hp-comment" aria-hidden="true" name="comment" autocomplete="new-password" style="padding:0 !important;clip:rect(1px, 1px, 1px, 1px) !important;position:absolute !important;white-space:nowrap !important;height:1px !important;width:1px !important;overflow:hidden !important;" tabindex="-1"></textarea><script data-noptimize>document.getElementById("comment").setAttribute( "id", "a75e686d38811a48ef4f741e200ada4d" );document.getElementById("f127f6ccbe").setAttribute( "id", "comment" );</script></p>

<p><input name="submit" type="submit" id="submit"  value="Submit Comment" />
<input type="hidden" name="comment_post_ID" value="18368" />
</p>
<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="322898535f" /></p><p style="display: none !important;" class="akismet-fields-container" data-prefix="ak_"><label>&#916;<textarea name="ak_hp_textarea" cols="45" rows="8" maxlength="100"></textarea></label><input type="hidden" id="ak_js_1" name="ak_js" value="249"/><script>document.getElementById( "ak_js_1" ).setAttribute( "value", ( new Date() ).getTime() );</script></p>
</form>


</div>
							</div>

			<div id="meta">
				<div id="issn">
					<p>ISSN 1940-5758</p>
				</div>
				<div class="search-sidebar">
				<form method="get" id="searchform" action="../index.html">
					<div>
						<input type="text" value="" aria-labelledby="searchsubmit" name="s" id="s" />
						<input type="submit" value="Search" id="searchsubmit"/>
					</div>
				</form>
				</div>
				<div id="archives">
					<h2>Current Issue</h2>
						<ul>
							<li><a href="../issues/issues/issue60.html">Issue 60, 2025-04-14</a></li>
						</ul>

					<h2>Previous Issues</h2>
						<ul>
              <li><a href="../issues/issues/issue59.html">Issue 59, 2024-10-07</a></li><li><a href="../issues/issues/issue58.html">Issue 58, 2023-12-04</a></li><li><a href="../issues/issues/issue57.html">Issue 57, 2023-08-29</a></li><li><a href="../issues/issues/issue56.html">Issue 56, 2023-04-21</a></li>              <li><a href="../index.html%3Fp=2476.html">Older Issues</a></li>
						</ul>
				</div>
				<div id="forauthors">
					<h2>For Authors</h2>
					<ul>
						<li class="page_item page-item-4"><a href="../index.html%3Fp=4.html">Call for Submissions</a></li>
<li class="page_item page-item-7"><a href="../index.html%3Fp=7.html">Article Guidelines</a></li>
					</ul>
				</div>
			</div>
						<div id="footer">
				<p id="login"><a href="../wp-login.php.html">Log in</a></p>
				<p id="copyright">This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/3.0/us/">Creative Commons Attribution 3.0 United States License</a>.<br /><a rel="license" href="http://creativecommons.org/licenses/by/3.0/us/"><img alt="Creative Commons License" src="http://i.creativecommons.org/l/by/3.0/us/80x15.png" /></a></p>
			</div>
			<script type="speculationrules">
{"prefetch":[{"source":"document","where":{"and":[{"href_matches":"\/*"},{"not":{"href_matches":["\/wp-*.php","\/wp-admin\/*","\/wp-content\/uploads\/*","\/wp-content\/*","\/wp-content\/plugins\/*","\/wp-content\/themes\/c4lj-theme\/*","\/*\\?(.+)"]}},{"not":{"selector_matches":"a[rel~=\"nofollow\"]"}},{"not":{"selector_matches":".no-prefetch, .no-prefetch a"}}]},"eagerness":"conservative"}]}
</script>
<script defer type="text/javascript" src="../wp-content/plugins/akismet/_inc/akismet-frontend.js%3Fver=1748382734" id="akismet-frontend-js"></script>
		</div>
	</body>
</html>
