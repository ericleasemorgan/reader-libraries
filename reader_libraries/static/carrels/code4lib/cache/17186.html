<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

		<title>The Code4Lib Journal &#8211; The viability of using an open source locally hosted AI for creating metadata in digital image collections</title>

		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta name="generator" content="WordPress 6.8.1" /> <!-- leave this for stats -->
    <link rel="shortcut icon" href="../wp-content/themes/c4lj-theme/images/favicon.ico" />
		<link rel="stylesheet" href="../wp-content/themes/c4lj-theme/style.css" type="text/css" media="screen, print" />
		<!--[if lte IE 7]>
		<link rel="stylesheet" href="https://journal.code4lib.org/wp-content/themes/c4lj-theme/fix-ie7.css" type="text/css" media="screen" />
		<![endif]-->
		<!--[if lte IE 6]>
		<link rel="stylesheet" href="https://journal.code4lib.org/wp-content/themes/c4lj-theme/fix-ie6.css" type="text/css" media="screen" />
		<![endif]-->
		<link rel="stylesheet" href="../wp-content/themes/c4lj-theme/print.css" type="text/css" media="print" />
		<link rel="alternate" type="application/rss+xml" title="The Code4Lib Journal Syndication Feed" href="http://feeds.feedburner.com/c4lj" />
		<link rel="pingback" href="https://journal.code4lib.org/xmlrpc.php" />

<!-- Google Scholar Stuff -->
	<meta name="citation_title" content="The viability of using an open source locally hosted AI for creating metadata in digital image collections">
 <meta name="citation_author" content="Ingrid Reiche">
<meta name="citation_publication_date" content="2023/04/21">
	<meta name="citation_journal_title" content="Code4Lib Journal">
		<meta name="citation_issue" content="56">
<!-- end  Google Scholar Stuff -->

<meta name='robots' content='max-image-preview:large' />
	<style>img:is([sizes="auto" i], [sizes^="auto," i]) { contain-intrinsic-size: 3000px 1500px }</style>
	<link rel="alternate" type="application/rss+xml" title="The Code4Lib Journal &raquo; The viability of using an open source locally hosted AI for creating metadata in digital image collections Comments Feed" href="17186/feed" />
<script type="text/javascript">
/* <![CDATA[ */
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.1.0\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.1.0\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/journal.code4lib.org\/wp-includes\/js\/wp-emoji-release.min.js?ver=6.8.1"}};
/*! This file is auto-generated */
!function(i,n){var o,s,e;function c(e){try{var t={supportTests:e,timestamp:(new Date).valueOf()};sessionStorage.setItem(o,JSON.stringify(t))}catch(e){}}function p(e,t,n){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);var t=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data),r=(e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(n,0,0),new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data));return t.every(function(e,t){return e===r[t]})}function u(e,t,n){switch(t){case"flag":return n(e,"\ud83c\udff3\ufe0f\u200d\u26a7\ufe0f","\ud83c\udff3\ufe0f\u200b\u26a7\ufe0f")?!1:!n(e,"\ud83c\uddfa\ud83c\uddf3","\ud83c\uddfa\u200b\ud83c\uddf3")&&!n(e,"\ud83c\udff4\udb40\udc67\udb40\udc62\udb40\udc65\udb40\udc6e\udb40\udc67\udb40\udc7f","\ud83c\udff4\u200b\udb40\udc67\u200b\udb40\udc62\u200b\udb40\udc65\u200b\udb40\udc6e\u200b\udb40\udc67\u200b\udb40\udc7f");case"emoji":return!n(e,"\ud83d\udc26\u200d\ud83d\udd25","\ud83d\udc26\u200b\ud83d\udd25")}return!1}function f(e,t,n){var r="undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?new OffscreenCanvas(300,150):i.createElement("canvas"),a=r.getContext("2d",{willReadFrequently:!0}),o=(a.textBaseline="top",a.font="600 32px Arial",{});return e.forEach(function(e){o[e]=t(a,e,n)}),o}function t(e){var t=i.createElement("script");t.src=e,t.defer=!0,i.head.appendChild(t)}"undefined"!=typeof Promise&&(o="wpEmojiSettingsSupports",s=["flag","emoji"],n.supports={everything:!0,everythingExceptFlag:!0},e=new Promise(function(e){i.addEventListener("DOMContentLoaded",e,{once:!0})}),new Promise(function(t){var n=function(){try{var e=JSON.parse(sessionStorage.getItem(o));if("object"==typeof e&&"number"==typeof e.timestamp&&(new Date).valueOf()<e.timestamp+604800&&"object"==typeof e.supportTests)return e.supportTests}catch(e){}return null}();if(!n){if("undefined"!=typeof Worker&&"undefined"!=typeof OffscreenCanvas&&"undefined"!=typeof URL&&URL.createObjectURL&&"undefined"!=typeof Blob)try{var e="postMessage("+f.toString()+"("+[JSON.stringify(s),u.toString(),p.toString()].join(",")+"));",r=new Blob([e],{type:"text/javascript"}),a=new Worker(URL.createObjectURL(r),{name:"wpTestEmojiSupports"});return void(a.onmessage=function(e){c(n=e.data),a.terminate(),t(n)})}catch(e){}c(n=f(s,u,p))}t(n)}).then(function(e){for(var t in e)n.supports[t]=e[t],n.supports.everything=n.supports.everything&&n.supports[t],"flag"!==t&&(n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&n.supports[t]);n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&!n.supports.flag,n.DOMReady=!1,n.readyCallback=function(){n.DOMReady=!0}}).then(function(){return e}).then(function(){var e;n.supports.everything||(n.readyCallback(),(e=n.source||{}).concatemoji?t(e.concatemoji):e.wpemoji&&e.twemoji&&(t(e.twemoji),t(e.wpemoji)))}))}((window,document),window._wpemojiSettings);
/* ]]> */
</script>
<style id='wp-emoji-styles-inline-css' type='text/css'>

	img.wp-smiley, img.emoji {
		display: inline !important;
		border: none !important;
		box-shadow: none !important;
		height: 1em !important;
		width: 1em !important;
		margin: 0 0.07em !important;
		vertical-align: -0.1em !important;
		background: none !important;
		padding: 0 !important;
	}
</style>
<link rel='stylesheet' id='wp-block-library-css' href='../wp-includes/css/dist/block-library/style.min.css%3Fver=6.8.1.css' type='text/css' media='all' />
<style id='classic-theme-styles-inline-css' type='text/css'>
/*! This file is auto-generated */
.wp-block-button__link{color:#fff;background-color:#32373c;border-radius:9999px;box-shadow:none;text-decoration:none;padding:calc(.667em + 2px) calc(1.333em + 2px);font-size:1.125em}.wp-block-file__button{background:#32373c;color:#fff;text-decoration:none}
</style>
<style id='global-styles-inline-css' type='text/css'>
:root{--wp--preset--aspect-ratio--square: 1;--wp--preset--aspect-ratio--4-3: 4/3;--wp--preset--aspect-ratio--3-4: 3/4;--wp--preset--aspect-ratio--3-2: 3/2;--wp--preset--aspect-ratio--2-3: 2/3;--wp--preset--aspect-ratio--16-9: 16/9;--wp--preset--aspect-ratio--9-16: 9/16;--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--x-large: 42px;--wp--preset--spacing--20: 0.44rem;--wp--preset--spacing--30: 0.67rem;--wp--preset--spacing--40: 1rem;--wp--preset--spacing--50: 1.5rem;--wp--preset--spacing--60: 2.25rem;--wp--preset--spacing--70: 3.38rem;--wp--preset--spacing--80: 5.06rem;--wp--preset--shadow--natural: 6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep: 12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp: 6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined: 6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp: 6px 6px 0px rgba(0, 0, 0, 1);}:where(.is-layout-flex){gap: 0.5em;}:where(.is-layout-grid){gap: 0.5em;}body .is-layout-flex{display: flex;}.is-layout-flex{flex-wrap: wrap;align-items: center;}.is-layout-flex > :is(*, div){margin: 0;}body .is-layout-grid{display: grid;}.is-layout-grid > :is(*, div){margin: 0;}:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}
:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}
:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}
:root :where(.wp-block-pullquote){font-size: 1.5em;line-height: 1.6;}
</style>
<style id='akismet-widget-style-inline-css' type='text/css'>

			.a-stats {
				--akismet-color-mid-green: #357b49;
				--akismet-color-white: #fff;
				--akismet-color-light-grey: #f6f7f7;

				max-width: 350px;
				width: auto;
			}

			.a-stats * {
				all: unset;
				box-sizing: border-box;
			}

			.a-stats strong {
				font-weight: 600;
			}

			.a-stats a.a-stats__link,
			.a-stats a.a-stats__link:visited,
			.a-stats a.a-stats__link:active {
				background: var(--akismet-color-mid-green);
				border: none;
				box-shadow: none;
				border-radius: 8px;
				color: var(--akismet-color-white);
				cursor: pointer;
				display: block;
				font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen-Sans', 'Ubuntu', 'Cantarell', 'Helvetica Neue', sans-serif;
				font-weight: 500;
				padding: 12px;
				text-align: center;
				text-decoration: none;
				transition: all 0.2s ease;
			}

			/* Extra specificity to deal with TwentyTwentyOne focus style */
			.widget .a-stats a.a-stats__link:focus {
				background: var(--akismet-color-mid-green);
				color: var(--akismet-color-white);
				text-decoration: none;
			}

			.a-stats a.a-stats__link:hover {
				filter: brightness(110%);
				box-shadow: 0 4px 12px rgba(0, 0, 0, 0.06), 0 0 2px rgba(0, 0, 0, 0.16);
			}

			.a-stats .count {
				color: var(--akismet-color-white);
				display: block;
				font-size: 1.5em;
				line-height: 1.4;
				padding: 0 13px;
				white-space: nowrap;
			}
		
</style>
<link rel="https://api.w.org/" href="../wp-json/index.html" /><link rel="alternate" title="JSON" type="application/json" href="../wp-json/wp/v2/posts/17186" /><link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://journal.code4lib.org/xmlrpc.php?rsd" />
<link rel="canonical" href="../index.html%3Fp=17186.html" />
<link rel='shortlink' href='../index.html%3Fp=17186.html' />
<link rel="alternate" title="oEmbed (JSON)" type="application/json+oembed" href="../wp-json/oembed/1.0/embed%3Furl=https:%252F%252Fjournal.code4lib.org%252Farticles%252F17186" />
<link rel="alternate" title="oEmbed (XML)" type="text/xml+oembed" href="../wp-json/oembed/1.0/embed%3Furl=https:%252F%252Fjournal.code4lib.org%252Farticles%252F17186&amp;format=xml" />
<style>
@media all and (max-width : 768px) {
.syntaxhighlighter a, .syntaxhighlighter div, .syntaxhighlighter code, .syntaxhighlighter table, .syntaxhighlighter table td, .syntaxhighlighter table tr, .syntaxhighlighter table tbody, .syntaxhighlighter table thead, .syntaxhighlighter table caption, .syntaxhighlighter textarea
{
	font-size: 0.95em !important;
}
}
</style>
	</head>
	<body>
		<div id="page">
			<div id="header">
				<div id="headerbackground">
					<h1><a href="../index.html"><img src="../wp-content/themes/c4lj-theme/images/logo.png" alt="The Code4Lib Journal" /></a></h1>
				</div>
				<div id="about">
					<ul>
						<li class="page_item page-item-5"><a href="../index.html%3Fp=5.html">Mission</a></li>
<li class="page_item page-item-6"><a href="../editorial-committee/index.html">Editorial Committee</a></li>
<li class="page_item page-item-8"><a href="../process/index.html">Process and Structure</a></li>
						<li><a href="http://code4lib.org/">Code4Lib</a></li>
					</ul>
				</div>
				<div class="mobile-search">
					<form method="get" id="searchform" action="../index.html">
						<div>
							<input type="text" value="" aria-labelledby="searchsubmit" name="s" id="s" />
							<input type="submit" value="Search" id="searchsubmit" />
						</div>
					</form>
				</div>
			</div>

			<div id="content">
								<div class="article" id="post-17186">
					<p id="issueDesignation"><a href="../issues/issues/issue56.html">Issue 56, 2023-04-21</a></p>
					<h1 class="articletitle">The viability of using an open source locally hosted AI for creating metadata in digital image collections</h1>
					<div class="abstract">
						<p>Artificial intelligence (AI) can support metadata creation for images by generating descriptions, titles, and keywords for digital collections in libraries. Many AI options are available, ranging from cloud-based corporate software solutions, including Microsoft Azure Custom Vision and Google Cloud Vision, to open-source locally hosted software packages. This case study examines the feasibility of deploying the open-source, locally hosted AI software, Sheeko, and the accuracy of the descriptions generated for images using two of the pre-trained models. The study aims to ascertain if Sheeko’s AI would be a viable solution for producing metadata in the form of descriptions, or titles for digital collections in Libraries and Cultural Resources at the University of Calgary.</p>
<p>by Ingrid Reiche</p>
					</div>
					<div class="entry">
						<h2><b>Introduction</b></h2>
<p><span style="font-weight: 400;">One application of Artificial intelligence (AI) in libraries is its potential to assist in metadata creation for images by generating descriptions, titles, or keywords for digital collections. Many AI options are available, ranging from cloud-based corporate software solutions, including Microsoft Azure Custom Vision and Computer Vision, and Google Cloud Vision, to open-source locally hosted software packages like OpenCV and Sheeko. This case study examines the feasibility of deploying the open-source, locally hosted AI software, Sheeko (</span><a href="https://sheeko.org/"><span style="font-weight: 400;">https://sheeko.org/</span></a><span style="font-weight: 400;">), and the accuracy of the descriptions generated for images using two of the pre-trained models. The study aims to ascertain if Sheeko’s AI would be a viable solution for producing metadata in the form of descriptions or titles for digital collections within the Libraries and Cultural Resources (LCR) at the University of Calgary. </span></p>
<h2><b>Setting</b></h2>
<p><span style="font-weight: 400;">As with many institutions, the University of Calgary’s Libraries and Cultural Resources has a large number of digital collections; many of which have insufficient descriptive metadata in the form of accurate titles and descriptions. Much of LCR’s digital collections are archival in nature, and have donor agreements that would require us to seek permission to process them through third-party AI services. During COVID-19, LCR was in the process of migrating our digital collections from CONTENTdm to a new Digital Assets Management (DAM) system by Orange Logic (</span><a href="https://www.orangelogic.com/products/digital-asset-management-system"><span style="font-weight: 400;">https://www.orangelogic.com/products/digital-asset-management-system</span></a><span style="font-weight: 400;">). While Orange Logic’s DAM offers third party AI services from Google and Microsoft for keywords generation, facial recognition, and flagging offensive content, but it does not offer image captioning as part of its AI services. As we looked to AI to help improve and potentially automate some aspects of metadata creation for our digital collections, project Sheeko offered a potential solution in the form of a local open-source software for producing captions that we hoped could be used to create descriptions or titles for images; particularly within some of our larger collections.  </span></p>
<h2><b>Potential Corporate Service Solutions for AI Images in Processing </b></h2>
<p><span style="font-weight: 400;">Before delving into the specifics of this case study of testing Sheeko’s locally hosted AI, it is useful to do a brief environmental scan of the solutions that were now available to the University of  Calgary, and weigh some of the strengths and weaknesses in relation to metadata creation. Microsoft’s Azure Custom Vision (</span><a href="https://learn.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/overview"><span style="font-weight: 400;">https://learn.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/overview</span></a><span style="font-weight: 400;">) “is an image recognition service that lets you build, deploy, and improve your own image identifier models” (Farley et al. 2022). There is one major difference between  Microsoft Computer Vision and Custom Vision services. Custom Vision gives users the ability to specify labels to be used on objects within images and train models to detect those objects, whereas Computer Vision offers users a fixed “ontology of more than 10,000 concepts and objects” that can be applied to images (Computer Vision . . . c2023). Computer Vision also offers a suite of other services including Optical Character Recognition (OCR), and Facial Recognition services that are also useful for the discovery of images in digital collections but not addressed in this article. While Azure’s Computer Vision (</span><a href="https://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/overview"><span style="font-weight: 400;">https://learn.microsoft.com/en-us/azure/cognitive-services/computer-vision/overview</span></a><span style="font-weight: 400;">) does offer a service for captioning as part of the Cognitive Services suite, this product is not included in Orange Logic third party services and would require further customization. Google Vision (</span><a href="https://cloud.google.com/vision"><span style="font-weight: 400;">https://cloud.google.com/vision</span></a><span style="font-weight: 400;">) allows users to “</span><span style="font-weight: 400;">assign labels to images and quickly classify </span><span style="font-weight: 400;">them into millions of predefined categories. Detect objects, read printed and handwritten text, and build valuable metadata into your image catalog” (Vision AI … n.d.). LCR does plan on testing </span><span style="font-weight: 400;">facial recognition technology, and the auto tagging of images with keywords in the future, but neither were useful on their own in deriving standard descriptions or titles we presently wanted. Both Microsoft and Google services require the images to be sent via an API to their services, then processed and the results returned, which takes time and processing offsite. Were this processing to be done on site with a local machine, image data could be processed at a higher volume with improved speed. Additionally, both Google’s and Microsoft’s services have a cost associated, though the costs for these services are not substantial enough to be a deterrent from using these services [<a href="../index.html%3Fp=17186.html#ref1" name="n1">1</a>][<a href="../index.html%3Fp=17186.html#ref2" name="n2">2</a>]. The biggest strength of these third party services offered via Orange Logic is their ease of use, their integration with the digital collection platform, and their ability to recognize discrete objects or people and then auto-tag images. However, neither of these services offered the more robust solution of generating multiple words used in captions that are needed for the description of images, or their titles. On the other hand, Sheeko did potentially offer a captioning solution.</span></p>
<p><b>Open-source local solution for image AI Sheeko  </b></p>
<p><span style="font-weight: 400;">Sheeko is a locally-hosted AI software environment developed by a team from the University of Utah’s J. Willard Marriott Library, consisting of Harish Maringanti (Associate Dean for IT &amp; Digital Library  Services), Dhanushka Samarakoon (Assistant Head, Software Development), and Bohan Zhum (Software Developer). Sheeko applies machine learning models to image collections to detect  objects, and generate captions. Sheeko is built on an open-source software environment whose components are listed or available for download via GitHub. Sheeko provides potential users three options of how to use the environment, as described in the “Training a Model” and “Sheeko Pretrained Models Resource” sections of the README in the GitHub repository (Samarakoon and Zhu 2021). Sheeko users can train their own model (using Inception v3 model), fine tune an existing model (using the im2txt model), or use one of seven pre-trained models available for download on their website (Maringanti et al. n.d). However, before embarking on this process, users must install the package either in VirtualBox or by downloading the code package and additional prerequisite software.  </span></p>
<p><span style="font-weight: 400;">The intent of the testing of Sheeko at the University of Calgary was to ascertain if training a model, or deploying a pre-trained model of Sheeko was a feasible answer to the need for description and title metadata required for image collections that were currently undescribed.  The test at the University of Calgary was carried out in 2020 during COVID-19, while most staff, myself included, were working from home. An initial attempt to install the VirtualBox was unsuccessful, and I asked our Technology Services staff for assistance. </span></p>
<p><span style="font-weight: 400;">Because the virtual box environment is supported using vagrant and requires that users enable BIOS mode, and this work was to be performed on a University managed laptop currently in my home, with remote assistance from Technology Services, the use of the Virtual Box was not an option. Therefore, a local install of the code package was the chosen method. To deploy Sheeko locally, there are specific hardware configurations required, and a dozen code package dependencies as listed in the Catalyst white paper and in the getting started section of their Github (Maringant et al. 2019; Samarakoon and Zhu 2021). Of the many system perquisites required there are specific GPU and driver installations, and a knowledge of Python scripting at more than an introductory level. These tasks took a number of weeks for Technology Services staff to work through. Once the software was installed we decided to test two pre-trained models on a set of 114 images. All of the pre-trained models available for download from project Sheeko use Tensorflow’s “Show and Tell” model as a base model which is refined to different specifications in each of the seven models available. Tensorflow is an “end-to-end open source platform for machine learning” (Tensorflow n.d.). The “Show and Tell model is a deep neural network that learns to describe the content of images” (Kim et al. 2020). This case study used two models:  the MS COCO model (ptm-im2txt-incv3-mscoco-3m) that was trained on images from  Microsoft Common Objects in Context; and Marriott model (ptm-im2txt-incv3-mlib-cleaned-3m) that was trained on historical photographs from the Marriott Library’s digital collection. The main goal for the testing was to see if either of these models would be suitable for any of a variety of collections ranging from historical to more contemporary photographs. In the vein of more scientific experiments, we wanted to keep as many variables as possible constant across the two models. Both the selected pre-trained models were previously trained with the Show and Tell model, with a starting point of Inception v3 and 3 million steps. The two major differences between the models were the images used for the training, and that the Marriott model went through natural language processing and cleaning to remove proper nouns.  </span></p>
<h2><b>Image Sample Selection</b></h2>
<p><span style="font-weight: 400;">To ascertain the strengths and weaknesses of the two Sheeko models selected, and to see which model performed best for images with certain qualities or objects, a cross section of 114 images from large digital collections that were lacking descriptive titles was selected. This included Images from the Glenbow Library and Archives (</span><a href="https://digitalcollections.ucalgary.ca/Package/2R340826N9XM"><span style="font-weight: 400;">https://digitalcollections.ucalgary.ca/Package/2R340826N9XM</span></a><span style="font-weight: 400;">), featuring largely historical images about Western Canada and Alberta’s provincial history and settler life, dating from the late 1800s onward. Images from our Mountain Studies collection (</span><a href="https://digitalcollections.ucalgary.ca/Package/2R340822NL8R"><span style="font-weight: 400;">https://digitalcollections.ucalgary.ca/Package/2R340822NL8R</span></a><span style="font-weight: 400;">), were of the national parks in the Canadian Rockies and featured mountains, landscapes, and wildlife. Images from the Calgary Stampede collection (</span><a href="https://digitalcollections.ucalgary.ca/Package/2R3BF1GBVDJ"><span style="font-weight: 400;">https://digitalcollections.ucalgary.ca/Package/2R3BF1GBVDJ</span></a><span style="font-weight: 400;">), were again Western themed but more contemporary and with a focus on horses, cowboys and fairgrounds, dating from the 1960s onward, though the collection itself contains images from 1908 onward. Images from EMI Music Canada Archive (</span><a href="https://digitalcollections.ucalgary.ca/Package/2R340822NOZP"><span style="font-weight: 400;">https://digitalcollections.ucalgary.ca/Package/2R340822NOZP</span></a><span style="font-weight: 400;">), were of music objects including tapes, records, CDs and other carrier cases. Images from the Nickle Galleries were of a few art pieces, and currently not available to the public. Images from the University of Calgary Photographs collection (</span><a href="https://digitalcollections.ucalgary.ca/Package/2R3BF1SQXF7JW"><span style="font-weight: 400;">https://digitalcollections.ucalgary.ca/Package/2R3BF1SQXF7JW</span></a><span style="font-weight: 400;">) featured university buildings from 1950s onward. The remaining images were taken from the Alberta Airphotos collection (</span><a href="https://digitalcollections.ucalgary.ca/Package/2R34082235HO"><span style="font-weight: 400;">https://digitalcollections.ucalgary.ca/Package/2R34082235HO</span></a><span style="font-weight: 400;">), the Arctic Institute of North America Photographic Archives collection (</span><a href="https://digitalcollections.ucalgary.ca/Package/2R3BF1SS8HHXX"><span style="font-weight: 400;">https://digitalcollections.ucalgary.ca/Package/2R3BF1SS8HHXX</span></a><span style="font-weight: 400;">) and the Winnipeg General Strike collection (</span><a href="https://digitalcollections.ucalgary.ca/Package/2R340822NUM1"><span style="font-weight: 400;">https://digitalcollections.ucalgary.ca/Package/2R340822NUM1</span></a><span style="font-weight: 400;">).</span></p>
<h2><b>Ranking system for Sheeko captions</b></h2>
<p><span style="font-weight: 400;">When Sheeko runs a model to generate captions for images, each model will output a json file that contains the all file names of the images processed, the three captions generated for each file name, and a confidence rating for each caption. A higher confidence rating did not necessarily correspond to the captions chosen as the best fit.  While the json output is useful, it does mean that users must view the files separately from the captions, which is not user friendly or helpful when one is trying to assess the best captions for an image. One of the Library Technology Support Analysts, Karynn Martin-Li, at Libraries and Cultural Resources had the brilliant idea of building a viewer that would display the image and the three captions from each model tested. The custom viewer made the process of selecting the appropriate caption from each model much more user friendly. This was custom built by Martin-Li and not included in Sheeko, but this created an interface and made it much easier to use. </span></p>
<pre class="brush: jscript; title: ; notranslate" title=""> 
{&quot;file_name&quot;: &quot;glenbowid21521.jpg&quot;, 
&quot;captions&quot;: &#x5B;{&quot;caption_text&quot;: &quot;a black and white cow standing in a field.&quot;, 
&quot;score&quot;: 0.0012332111498308891}, 
{&quot;caption_text&quot;: &quot;a black and white photo of a cow standing in the snow.&quot;, 
&quot;score&quot;: 0.0004844283302840979}, 
{&quot;caption_text&quot;: &quot;a black and white cow standing in a field&quot;, 
&quot;score&quot;: 0.00048442775066202847}]}, 
{&quot;file_name&quot;: &quot;glenbowid21745.jpg&quot;, 
&quot;captions&quot;: &#x5B;{&quot;caption_text&quot;: &quot;a black and white photo of a city street.&quot;, 
&quot;score&quot;: 0.0016210542383307202}, 
{&quot;caption_text&quot;: &quot;a black and white photo of a city street&quot;, 
&quot;score&quot;: 0.000650292656660278}, 
{&quot;caption_text&quot;: &quot;a black and white photo of a city bus&quot;, 
&quot;score&quot;: 0.0004501958654109137}]}, 
{&quot;file_name&quot;: &quot;glenbowid21841.jpg&quot;, 
&quot;captions&quot;: &#x5B;{&quot;caption_text&quot;: &quot;a black and white photo of a city street.&quot;, 
&quot;score&quot;: 0.001934344178677972}, 
{&quot;caption_text&quot;: &quot;a black and white photo of a city street&quot;, 
&quot;score&quot;: 0.0014626184147026346}, 
{&quot;caption_text&quot;: &quot;a black and white photo of a building&quot;, 
&quot;score&quot;: 0.0007475796404555153}]},
</pre>
<p><span style="font-weight: 400;">For all 114 images processed, one of the three captions for each image was selected as the best fit from both the MS COCO model and the Marriott model. The selected captions from each of these models were ranked on a scale of zero to five (zero being the lowest quality caption, and five being the highest quality caption). Ranking the captions was based on three variables: the identification of the subjects or objects present in the image (0-2 points), the background or setting of the image (0-2 points), and the image type (0-1 points). A total rank of three or above is something that would be considered usable as a base for a description. Examining a few captions from each of the models will illustrate how the rank system worked.</span></p>
<h2><b>Observations of Sheeko’s captions</b></h2>
<p><span style="font-weight: 400;">The photograph titled “’Canso’ at head of Coronation Fiord” in the University of Calgary Arctic Institute of North America collection was given rank of five for MS COCO model’s caption “a black and white photo of a plane in the water” (Fig.1). The MS COCO caption has appropriately described the type of photo as black and white, and the object of an airplane, and the background setting of water. The Marriott model’s caption for this image was assigned a rank of one for its caption “photo shows a helicopter being used like a crane to transport a tower during construction of a lift”. This caption accurately describes the type of image as a photo, but does not correctly identify the object, or background in the image.</span></p>
<p class="caption"><img decoding="async" src="../media/issue56/reiche/Fig1.jpg" /><br />
<strong>Figure 1.</strong></p>
<p><span style="font-weight: 400;">The image titled “Cattalo, male, named &#8216;King&#8217; in Buffalo National Park near Wainwright, Alberta.” in the Glenbow Library and Archives digital collection received a rank of 4 for the MS COCO model’s caption &#8220;a black and white photo of a cow standing in the snow&#8221; (Fig.2). This caption correctly identifies the type of photo as black and white, but description of the objects is only partially  accurate. The animal in this image is a cattalo, a cross between a cow and a buffalo, granted it was anticipated that no AI model has been trained on cattalos at this point. But nonetheless the animal does appear much like a buffalo, and so the caption is half correct when it categorizes the subject as a cow. The background is also only partially correct as the animal is standing in the snow, but there is no mention of a field, pasture, or trees present in the background. The Marriott caption for this image was “photo show a cow grazing” ranked at a score of 2, as the type of image being  described as photo is correct, but lacking the descriptors of black and white. The subject of a cow is half correct, and the animal is not grazing, and no background is mentioned. </span></p>
<p class="caption"><img decoding="async" src="../media/issue56/reiche/Fig2.jpg" /><br />
<strong>Figure 2.</strong></p>
<p><span style="font-weight: 400;">For the postcard titled “Dog Team in Heavy” by photographer Byron Harmon, in the Mountain Studies digital collection, both models produced captions with a rank of 1 (Fig.3). The MS COCO model generated the caption “picture of a vase of flowers on a table”, correctly identifying only the image type as a picture, and being incorrect about the dogs and man in the postcard, as well as the setting of a snow covered forest. The Marriott model produced the caption “artwork a person” and received a rank of 1, as the image is an artwork but the artwork is a photograph so there were no points given for the type of image. The model did correctly identify one of the subjects as a person, but does not mention the dogs, nor does it describe the background at all.  </span></p>
<p class="caption"><img decoding="async" src="../media/issue56/reiche/Fig3.jpg" /><br />
<strong>Figure 3.</strong></p>
<p><span style="font-weight: 400;">The overall results were that the MS COCO pre-trained model performed significantly better, with 53% of the images having a score of 3 or above. The Marriott Library pre-trained model had only 15% of the images with a score of 3 or above. A full breakdown of the results can be seen in Table 1.</span></p>
<div class="caption"><strong>Table 1.</strong></div>
<table>
<tbody>
<tr>
<th><strong>Rank</strong></th>
<th><strong>MS COCO count of rank </strong></th>
<th><strong>Marriott count of rank</strong></th>
</tr>
<tr>
<td>0</td>
<td>22 (19 %)</td>
<td>58 (50 %)</td>
</tr>
<tr>
<td>1</td>
<td>10 (9 %)</td>
<td>25 (21%)</td>
</tr>
<tr>
<td>2</td>
<td>21 (18 %)</td>
<td>16 (14 %)</td>
</tr>
<tr>
<td>3</td>
<td>29 (25 %)</td>
<td>13 (11 %)</td>
</tr>
<tr>
<td>4</td>
<td>24 (21 %)</td>
<td>1 (&lt; 1 %)</td>
</tr>
<tr>
<td>5</td>
<td>8 (7 %)</td>
<td>1 (&lt; 1 %)</td>
</tr>
<tr>
<td><strong>Total Images processed</strong></td>
<td>114</td>
<td>114</td>
</tr>
</tbody>
</table>
<div></div>
<div>
<p><span style="font-weight: 400;">Of the images processed, both models performed poorly on images from EMI Music Canada Archive, the Nickle Galleries, and the University of Calgary Photographs collections, which contain particular types of objects such as cassettes, records, artwork, and portraits. From these results it was determined that Sheeko would not be a good fit for images that require the identification of a particular location or images requiring individuals to be named. The models performed the best on images from Calgary Stampede collection and Glenbow collections where it was describing more general objects such as groups of people, buildings, animals, or landscapes. Were further testing to be done using Sheeko, the MS COCO model would be used as a basis for further training.  </span></p>
<p><span style="font-weight: 400;">In the white paper report written by Sheeko’s creators, “Machine learning meets library archives: Image analysis to generate descriptive metadata” they state MS COCO models and other existing models “were developed with a focus on born-digital photographs of everyday objects and scenarios (Maringanti et al. 2019). These models contained objects that were not present in historical photographs” (Maringanti et al. 2019). The misidentification of certain subjects or objects was supported in this case study. It was clear that both models were trained on certain types of object and subject categories, for example horses and cows, but not bison, buffalo or cattalo. When the models see a cattalo it will be described as a horse or a cow, and not a bison. Similarly, neither model produced useful captions for cassettes which the MS COCO model identified as a Nintendo Wii game controller in one case, and the Marriott model described as an artwork. To train a model to identify a particular object category not currently included in that model, such as a bison or a cassette, 10,000 images of that object need to be processed from various angles. Training the AI models does not require a large time investment in processing, but would require a significant time investment in the form collection analysis and image identification to identify the subject and object categories required for training the models, and gathering the images to train a model.</span></p>
<h2><b>Conclusion</b></h2>
<p><span style="font-weight: 400;">The goal of this case study was to test two of Sheeko’s pre-trained machine learning models to ascertain if they would produce captions that could be used as descriptions or titles, and thereby reduce the amount of time and labor required by staff to describe images in the University of Calgary’s digital collections. Sheeko’s results show that it does produce captions that could be used as a basis for descriptions, with moderate human intervention. The captions would not be suitable for title creation without significant human intervention. Were Sheeko to be used for description generation, it is unclear if the amount of time required to select the most appropriate caption from the three choices generated and then create additional information, or modify existing description with information from the Sheeko generated captions, would be less time than it would take for a person to create description metadata from scratch. At the end of this case study it was determined that Sheeko would not be pursued as a potential AI solution for creating descriptive metadata at this time. The stand alone results from the MS COCO model, though significantly better than those from the Marriott model, only yielded usable captions for 53% of the images. The potential time commitment and resources required from personnel to train models to identify the type of objects and subjects in LCR’s digital collections without descriptions, or with minimal descriptions, were disproportionate to the quality of the results in this case study. The fact that deploying Sheeko, and any further training of the models in Sheeko would have leaned heavily on the skills of staff in LCR’s IT, and required time to create models and source the images needed for modeling were all deterring factors in pursuing Sheeko. This case study was initially designed as a research project for myself, the Digital Metadata Librarian; and Technology Support were to enable this initiative where and when necessary. The reality was that this project was in many ways led by LCR’s IT, to whom I am most grateful. </span></p>
<p><span style="font-weight: 400;">This case study took place while our Digital Services Department was in the process of migrating from our old digital collections software CONTENTdm to our new Digital Asset Management system by Orange Logic. Now that we have completed the migration, and are more familiar with Orange Logic, it is time to test the capabilities of the built-in third-party AI services, and explore other captioning solutions. Sheeko presented a potential solution for describing a large amount of content locally, without the use of third-party providers, their services fees, or the processing time  associated with them. Sheeko and local open-source environments also eliminate the need to pursue seeking permissions and re-writing donor agreements as the images are not processed outside of the home institution, also mitigating any potential data sovereignty issues. A local software solution similar to Sheeko may still be useful in filling a gap in AI image description present in our new digital asset management system. The cost of Sheeko was not a monetary services fee, but rather the cost of wages and valued staff time of our Technology Services staff to do the manual set-up of  the software environment needed to run Sheeko, and build additional customization in the form of a viewer to facilitate ease of use. Ultimately Sheeko’s cost was great enough that it has been  eliminated as a potential solution for large-scale image captioning at present.</span></p>
<h2><b>Notes</b></h2>
<p><span style="font-weight: 400;">[<a href="../index.html%3Fp=17186.html#n1" name="ref1">1</a>] The price list for Azure cognitive services, including Computer Vision is available at: </span><a href="https://azure.microsoft.com/en-us/pricing/details/cognitive-services/"><span style="font-weight: 400;">https://azure.microsoft.com/en-us/pricing/details/cognitive-services/</span></a><span style="font-weight: 400;">. The Describe service is listed at $1.50/1,000 transactions when processing 0-1 million transactions. After 1 million transactions it is $0.60/1,000 transactions. If Libraries and Cultural Resources were to process all 2,241,793 image in our collection it would cost $2,245.</span></p>
<p><span style="font-weight: 400;">[<a href="../index.html%3Fp=17186.html#n1" name="ref2">2</a>] The price list for Google Vision services is available at: </span><a href="https://cloud.google.com/vision/pricing/"><span style="font-weight: 400;">https://cloud.google.com/vision/pricing/</span></a><span style="font-weight: 400;"> The price for the majority of services, with the exception of Web Detection and Object Localization is $1.50/1,000 transactions when processing 0-5 million transactions.  If Libraries and Cultural Resources were to process all 2,241,793 images in our collection, using any one of Google Vision’s services it would cost $3,362. However, Google Vision does not offer captioning.</span></p>
<h2><strong>About the Author</strong></h2>
<p><span style="font-weight: 400;">Ingrid Reiche is the Digital Metadata Librarian with Libraries and Cultural Resources at the University of Calgary. Ingrid works largely with Digital Collections and is hopeful that AI can supplement human metadata creation to help describe the growing digital content in academic libraries.</span></p>
<h2><b>Bibliography</b></h2>
<p><span style="font-weight: 400;">Maringanti H, Samarakoon D, Zhu B. (2019). Machine learning meets library archives: Image </span><span style="font-weight: 400;">analysis to generate descriptive metadata. LYRASIS Research Publications. </span><a href="https://doi.org/10.48609/pt6w-p810"><span style="font-weight: 400;">https://doi.org/10.48609/pt6w-p810</span></a><span style="font-weight: 400;"> </span></p>
<p><span style="font-weight: 400;">Maringanti H, Samarakoon D, Zhu B. n.d. Sheeko: A computational helper [Internet]. Atlanta (GA): Lyrasis; [cited 2023 Feb 17]. Available from: </span><a href="https://sheeko.org/"><span style="font-weight: 400;">https://sheeko.org/</span></a><span style="font-weight: 400;">   </span></p>
<p><span style="font-weight: 400;">Samarakoon D, Zhu B. [updated 2021 Feb 17]. sheeko-vagrant</span><span style="font-weight: 400;">. San Francisco (CA): GitHub; [cited 2023 Feb 17]. Available from: </span><a href="https://github.com/marriott-library/sheeko-vagrant"><span style="font-weight: 400;">https://github.com/marriott-library/sheeko-vagrant</span></a></p>
<p><span style="font-weight: 400;">Farley P,  Buck A, MacGregor H, McSharry C, Downer R, Coulter D, contributors. 2022 Nov 22. What is Custom Vision? [Internet]. Redmond (WA): Microsoft; [cited 2023 Feb 17]. Available from: </span><a href="https://learn.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/overview"><span style="font-weight: 400;">https://learn.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/overview</span></a><span style="font-weight: 400;"> </span></p>
<p><span style="font-weight: 400;">Computer Vision [Internet]. c2023. Redmond (WA): Microsoft; [cited 2023 Feb 17]. Available from: </span><a href="https://azure.microsoft.com/en-us/products/cognitive-services/computer-vision"><span style="font-weight: 400;">https://azure.microsoft.com/en-us/products/cognitive-services/computer-vision</span></a></p>
<p><span style="font-weight: 400;">Vision AI [Internet]. n.d. Mountain View (CA): Google; [cited 2023 Feb 17]. Available from: </span><a href="https://cloud.google.com/vision#section-3catalog"><span style="font-weight: 400;">https://cloud.google.com/vision#section-3catalog</span></a></p>
<p><span style="font-weight: 400;">Tensorflow [Internet]. n.d. [cited 2023 Feb 17]. Available from: </span><a href="https://www.tensorflow.org/overview"><span style="font-weight: 400;">https://www.tensorflow.org/overview</span></a></p>
<p><span style="font-weight: 400;">Kim J, Murdopo A, Friedman J, Wu N, contributors. [updated 2020, Apr 12]. Show and Tell: A Neural Image Caption Generator</span><span style="font-weight: 400;">. San Francisco (CA): GitHub; [cited 2023 Feb 17]. Available from: </span><a href="https://github.com/tensorflow/models/blob/archive/research/im2txt/README.md"><span style="font-weight: 400;">https://github.com/tensorflow/models/blob/archive/research/im2txt/README.md </span></a></p>
</div>
					</div>
														</div>
				<!-- You can start editing here. -->

<div class="comments">
	<p class="subscriptionlinks">Subscribe to comments: <a href="17186/feed">For this article</a> | <a href="http://feeds.feedburner.com/c4lj/comments">For all articles</a></p>

			<!-- If comments are open, but there are no comments. -->

	 

<h3 id="respond">Leave a Reply</h3>


<form action="https://journal.code4lib.org/wp-comments-post.php" method="post" id="commentform">


<p><input type="text" name="author" id="author" value=""/>
<label for="author">Name (required)</label></p>

<p><input type="text" name="email" id="email" value="" />
<label for="email">Mail (will not be published) (required)</label></p>

<p><input type="text" name="url" id="url" value="" />
<label for="url">Website</label></p>


<p><textarea autocomplete="new-password"  aria-label="Comment box" id="f127f6ccbe"  name="f127f6ccbe"   cols="50" rows="10"></textarea><textarea id="comment" aria-label="hp-comment" aria-hidden="true" name="comment" autocomplete="new-password" style="padding:0 !important;clip:rect(1px, 1px, 1px, 1px) !important;position:absolute !important;white-space:nowrap !important;height:1px !important;width:1px !important;overflow:hidden !important;" tabindex="-1"></textarea><script data-noptimize>document.getElementById("comment").setAttribute( "id", "acdf45d2eacb4a0e679ce0348247ad6f" );document.getElementById("f127f6ccbe").setAttribute( "id", "comment" );</script></p>

<p><input name="submit" type="submit" id="submit"  value="Submit Comment" />
<input type="hidden" name="comment_post_ID" value="17186" />
</p>
<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="753ce143ee" /></p><p style="display: none !important;" class="akismet-fields-container" data-prefix="ak_"><label>&#916;<textarea name="ak_hp_textarea" cols="45" rows="8" maxlength="100"></textarea></label><input type="hidden" id="ak_js_1" name="ak_js" value="140"/><script>document.getElementById( "ak_js_1" ).setAttribute( "value", ( new Date() ).getTime() );</script></p>
</form>


</div>
							</div>

			<div id="meta">
				<div id="issn">
					<p>ISSN 1940-5758</p>
				</div>
				<div class="search-sidebar">
				<form method="get" id="searchform" action="../index.html">
					<div>
						<input type="text" value="" aria-labelledby="searchsubmit" name="s" id="s" />
						<input type="submit" value="Search" id="searchsubmit"/>
					</div>
				</form>
				</div>
				<div id="archives">
					<h2>Current Issue</h2>
						<ul>
							<li><a href="../issues/issues/issue60.html">Issue 60, 2025-04-14</a></li>
						</ul>

					<h2>Previous Issues</h2>
						<ul>
              <li><a href="../issues/issues/issue59.html">Issue 59, 2024-10-07</a></li><li><a href="../issues/issues/issue58.html">Issue 58, 2023-12-04</a></li><li><a href="../issues/issues/issue57.html">Issue 57, 2023-08-29</a></li><li><a href="../issues/issues/issue56.html">Issue 56, 2023-04-21</a></li>              <li><a href="../index.html%3Fp=2476.html">Older Issues</a></li>
						</ul>
				</div>
				<div id="forauthors">
					<h2>For Authors</h2>
					<ul>
						<li class="page_item page-item-4"><a href="../index.html%3Fp=4.html">Call for Submissions</a></li>
<li class="page_item page-item-7"><a href="../index.html%3Fp=7.html">Article Guidelines</a></li>
					</ul>
				</div>
			</div>
						<div id="footer">
				<p id="login"><a href="../wp-login.php.html">Log in</a></p>
				<p id="copyright">This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/3.0/us/">Creative Commons Attribution 3.0 United States License</a>.<br /><a rel="license" href="http://creativecommons.org/licenses/by/3.0/us/"><img alt="Creative Commons License" src="http://i.creativecommons.org/l/by/3.0/us/80x15.png" /></a></p>
			</div>
			<script type="speculationrules">
{"prefetch":[{"source":"document","where":{"and":[{"href_matches":"\/*"},{"not":{"href_matches":["\/wp-*.php","\/wp-admin\/*","\/wp-content\/uploads\/*","\/wp-content\/*","\/wp-content\/plugins\/*","\/wp-content\/themes\/c4lj-theme\/*","\/*\\?(.+)"]}},{"not":{"selector_matches":"a[rel~=\"nofollow\"]"}},{"not":{"selector_matches":".no-prefetch, .no-prefetch a"}}]},"eagerness":"conservative"}]}
</script>
<script type="text/javascript" src="../wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shCore.js%3Fver=3.0.9b" id="syntaxhighlighter-core-js"></script>
<script type="text/javascript" src="../wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/scripts/shBrushJScript.js%3Fver=3.0.9b" id="syntaxhighlighter-brush-jscript-js"></script>
<script type='text/javascript'>
	(function(){
		var corecss = document.createElement('link');
		var themecss = document.createElement('link');
		var corecssurl = "https://journal.code4lib.org/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shCore.css?ver=3.0.9b";
		if ( corecss.setAttribute ) {
				corecss.setAttribute( "rel", "stylesheet" );
				corecss.setAttribute( "type", "text/css" );
				corecss.setAttribute( "href", corecssurl );
		} else {
				corecss.rel = "stylesheet";
				corecss.href = corecssurl;
		}
		document.head.appendChild( corecss );
		var themecssurl = "https://journal.code4lib.org/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shThemeDefault.css?ver=3.0.9b";
		if ( themecss.setAttribute ) {
				themecss.setAttribute( "rel", "stylesheet" );
				themecss.setAttribute( "type", "text/css" );
				themecss.setAttribute( "href", themecssurl );
		} else {
				themecss.rel = "stylesheet";
				themecss.href = themecssurl;
		}
		document.head.appendChild( themecss );
	})();
	SyntaxHighlighter.config.strings.expandSource = '+ expand source';
	SyntaxHighlighter.config.strings.help = '?';
	SyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\n\n';
	SyntaxHighlighter.config.strings.noBrush = 'Can\'t find brush for: ';
	SyntaxHighlighter.config.strings.brushNotHtmlScript = 'Brush wasn\'t configured for html-script option: ';
	SyntaxHighlighter.defaults['pad-line-numbers'] = false;
	SyntaxHighlighter.defaults['toolbar'] = false;
	SyntaxHighlighter.all();

	// Infinite scroll support
	if ( typeof( jQuery ) !== 'undefined' ) {
		jQuery( function( $ ) {
			$( document.body ).on( 'post-load', function() {
				SyntaxHighlighter.highlight();
			} );
		} );
	}
</script>
<script defer type="text/javascript" src="../wp-content/plugins/akismet/_inc/akismet-frontend.js%3Fver=1748382734" id="akismet-frontend-js"></script>
		</div>
	</body>
</html>
