<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

		<title>The Code4Lib Journal &#8211; Distant Listening: Using Python and Apps Scripts to Text Mine and Tag Oral History Collections</title>

		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta name="generator" content="WordPress 6.8.1" /> <!-- leave this for stats -->
    <link rel="shortcut icon" href="../wp-content/themes/c4lj-theme/images/favicon.ico" />
		<link rel="stylesheet" href="../wp-content/themes/c4lj-theme/style.css" type="text/css" media="screen, print" />
		<!--[if lte IE 7]>
		<link rel="stylesheet" href="https://journal.code4lib.org/wp-content/themes/c4lj-theme/fix-ie7.css" type="text/css" media="screen" />
		<![endif]-->
		<!--[if lte IE 6]>
		<link rel="stylesheet" href="https://journal.code4lib.org/wp-content/themes/c4lj-theme/fix-ie6.css" type="text/css" media="screen" />
		<![endif]-->
		<link rel="stylesheet" href="../wp-content/themes/c4lj-theme/print.css" type="text/css" media="print" />
		<link rel="alternate" type="application/rss+xml" title="The Code4Lib Journal Syndication Feed" href="http://feeds.feedburner.com/c4lj" />
		<link rel="pingback" href="https://journal.code4lib.org/xmlrpc.php" />

<!-- Google Scholar Stuff -->
	<meta name="citation_title" content="Distant Listening: Using Python and Apps Scripts to Text Mine and Tag Oral History Collections">
 <meta name="citation_author" content="Andrew Weymouth">
<meta name="citation_publication_date" content="2025/04/14">
	<meta name="citation_journal_title" content="Code4Lib Journal">
		<meta name="citation_issue" content="60">
<!-- end  Google Scholar Stuff -->

<meta name='robots' content='max-image-preview:large' />
	<style>img:is([sizes="auto" i], [sizes^="auto," i]) { contain-intrinsic-size: 3000px 1500px }</style>
	<link rel="alternate" type="application/rss+xml" title="The Code4Lib Journal &raquo; Distant Listening: Using Python and Apps Scripts to Text Mine and Tag Oral History Collections Comments Feed" href="18286/feed" />
<script type="text/javascript">
/* <![CDATA[ */
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.1.0\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.1.0\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/journal.code4lib.org\/wp-includes\/js\/wp-emoji-release.min.js?ver=6.8.1"}};
/*! This file is auto-generated */
!function(i,n){var o,s,e;function c(e){try{var t={supportTests:e,timestamp:(new Date).valueOf()};sessionStorage.setItem(o,JSON.stringify(t))}catch(e){}}function p(e,t,n){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);var t=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data),r=(e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(n,0,0),new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data));return t.every(function(e,t){return e===r[t]})}function u(e,t,n){switch(t){case"flag":return n(e,"\ud83c\udff3\ufe0f\u200d\u26a7\ufe0f","\ud83c\udff3\ufe0f\u200b\u26a7\ufe0f")?!1:!n(e,"\ud83c\uddfa\ud83c\uddf3","\ud83c\uddfa\u200b\ud83c\uddf3")&&!n(e,"\ud83c\udff4\udb40\udc67\udb40\udc62\udb40\udc65\udb40\udc6e\udb40\udc67\udb40\udc7f","\ud83c\udff4\u200b\udb40\udc67\u200b\udb40\udc62\u200b\udb40\udc65\u200b\udb40\udc6e\u200b\udb40\udc67\u200b\udb40\udc7f");case"emoji":return!n(e,"\ud83d\udc26\u200d\ud83d\udd25","\ud83d\udc26\u200b\ud83d\udd25")}return!1}function f(e,t,n){var r="undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?new OffscreenCanvas(300,150):i.createElement("canvas"),a=r.getContext("2d",{willReadFrequently:!0}),o=(a.textBaseline="top",a.font="600 32px Arial",{});return e.forEach(function(e){o[e]=t(a,e,n)}),o}function t(e){var t=i.createElement("script");t.src=e,t.defer=!0,i.head.appendChild(t)}"undefined"!=typeof Promise&&(o="wpEmojiSettingsSupports",s=["flag","emoji"],n.supports={everything:!0,everythingExceptFlag:!0},e=new Promise(function(e){i.addEventListener("DOMContentLoaded",e,{once:!0})}),new Promise(function(t){var n=function(){try{var e=JSON.parse(sessionStorage.getItem(o));if("object"==typeof e&&"number"==typeof e.timestamp&&(new Date).valueOf()<e.timestamp+604800&&"object"==typeof e.supportTests)return e.supportTests}catch(e){}return null}();if(!n){if("undefined"!=typeof Worker&&"undefined"!=typeof OffscreenCanvas&&"undefined"!=typeof URL&&URL.createObjectURL&&"undefined"!=typeof Blob)try{var e="postMessage("+f.toString()+"("+[JSON.stringify(s),u.toString(),p.toString()].join(",")+"));",r=new Blob([e],{type:"text/javascript"}),a=new Worker(URL.createObjectURL(r),{name:"wpTestEmojiSupports"});return void(a.onmessage=function(e){c(n=e.data),a.terminate(),t(n)})}catch(e){}c(n=f(s,u,p))}t(n)}).then(function(e){for(var t in e)n.supports[t]=e[t],n.supports.everything=n.supports.everything&&n.supports[t],"flag"!==t&&(n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&n.supports[t]);n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&!n.supports.flag,n.DOMReady=!1,n.readyCallback=function(){n.DOMReady=!0}}).then(function(){return e}).then(function(){var e;n.supports.everything||(n.readyCallback(),(e=n.source||{}).concatemoji?t(e.concatemoji):e.wpemoji&&e.twemoji&&(t(e.twemoji),t(e.wpemoji)))}))}((window,document),window._wpemojiSettings);
/* ]]> */
</script>
<style id='wp-emoji-styles-inline-css' type='text/css'>

	img.wp-smiley, img.emoji {
		display: inline !important;
		border: none !important;
		box-shadow: none !important;
		height: 1em !important;
		width: 1em !important;
		margin: 0 0.07em !important;
		vertical-align: -0.1em !important;
		background: none !important;
		padding: 0 !important;
	}
</style>
<link rel='stylesheet' id='wp-block-library-css' href='../wp-includes/css/dist/block-library/style.min.css%3Fver=6.8.1.css' type='text/css' media='all' />
<style id='classic-theme-styles-inline-css' type='text/css'>
/*! This file is auto-generated */
.wp-block-button__link{color:#fff;background-color:#32373c;border-radius:9999px;box-shadow:none;text-decoration:none;padding:calc(.667em + 2px) calc(1.333em + 2px);font-size:1.125em}.wp-block-file__button{background:#32373c;color:#fff;text-decoration:none}
</style>
<style id='global-styles-inline-css' type='text/css'>
:root{--wp--preset--aspect-ratio--square: 1;--wp--preset--aspect-ratio--4-3: 4/3;--wp--preset--aspect-ratio--3-4: 3/4;--wp--preset--aspect-ratio--3-2: 3/2;--wp--preset--aspect-ratio--2-3: 2/3;--wp--preset--aspect-ratio--16-9: 16/9;--wp--preset--aspect-ratio--9-16: 9/16;--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--x-large: 42px;--wp--preset--spacing--20: 0.44rem;--wp--preset--spacing--30: 0.67rem;--wp--preset--spacing--40: 1rem;--wp--preset--spacing--50: 1.5rem;--wp--preset--spacing--60: 2.25rem;--wp--preset--spacing--70: 3.38rem;--wp--preset--spacing--80: 5.06rem;--wp--preset--shadow--natural: 6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep: 12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp: 6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined: 6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp: 6px 6px 0px rgba(0, 0, 0, 1);}:where(.is-layout-flex){gap: 0.5em;}:where(.is-layout-grid){gap: 0.5em;}body .is-layout-flex{display: flex;}.is-layout-flex{flex-wrap: wrap;align-items: center;}.is-layout-flex > :is(*, div){margin: 0;}body .is-layout-grid{display: grid;}.is-layout-grid > :is(*, div){margin: 0;}:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}
:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}
:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}
:root :where(.wp-block-pullquote){font-size: 1.5em;line-height: 1.6;}
</style>
<style id='akismet-widget-style-inline-css' type='text/css'>

			.a-stats {
				--akismet-color-mid-green: #357b49;
				--akismet-color-white: #fff;
				--akismet-color-light-grey: #f6f7f7;

				max-width: 350px;
				width: auto;
			}

			.a-stats * {
				all: unset;
				box-sizing: border-box;
			}

			.a-stats strong {
				font-weight: 600;
			}

			.a-stats a.a-stats__link,
			.a-stats a.a-stats__link:visited,
			.a-stats a.a-stats__link:active {
				background: var(--akismet-color-mid-green);
				border: none;
				box-shadow: none;
				border-radius: 8px;
				color: var(--akismet-color-white);
				cursor: pointer;
				display: block;
				font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen-Sans', 'Ubuntu', 'Cantarell', 'Helvetica Neue', sans-serif;
				font-weight: 500;
				padding: 12px;
				text-align: center;
				text-decoration: none;
				transition: all 0.2s ease;
			}

			/* Extra specificity to deal with TwentyTwentyOne focus style */
			.widget .a-stats a.a-stats__link:focus {
				background: var(--akismet-color-mid-green);
				color: var(--akismet-color-white);
				text-decoration: none;
			}

			.a-stats a.a-stats__link:hover {
				filter: brightness(110%);
				box-shadow: 0 4px 12px rgba(0, 0, 0, 0.06), 0 0 2px rgba(0, 0, 0, 0.16);
			}

			.a-stats .count {
				color: var(--akismet-color-white);
				display: block;
				font-size: 1.5em;
				line-height: 1.4;
				padding: 0 13px;
				white-space: nowrap;
			}
		
</style>
<link rel="https://api.w.org/" href="../wp-json/index.html" /><link rel="alternate" title="JSON" type="application/json" href="../wp-json/wp/v2/posts/18286" /><link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://journal.code4lib.org/xmlrpc.php?rsd" />
<link rel="canonical" href="../index.html%3Fp=18286.html" />
<link rel='shortlink' href='../index.html%3Fp=18286.html' />
<link rel="alternate" title="oEmbed (JSON)" type="application/json+oembed" href="../wp-json/oembed/1.0/embed%3Furl=https:%252F%252Fjournal.code4lib.org%252Farticles%252F18286" />
<link rel="alternate" title="oEmbed (XML)" type="text/xml+oembed" href="../wp-json/oembed/1.0/embed%3Furl=https:%252F%252Fjournal.code4lib.org%252Farticles%252F18286&amp;format=xml" />
<style>
@media all and (max-width : 768px) {
.syntaxhighlighter a, .syntaxhighlighter div, .syntaxhighlighter code, .syntaxhighlighter table, .syntaxhighlighter table td, .syntaxhighlighter table tr, .syntaxhighlighter table tbody, .syntaxhighlighter table thead, .syntaxhighlighter table caption, .syntaxhighlighter textarea
{
	font-size: 0.95em !important;
}
}
</style>
	</head>
	<body>
		<div id="page">
			<div id="header">
				<div id="headerbackground">
					<h1><a href="../index.html"><img src="../wp-content/themes/c4lj-theme/images/logo.png" alt="The Code4Lib Journal" /></a></h1>
				</div>
				<div id="about">
					<ul>
						<li class="page_item page-item-5"><a href="../index.html%3Fp=5.html">Mission</a></li>
<li class="page_item page-item-6"><a href="../editorial-committee/index.html">Editorial Committee</a></li>
<li class="page_item page-item-8"><a href="../process/index.html">Process and Structure</a></li>
						<li><a href="http://code4lib.org/">Code4Lib</a></li>
					</ul>
				</div>
				<div class="mobile-search">
					<form method="get" id="searchform" action="../index.html">
						<div>
							<input type="text" value="" aria-labelledby="searchsubmit" name="s" id="s" />
							<input type="submit" value="Search" id="searchsubmit" />
						</div>
					</form>
				</div>
			</div>

			<div id="content">
								<div class="article" id="post-18286">
					<p id="issueDesignation"><a href="../issues/issues/issue60.html">Issue 60, 2025-04-14</a></p>
					<h1 class="articletitle">Distant Listening: Using Python and Apps Scripts to Text Mine and Tag Oral History Collections</h1>
					<div class="abstract">
						<p>This article presents a case study for creating subject tags utilizing transcription data across entire oral history collections, adapting Franco Moretti’s distant reading approach to narrative audio material. Designed for oral history project managers, the workflow empowers student workers to generate, modify, and expand subject tags during transcription editing, thereby enhancing the overall accuracy and discoverability of the collection. The paper details the workflow, surveys challenges the process addresses, shares experiences of transcribers, and examines the limitations of data-driven, human-edited tagging.</p>
					</div>
					<div class="entry">
						<p>By Andrew Weymouth</p>
<h2>Background</h2>
<p>The University of Idaho’s Digital Scholarship and Open Strategies (DSOS) department was established in 2008 to digitize the International Jazz Collection and has since expanded to over 130 digital collections.[<a id="ref1" href="../index.html%3Fp=18286.html#note1">1</a>] These are constructed with CollectionBuilder, an “open source framework for creating digital collections and exhibit websites that are driven by metadata and modern static web technology”[<a id="ref2" href="../index.html%3Fp=18286.html#note2">2</a>]. A companion framework named Oral History as Data (OHD) was developed in 2016 to visualize encoded transcriptions and allow researchers to explore oral history recordings by keywords and tags. In this paper, “tagging” refers to a custom set of subject designations that can be tailored by the transcriber depending on the recording’s content and themes.</p>
<p class="caption"><img decoding="async" src="../media/issue60/weymouth/cb_interface.gif" /><br />
<strong>Figure 1.</strong> CollectionBuilder browse site and CollectionBuilder template interface</p>
<p>Our physical workspace at the library is the Center for Digital Inquiry and Learning (CDIL), where our Digital Labs Manager, Digital Project Manager and I support the labor of a small group of student workers and fellowship recipients, generally around 2-5 a semester. Both the CollectionBuilder and OHD frameworks have been designed to be simple and accessible, only requiring someone with access to Google Sheets, Visual Studio Code and minimal software installation to create, maintain, and export digital collections.</p>
<p>For the process outlined in this paper, student transcribers only need access to Google Sheets to generate and edit subject tags using Google’s Apps Script extension, while the project manager needs access to Adobe Premiere and a text editor to run the Python Script to generate transcriptions, text mine the material, and to create subject tags.</p>
<p>The incentive for this project arose from realizing a number of oral history recordings were either untranscribed, partially transcribed or lacking in accuracy following a data migration of our digital collections away from ContentDM in the winter of 2023. Because of the volume of text that needed updating, it was worthwhile to rethink workflows for overall efficiency and accuracy.</p>
<h2>Distant Listening</h2>
<p>One element I wanted to focus on was the creation of subject tagging to enrich the transcripts. In addition to keyword searching, the OHD interface allows custom subject tags to be highlighted and visualized across the duration of recordings. New York University’s Weatherly A. Stephan details the importance of subject tagging oral history collections, noting how “transcription alone cannot address the perennial gap of supporting serendipitous discovery through subject-based inquiry rather than simply known-item searching.”[<a id="ref3" href="../index.html%3Fp=18286.html#note3">3</a>]</p>
<p>Previous transcription practices had student workers and CDIL Fellowship recipients adding subject tags to automated transcripts as they were copy editing dialogue, using the recordings as a reference. Transcribers would tag individual recordings of collections that ranged from 20 to over 100 recordings, making their best judgments to what subjects seemed like they might be indicative of the overall collection. This approach, which I am calling linear listening, may mislead transcribers by establishing repeating themes that do not occur across the collection or missing themes that only begin to appear in later recordings.</p>
<p class="caption"><img decoding="async" src="../media/issue60/weymouth/linear_listening.png" /><br />
<strong>Figure 2.</strong> Challenges of Linear Listening Visualization</p>
<p>Our physical workspace at the library is the Center for Digital Inquiry and Learning (CDIL), where our Digital Labs Manager, Digital Project Manager and I support the labor of a small group of student workers and fellowship recipients, generally around 2-5 a semester. Both the CollectionBuilder and OHD frameworks have been designed to be simple and accessible, only requiring someone with access to Google Sheets, Visual Studio Code and minimal software installation to create, maintain, and export digital collections.</p>
<p>Distant listening is an alternate approach that mines combined transcripts and generates tags before the transcriber begins the copy editing process. This moniker is an adaptation of Franco Moretti’s concept of distant listening, “where distance[&#8230;] allows you to focus on units that are much smaller or much larger than the text: devices, themes, tropes—or genres and systems” [<a id="ref4" href="../index.html%3Fp=18286.html#note4">4</a>]. By searching across collections for terms and phrases indicating subject matter, oral history project managers can produce richer, more accurate data that increases the discoverability of recordings and makes the transcription process more dynamic and pedagogically rewarding for transcribers.</p>
<p>This case study details my experience over the winter and spring of 2024 developing the tools independently and testing the process with two student transcribers working through 30 transcriptions, incorporating their feedback and streamlining processes. I also had a chance to iterate over this process in January 2025, this time working with the CDIL Digital Projects Manager to transcribe, tag and copy edit 100 more recordings for digital preservation.</p>
<h3>Challenges</h3>
<p>The time-intensive nature of transcription, subject tagging and web hosting has made oral history recordings an undervalued format in digital initiatives. As Doug Boyd, director of the Louie B. Nunn Center for Oral History at the University of Kentucky recounts in his chapter Oral History Archives, Orality and Usability (2015):</p>
<blockquote><p>“From the archival perspective, oral history proved an exciting and enticing resource to acquire. However, the difﬁculties posed by time-intensive and ﬁnancially draining realities of processing oral history collections resulted in an analog crisis in the late 1990s. Hundreds of oral history archives around the United States claimed large collections, but the overwhelming majority of these collections containing thousands of interviews remained unprocessed, analog, inaccessible, and unused” [<a id="ref5" href="../index.html%3Fp=18286.html#note5">5</a>].</p></blockquote>
<p>In <em>Quantifying the Need: A Survey of Existing Sound Recordings in Collections in the United States</em> (2015), created in collaboration with the Northeast Document Conservation Center, the authors highlight the scale of archival audio preservation challenges. They conclude that only 17% of audio holdings in U.S. collections have been digitized. The survey estimates that over 250 million preservation-worthy items remain undigitized. Of these, more than 80 million (32%) will require a specialized audio preservation workflow. The report goes on to detail how the National Recordings Preservation Plans states that many of these analog recordings must be digitized between 2027 and 2033 before material degradation [<a id="ref6" href="../index.html%3Fp=18286.html#note6">6</a>].</p>
<p>In contrast to the digitization of archival photographs or documents, meeting accessibility standards for oral history recordings involves not only transcribing recordings but also presenting them in an intuitive, keyword navigable digital interface. OHD developer Devin Becker’s solution displays the audio at the top of the page, followed by a visualization of the recording with color valued tags, a key to the tags, a search bar for keyword queries and the transcription below. This allows researchers to follow along with the time stamped transcript as the audio plays.</p>
<p class="caption"><img decoding="async" src="../media/issue60/weymouth/dtt_02.gif" /><br />
<strong>Figure 3.</strong> Demonstration of Oral History as Data transcription/recording interface, keyword search functionality and tag visualization showing ability to read through audio material both chronologically and vertically</p>
<p>Despite this advancement in the audio player interface, the initial transcription and tagging remained a significant hurdle in developing these collections. While machine learning speech to text technology has improved considerably since the development of the OHD platform in 2016, early, no cost transcription services were often so poor that they required extensive manual correction. Fully human-driven transcription and tagging has its own challenges: it is tedious, slow moving work that, without close supervision, can result in uncontrolled vocabulary, knowledge gaps, and bias from linear listening.</p>
<h1>Process</h1>
<p class="caption"><img decoding="async" src="../media/issue60/weymouth/overall_process.png" /><br />
<strong>Figure 4.</strong> visualization of workflow from audio files to CSV to Python text mining and Google Sheets</p>
<h2>Overview</h2>
<p>To summarize the process:</p>
<ol>
<li>Transcription: audio is transcribed into Comma Separated Values (CSV) files using Adobe Premiere’s Speech to Text tool</li>
<li>CSVs are made into individual Google Sheets, exported using an Google’s Apps Script extension and added to a folder in the Python Transcription Mining Tool</li>
<li>On running the Python script, these items are combined and searched for all associated terms and phrases built into the different subject tag categories</li>
<li>The tool generates a tally of these terms and phrases, which is used to create the Primary Tag Sheet in another Google Sheet</li>
<li>Using an Apps Script function, all individual transcripts are linked to the Primary Tag Sheet so each transcript’s tag column is automatically generated</li>
<li>New subject categories or associated terms can be added or removed from the Primary Tag Sheet and these changes can be implemented across all individual transcripts by simply re-running the code</li>
</ol>
<h2>Transcription</h2>
<p>Moving away from services the department had been working with, I tested Adobe Premiere’s Speech to Text tool and found it uniquely well-suited for the OHD framework, with advantages including:</p>
<ul>
<li>Powered by Adobe Sensei, machine learning dramatically increased accuracy in differentiating speakers and transcribing dialogue, even with obscure, regional proper nouns.</li>
<li>Significantly faster transcription speed, from one 1.5-hour recording every two to three business days up to twenty 1.5-hour recordings in one day.</li>
<li>Costs covered by our university-wide Adobe subscription.</li>
<li>Direct export to CSV UTF-8 (avoiding conversion errors necessary for OHD)</li>
<li>Available non-English language packs, enabling the creation of the department’s first Spanish and French language oral history collections.</li>
<li>Privacy standards with Premiere’s General Data Protection Regulation compliance, ensuring all transcription material is stored locally and not uploaded to the cloud.[<a id="ref7" href="../index.html%3Fp=18286.html#note7">7</a>].</li>
</ul>
<p class="caption"><img decoding="async" src="../media/issue60/weymouth/dl_01.png" /><br />
<strong>Figure 5.</strong> Excerpt of transcript with the header names Speaker Name, Start Time, End Time and Text below a portion of sample dialogue</p>
<p>That said, the tool is not perfect. While modern recordings in good audio conditions have extremely high transcription accuracy, poor quality recordings and interviews between two similar sounding people can require significant correction.</p>
<h2>Python Text Mining</h2>
<p>After using the web-based text mining tool Voyant to develop subject tags for previous oral history collections, I wanted to create a text mining tool from scratch using Python that would allow the targeting of specific words and phrases and create custom tagging categories. While the Natural Language Toolkit (NLTK) has many more complicated modules for text processing, such as tokenization, parts of speech tagging and fuzzy string matching, I found these approaches generated too many false positives when it was attributing areas of the transcripts to subject tags.</p>
<p>Instead, this text mining approach favors less automation and more transparent and customizable controls. Custom subject sections are created which contain around 50 terms or two-word phrases. Running the Python script concatenates the CSV files, minimally processes them and then searches for these terms, then tallies and prints the terms in the terminal according to word frequency.</p>
<p>Once the CSVs of the transcripts are generated in Premiere, they are added to a Google Drive folder that is shared with the student workers who will be copy editing the transcriptions. Using the Apps Script downloadSheetsAsCSV code (see appendix 2) is run to generate a CSV with only the dialogue column. The CSVs are then added to the “CSV” folder in the Python workspace.</p>
<p>The code begins by importing the Pandas library for data manipulation, String for punctuation removal, Natural Language Toolkit (NLTK) stopwords (words removed from text before processing and analysis) for each collection and Collection Counter to tally identified terms within the dialogue.</p>
<p>Next, the ‘preprocess_text’ function removes punctuation, converts text to lowercase and handles missing values by replacing them with an empty string. CSV file paths are constructed, and the text data is concatenated into a single string corpus. Word frequency is tallied and the 20-50 most frequent words and phrases for each subject tag section are generated when the code is run.</p>
<p>Below this header material in the Python file are the three subject tag categories:</p>
<ul>
<li>General: agriculture, animals, clothing, etc.</li>
<li>Geographic: (based loosely on migration statistics from the 1910 Idaho census): Basque, Britain, Canada, China, etc.</li>
<li>Custom: (example from our Rural Women’s History Project): marriage and divorce, motherhood, reproductive rights, etc.</li>
</ul>
<p>These fifty sections have a list of fifty associated terms and phrases that the script is searching for within the combined transcription corpus. These terms were generated using ChatGPT-4 turbo with the following qualifications:</p>
<ul>
<li>The word or phrase is only associated with one section. For example, regarding the sections agriculture and animals, the word “pasture” would be excluded since it could refer to both the land used for grazing animals and also the act of animals grazing.</li>
<li>Exclude homographs (words that are spelled the same but have different meanings). For example, “sow” refers both to an adult female pig and the agricultural act of planting seeds in the ground.</li>
<li>Placenames and how certain nationalities would refer to themselves for the geographic sections. For example, “Philippines”, “Filipino”, “Tagalog…”, “Norwegian”, “Norway”, “Oslo…” or “Japanese”, “Japan”, “Tokyo”, etc.</li>
<li>Terms and phrases favor informal, conversational speech.</li>
</ul>
<p>These text mining categories and sections produce a total of 2,250 associated terms or phrases that are being identified across the combined transcript corpus before the script tallies these words to generate the output shown below:</p>
<p class="caption"><img decoding="async" src="../media/issue60/weymouth/dl_03_new.png" /><br />
<strong>Figure 6.</strong> Sample of identified terms from combined transcripts, tallied in descending order.</p>
<p>See Appendix 1 for the header script and one subject section or visit the GitHub repository to view code in full.</p>
<h2>Apps Script Connection and Customization</h2>
<p>Once this text mining data is produced, it can be copied and pasted into the Primary Tag Sheet in Google Sheets, located in the same folder as the transcripts for student workers to access and edit. Using the Text to Columns function, subject tag sections can be split into column A and their associated words into column B using the programmed “##” as the separator.</p>
<p class="caption"><img decoding="async" src="../media/issue60/weymouth/dl_04.png" /><br />
<strong>Figure 7.</strong> Example of the formatted primary tags sheet with headers reading tags in column A and associated words in column B.</p>
<p>After minor formatting to the individual transcript, student workers access the Apps Script extension located in the drop-down menu. Transcribers then enter the code (see Appendix 3), and make two adjustments:</p>
<ul>
<li>Change the sheet name of the transcript they are editing on line 6</li>
<li>Change the URL of their primary tag sheet on line 13. Then save and run the code</li>
</ul>
<p>Now the individual transcript is connected to the Primary Tag Sheet, which will automatically search the text column for these terms and phrases and fill in the tag column of the transcript with its associated subject tag.</p>
<p>It is important to state that this process is not intended to replace human transcribers but shifts the focus from manual tagging to copy editing.</p>
<p>If transcribers notice that a tag is either not applicable or missing from the Primary Tag Sheet, they are encouraged to make these additions or subtractions and rerun the Apps Script on their individual Sheets, which will automatically enact these revisions across the entire document. If transcribers notice errors that are more specific to individual transcriptions, they can paste these edits into an additions or subtractions column to the right of the tag column, so the changes aren’t written over by future runs of the Apps Script code.</p>
<h2>Findings</h2>
<p class="caption"><img decoding="async" src="../media/issue60/weymouth/dl_06.png" /><br />
<strong>Figure 8.</strong> Example of a pre and post process tagging visualization of a recording, with the post process being dramatically more dense.</p>
<p>While initially testing this process, my main concerns were:</p>
<ul>
<li>Would transcribers find the Apps Script coding element confusing and/or anxiety-inducing?</li>
<li>Due to the complexity of language, would the automated tagging generate so many false positives that correcting these items would become a drag on productivity?</li>
</ul>
<p>Working with a student worker and a fellowship recipient, copy editing 30 transcripts over the course of two months in the summer of 2024, these factors were not an issue. Possibly helpful in this effort was weekly meetings where we checked in and tested the code, sometimes purposefully breaking it to show how those mistakes can be easily fixed and demonstrate how they can update the Primary Tags Sheet and rerun on their individual transcript sheets. Rather than simply asking student workers to transcribe recordings—work that offers little to highlight on a CV and can lead to burnout and high turnover—this process allows transcribers to engage in coding, create and modify tags, and see those changes reflected instantly through the Apps Script process.</p>
<p>I had the opportunity to revisit and iterate on these tools and processes for another oral history digital collection undertaken in January 2025, this time working with the Digital Projects Manager, as opposed to student transcribers. We were able to complete the process lifecycle of transcription, tagging and copy editing for 100 recordings in just over a week, increasing productivity from the summer 2024 initiative by 1233%.</p>
<p>Regarding the limitations of data-driven, human-edited automated tagging, program managers must communicate that automated tags are only a starting point. Tags may be incorrectly applied, missing or need to be applied more broadly to transcripts. Even when these measures are taken, the amount of detail this process accrues is easily distinguishable in the before and after OHD tagging visualization shown above (fig. 8). One could argue that the density of the data might now make it difficult for the researcher to navigate, especially on mobile devices. This continues to be a dialogue as we refine this workflow.</p>
<h2>Conclusion</h2>
<p>While discussing grant funding for digital initiatives, a colleague pointed out that the time-intensive nature of oral history projects often leads to their neglect. As they put it:</p>
<blockquote><p>“Would you rather present ten oral history recordings or 500 photographs?”</p></blockquote>
<p>This quantity-focused selection criteria ultimately poses an existential threat, leaving these materials physically vulnerable as they languish in the archives. Bicentennial and community oral history initiatives, rich in non-academic perspective, offer a uniquely biographical account of places and provide valuable contrast and context to the accepted historical record. By utilizing machine learning, Python, and Apps Script approaches, this process seeks to make digitizing these resources more efficient and accessible, promoting their preservation and availability to the public.</p>
<h2>References and Notes</h2>
<p>[<a id="note1" href="../index.html%3Fp=18286.html#ref1">1</a>] Digital Collections, University of Idaho. University of Idaho Library Digital Initiatives. 2024 [cited 2024 Jul 8]. Available from: <a href="https://www.lib.uidaho.edu/digital/collections.html">https://www.lib.uidaho.edu/digital/collections.html</a><br />
[<a id="note2" href="../index.html%3Fp=18286.html#ref2">2</a>] Home. CollectionBuilder. [accessed 2025 Feb 13]. <a href="https://collectionbuilder.github.io/">https://collectionbuilder.github.io/</a><br />
[<a id="note3" href="../index.html%3Fp=18286.html#ref3">3</a>] Stephan W. The Platinum Rule Meets the Golden Minimum: Inclusive and Efficient Archival Description of Oral Histories. Journal of Contemporary Archival Studies. 2021;8(1). <a href="https://elischolar.library.yale.edu/jcas/vol8/iss1/11">https://elischolar.library.yale.edu/jcas/vol8/iss1/11</a><br />
[<a id="note4" href="../index.html%3Fp=18286.html#ref4">4</a>] Moretti F. Conjectures on World Literature. New Left Review. 2000;(1):54–68.<br />
[<a id="note5" href="../index.html%3Fp=18286.html#ref5">5</a>] Boyd DA. ‘I Just Want to Click on it to Listen’: Oral history archives, orality and usability. In: The Oral History Reader. 3rd ed. Routledge; 2015.<br />
[<a id="note6" href="../index.html%3Fp=18286.html#ref6">6</a>] AVP. Quantifying The Need: A Survey Of Existing Sound Recordings In Collections In The United States. AVP. 2014 [accessed 2025 Feb 14].<a href="https://www.weareavp.com/quantifying-the-need-a-survey-of-existing-sound-recordings-in-collections-in-the-united-states/"> https://www.weareavp.com/quantifying-the-need-a-survey-of-existing-sound-recordings-in-collections-in-the-united-states/</a><br />
[<a id="note7" href="../index.html%3Fp=18286.html#ref7">7</a>] Speech to text in Premiere Pro FAQ. Adobe. [cited 2024 Jul 8]. Available from: <a href="https://helpx.adobe.com/content/help/en/premiere-pro/using/speech-to-text-faq.html">https://helpx.adobe.com/content/help/en/premiere-pro/using/speech-to-text-faq.html</a></p>
<h2 class="abouttheauthor">About the Author</h2>
<p>Andrew Weymouth is the Digital Initiatives Librarian at University of Idaho, specializing in static web design to curate the institution’s special collections and partner with faculty and students on fellowship projects. His work spans digital scholarship projects at the universities of Oregon and Washington and the Tacoma Northwest Room archives, including long form audio public history projects, architectural databases, oral history and network visualizations. He writes about labor, architecture, underrepresented communities and using digital methods to survey equity in archival collections.</p>
<p>Professional Site: <a href="https://aweymo.github.io/base">aweymo.github.io/base</a></p>
<h2>Appendices</h2>
<h3>Appendix 1. Excerpt of Python Text Mining Tool</h3>
<pre><code>
import pandas as pd
import string
from nltk.corpus import stopwords
from collections import Counter
import re

# Download NLTK stopwords data
import nltk
nltk.download('stopwords')

# Define preprocess_text function
def preprocess_text(text):
    if isinstance(text, str):  # Check if text is a string
        text = text.translate(str.maketrans('', '', string.punctuation)) 
        text = text.lower()  # Convert text to lowercase
    else:
        text = ''  # Replace NaNs with an empty string
    return text

# Load stopwords for both Spanish and English
stop_words_spanish = set(stopwords.words('spanish'))
stop_words_english = set(stopwords.words('english'))

# Combine both sets of stopwords
stop_words = stop_words_spanish.union(stop_words_english)

import os

# Directory containing CSV files
directory = "/Users/andrewweymouth/Documents/GitHub/transcript_mining_base/CSV"

# List of CSV file names
file_names = [
    'example_01.csv', 'example_02.csv', 'example_03.csv'
]

# Construct file paths using os.path.join()
file_paths = [os.path.join(directory, file_name) for file_name in file_names]

# Initialize an empty list to hold the DataFrames
dfs = []

# Try reading each CSV file and print which file is being processed
for file_path in file_paths:
    try:
        print(f"Processing: {file_path}")
        # Add quotechar and escapechar for handling CSVs with quotes
        dfs.append(pd.read_csv(file_path, encoding='utf-8', quotechar='"', escapechar='\\'))
    except Exception as e:
        print(f"Error with file {file_path}: {e}")

# Concatenate text data from all dataframes into a single corpus
corpus = ''
for df in dfs:
    text_series = df['text'].fillna('').astype(str).str.lower().str.strip()  # Extract and preprocess text column
    corpus += ' '.join(text_series) + ' '  # Concatenate preprocessed text with space delimiter

# Preprocess the entire corpus
cleaned_corpus = preprocess_text(corpus)

# Remove stopwords from the corpus
filtered_words = [word for word in cleaned_corpus.split() if word not in stop_words and len(word) &gt;= 5]

# Count the frequency of each word
word_freq = Counter(filtered_words)

# Get top 100 most frequent distinctive words with occurrences
top_distinctive_words = word_freq.most_common(100)

# === General Section ===

def find_agriculture_terms(corpus):
    # Define a list of agriculture-related terms
    agriculture_terms = [term.lower() for term in ["harvest", "tractor", "acreage", "crop", "livestock", "farm field", "barn building", "ranch", "garden", "orchard", "dairy", "cattle", "poultry", "farming equipment", "fertilizer", "seed", "irrigation", "plow", "farmhand", "hoe", "shovel", "milking", "hay", "silage", "compost", "weeding", "crop rotation", "organic", "gmo", "sustainable", "farming", "rural", "homestead", "grain crop", "wheat", "corn maize", "soybean", "potato", "apple fruit", "berry", "honey", "apiary", "pasture", "combine harvester", "trailer", "baler", "thresher"
    ]]

    # Initialize a Counter to tally occurrences of agriculture-related terms
    agriculture_word_freq = Counter()

    # Tokenize the corpus to handle multi-word expressions
    tokens = re.findall(r'\b\w+\b', corpus.lower())

    # Iterate over each token in the corpus
    for word in tokens:
        if word in agriculture_terms:
            agriculture_word_freq[word] += 1

    # Return the top 20 most common agriculture-related terms
    return agriculture_word_freq.most_common(20)

# Call the function to find agriculture-related terms in the corpus
top_agriculture_terms = find_agriculture_terms(corpus)

# Print the top 50 agriculture-related terms
print("## agriculture")
for word, count in top_agriculture_terms:
    print(f"{word}: {count}")
</code></pre>
<h3>Appendix 2. Apps Script Code for Exporting Sheets to CSV for Text Mining</h3>
<pre><code>
function downloadSheetsAsCSV() {
  // Specify the folder ID of the folder containing the Google Sheets
  var folderId = 'folder-id';  // Replace with your folder ID
  var folder = DriveApp.getFolderById(folderId);
  var files = folder.getFiles();
  
  // Loop through each file in the folder
  while (files.hasNext()) {
    var file = files.next();
    
    // Check if the file is a Google Sheet
    if (file.getMimeType() === MimeType.GOOGLE_SHEETS) {
      var spreadsheet = SpreadsheetApp.openById(file.getId());
      var sheets = spreadsheet.getSheets();
      
      // Loop through all sheets and download each as CSV
      for (var i = 0; i &lt; sheets.length; i++) {
        var sheet = sheets[i];
        var csv = convertSheetToCSV(sheet);
        
        // Create a new CSV file in the same folder
        var csvFile = folder.createFile(sheet.getName() + '.csv', csv, MimeType.CSV);
        Logger.log('Downloaded: ' + csvFile.getName());
      }
    }
  }
}

function convertSheetToCSV(sheet) {
  var data = sheet.getDataRange().getValues();
  
  // Find the index of the "words" column and replace it with "text"
  var headerRow = data[0];
  var wordsIndex = headerRow.indexOf('words');  // Locate the "words" column index
  
  if (wordsIndex !== -1) {
    headerRow[wordsIndex] = 'text';  // Change "words" to "text"
  }
  
  // Start building the CSV with the header row
  var csv = 'text\n';
  
  // Loop through rows and extract the "words" column, removing line breaks
  for (var i = 1; i &lt; data.length; i++) {  // Start from 1 to skip the header row
    var row = data[i];
    
    // Extract the "words" column (index of "words" column)
    var cell = row[wordsIndex];
    
    // Remove all line breaks (carriage returns, newlines, etc.) within the "words" data
    if (typeof cell === 'string') {
      cell = cell.replace(/(\r\n|\n|\r)/gm, ' ');  // Replace all line breaks with space
      cell = cell.replace(/[^\w\s,.'"-]/g, '');  // Remove punctuation except for some valid ones
    }
    
    // Enclose the text in quotes to avoid column splitting due to commas
    cell = '"' + cell + '"';
    
    // Add the cleaned "text" to the CSV output
    csv += cell + '\n';
  }
  
  return csv;
}
</code></pre>
<h3>Appendix 3. Apps Script Code for Linking Transcript to Primary Tag Sheet</h3>
<pre><code>
function fillTags() {
  // Get the active spreadsheet
  var spreadsheet = SpreadsheetApp.getActiveSpreadsheet();
 
  // Get the transcript sheet by name
  var transcriptSheet = spreadsheet.getSheetByName("your-sheeet-name");
  if (!transcriptSheet) {
	Logger.log("Transcript sheet not found");
	return;
  }
 
  // Set the header in cell E1 to "tags"
  transcriptSheet.getRange("E1").setValue("tags");
 
  // Get the tags spreadsheet by URL
  var tagsSpreadsheet = SpreadsheetApp.openByUrl("your-spreadsheet-url");
  if (!tagsSpreadsheet) {
	Logger.log("Tags spreadsheet not found");
	return;
  }
 
  // Get the tags sheet within the tags spreadsheet
  var tagsSheet = tagsSpreadsheet.getSheetByName("tags");
  if (!tagsSheet) {
	Logger.log("Tags sheet not found");
	return;
  }
 
  // Get the range of the transcript column
  var transcriptRange = transcriptSheet.getRange("D2:D" + transcriptSheet.getLastRow());
  var transcriptValues = transcriptRange.getValues();
 
  // Get the range of example words and tags in the tags sheet
  var exampleWordsRange = tagsSheet.getRange("B2:B" + tagsSheet.getLastRow());
  var tagsRange = tagsSheet.getRange("A2:A" + tagsSheet.getLastRow());
  var exampleWordsValues = exampleWordsRange.getValues();
  var tagsValues = tagsRange.getValues();
 
  // Create a map of example words to tags
  var tagsMap = {};
  for (var i = 0; i &lt; exampleWordsValues.length; i++) {
	var word = exampleWordsValues[i][0].toLowerCase();
	var tag = tagsValues[i][0];
	tagsMap[word] = tag;
  }
 
  // Initialize an array to store the tags for each transcript entry
  var transcriptTags = [];
 
  // Loop through each transcript entry
  for (var i = 0; i &lt; transcriptValues.length; i++) {
	var text = transcriptValues[i][0];
	var uniqueTags = [];
	
	if (typeof text === 'string') {
  	// Use regular expression to extract words and handle punctuation
  	var words = text.match(/\b\w+['-]?\w*|\w+['-]?\w*\b/g);
  	
  	// Check each word in the transcript entry against the tags map
  	if (words) {
    	for (var j = 0; j &lt; words.length; j++) {
      	var word = words[j].toLowerCase().replace(/[.,!?;:()]/g, '');
      	var singularWord = word.endsWith('s') ? word.slice(0, -1) : word;
      	
      	if (tagsMap.hasOwnProperty(word) &amp;&amp; !uniqueTags.includes(tagsMap[word])) {
        	uniqueTags.push(tagsMap[word]);
      	} else if (tagsMap.hasOwnProperty(singularWord) &amp;&amp; !uniqueTags.includes(tagsMap[singularWord])) {
            uniqueTags.push(tagsMap[singularWord]);
      	}
    	}
  	}
	}
	
	// Add the determined tags to the array
    transcriptTags.push([uniqueTags.join(";")]);
  }
 
  // Get the range of the tags column in the transcript sheet, starting from E2
  var tagsColumn = transcriptSheet.getRange("E2:E" + (transcriptTags.length + 1));
 
  // Set the values in the tags column to the determined tags
  tagsColumn.setValues(transcriptTags);
}
</code></pre>
<p>&nbsp;</p>
					</div>
														</div>
				<!-- You can start editing here. -->

<div class="comments">
	<p class="subscriptionlinks">Subscribe to comments: <a href="18286/feed">For this article</a> | <a href="http://feeds.feedburner.com/c4lj/comments">For all articles</a></p>

			<!-- If comments are open, but there are no comments. -->

	 

<h3 id="respond">Leave a Reply</h3>


<form action="https://journal.code4lib.org/wp-comments-post.php" method="post" id="commentform">


<p><input type="text" name="author" id="author" value=""/>
<label for="author">Name (required)</label></p>

<p><input type="text" name="email" id="email" value="" />
<label for="email">Mail (will not be published) (required)</label></p>

<p><input type="text" name="url" id="url" value="" />
<label for="url">Website</label></p>


<p><textarea autocomplete="new-password"  aria-label="Comment box" id="f127f6ccbe"  name="f127f6ccbe"   cols="50" rows="10"></textarea><textarea id="comment" aria-label="hp-comment" aria-hidden="true" name="comment" autocomplete="new-password" style="padding:0 !important;clip:rect(1px, 1px, 1px, 1px) !important;position:absolute !important;white-space:nowrap !important;height:1px !important;width:1px !important;overflow:hidden !important;" tabindex="-1"></textarea><script data-noptimize>document.getElementById("comment").setAttribute( "id", "aaac7e26242407c38bc875a09b5a006d" );document.getElementById("f127f6ccbe").setAttribute( "id", "comment" );</script></p>

<p><input name="submit" type="submit" id="submit"  value="Submit Comment" />
<input type="hidden" name="comment_post_ID" value="18286" />
</p>
<p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="4552034de8" /></p><p style="display: none !important;" class="akismet-fields-container" data-prefix="ak_"><label>&#916;<textarea name="ak_hp_textarea" cols="45" rows="8" maxlength="100"></textarea></label><input type="hidden" id="ak_js_1" name="ak_js" value="43"/><script>document.getElementById( "ak_js_1" ).setAttribute( "value", ( new Date() ).getTime() );</script></p>
</form>


</div>
							</div>

			<div id="meta">
				<div id="issn">
					<p>ISSN 1940-5758</p>
				</div>
				<div class="search-sidebar">
				<form method="get" id="searchform" action="../index.html">
					<div>
						<input type="text" value="" aria-labelledby="searchsubmit" name="s" id="s" />
						<input type="submit" value="Search" id="searchsubmit"/>
					</div>
				</form>
				</div>
				<div id="archives">
					<h2>Current Issue</h2>
						<ul>
							<li><a href="../issues/issues/issue60.html">Issue 60, 2025-04-14</a></li>
						</ul>

					<h2>Previous Issues</h2>
						<ul>
              <li><a href="../issues/issues/issue59.html">Issue 59, 2024-10-07</a></li><li><a href="../issues/issues/issue58.html">Issue 58, 2023-12-04</a></li><li><a href="../issues/issues/issue57.html">Issue 57, 2023-08-29</a></li><li><a href="../issues/issues/issue56.html">Issue 56, 2023-04-21</a></li>              <li><a href="../index.html%3Fp=2476.html">Older Issues</a></li>
						</ul>
				</div>
				<div id="forauthors">
					<h2>For Authors</h2>
					<ul>
						<li class="page_item page-item-4"><a href="../index.html%3Fp=4.html">Call for Submissions</a></li>
<li class="page_item page-item-7"><a href="../index.html%3Fp=7.html">Article Guidelines</a></li>
					</ul>
				</div>
			</div>
						<div id="footer">
				<p id="login"><a href="../wp-login.php.html">Log in</a></p>
				<p id="copyright">This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/3.0/us/">Creative Commons Attribution 3.0 United States License</a>.<br /><a rel="license" href="http://creativecommons.org/licenses/by/3.0/us/"><img alt="Creative Commons License" src="http://i.creativecommons.org/l/by/3.0/us/80x15.png" /></a></p>
			</div>
			<script type="speculationrules">
{"prefetch":[{"source":"document","where":{"and":[{"href_matches":"\/*"},{"not":{"href_matches":["\/wp-*.php","\/wp-admin\/*","\/wp-content\/uploads\/*","\/wp-content\/*","\/wp-content\/plugins\/*","\/wp-content\/themes\/c4lj-theme\/*","\/*\\?(.+)"]}},{"not":{"selector_matches":"a[rel~=\"nofollow\"]"}},{"not":{"selector_matches":".no-prefetch, .no-prefetch a"}}]},"eagerness":"conservative"}]}
</script>
<script defer type="text/javascript" src="../wp-content/plugins/akismet/_inc/akismet-frontend.js%3Fver=1748382734" id="akismet-frontend-js"></script>
		</div>
	</body>
</html>
